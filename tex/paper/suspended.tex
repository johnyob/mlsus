%; whizzy section

%% Leave the above line for didier
%% No macros before \documentclass

\documentclass[acmsmall,screen,nonacm]{acmart}

\input{suspended.cfg}
\newcommand{\acmart}{\True}
\usepackage{suspended}

%% \Xfirstname defined in {mycomments}
%% Use either
%%   \Xfistname[text to comment]{your comment on the text}
%% or
%%   \Xfirstname{free comment}
%% Uncomment this line to hide all comments.
% \UNXXX{}

\usepackage{marginnote}

\title{Omnidirectional type inference with syspended constraints
   for static overloading and semi-explicit polymorphism}

%% Was 
%% \title{Omnidirectional type inference for \ML extensions}

\begin{document}

\begin{abstract}
We propose a new concept of \emph{omnidirectional} type inference, which is
the ability to resolve \ML-style typing constraints in disorder, by
contrast with all known implementations that always typecheck the
bindings before the bodies of let-expressions.
%
This relies on two technical devices: \emph{partial type schemes}
and \emph{suspended match constraints}. Partial here means  the
possibility of taking an instances of a type scheme that is not yet
completely solved with a mechanism to tack and update their instances when
the type scheme is refined, incrementally.
\emph{Suspended} match constraints means a mechanism to delay the resolution of
some constraints which typically require non-local not-yet-available
contextual information and to be discharged then when we have learned enough
information from the resolution of other constraints, typically when some type
variables have been instantiated.
%
We illustrate the benefit of omnidirectional type inference for several
advanced features present in the \OCaml language, including static
overloading of record labels and datatype constructors on the one hand and
semi-explicit polymorphism on the other hand.
\end{abstract}

\maketitle

\Xdidier
  {Show we replace semi-explicit by higher-rank?}


\section{Introduction}

ML polymorphism, also known as Hindley-Milner type system, has been a very
sweet spot in the design space of strongly typed programming languages, as
it enjoys the principal type property and more importantly is closed by
common program transformations such as let-contraction and let-expansion,
and permutation of the order or arguments.

Over the years, many extensions of \ML have been proposed.  While some of
them, such as flexible records, higher-kinded types, or dimensional types
fits nicely into the \ML framework, others such as GADTs or semi-explicit
polymorphism, or overloading of datatype constructors and record fields, are
less satisfactory, since they require to \emph{twist} the ML-style
specification with some direct or indirect algorithmic flavor in order to
preserve \emph{principality}.

All borderline extensions have in common that they introduce new construct
that require explicit type annotations that may sometimes but not always be
omitted. They can actually be viewed as an elaboration into the slightly weaker
language that makes annotations mandatory, but itself fits nicely into
the \ML framework. The elaboration then defines which annotations can be
omitted and rebuild from context.
%
The difficulty lies in finding a methodical such specification that comes
with a complete (hence principal) and effective elaboration algorithm.

The solutions proposed so far all enforce some ordering in which type
inference is performed, which can then be used to propagate both inferred
types and user-provided type annotations as \emph{known} types that can be
used for disambiguation and enable the omission of some annotations.

\paragraph{Bidirectional type inference}

Bidirectional type inference is a known alternative to unification for
propagating type information. It can be presented by adding an optional
expected type in addition to the expression to be typechecked and the
context in which it should be typechecked to the type inference algorithm.
Type inference is said in checking mode the expected type is present, and in
(the usual) inference mode when it is absent. The checking mode allows to
pass source type annotations but also previously inferred types such as the
codomain of a function in an application position during type inference of
the arguments.  The modes of operation are specified by typing rules.  Hence
bidirectional type inference is a framework that can be instantiated
differently, depending on the choice of modes in typing rules.
Bidirectional type inference has been largely used for languages with
higher-rank polymorphism or subtyping~\TODO{citations}. Still, both \OCaml
and \Haskell use a limited form of bidirectional type inference.



\paragraph{Poly-directional type inference}

\OCaml uses another mechanism for propagation of type
information, based on polymorphism, which actually enforces an ordering in
the typechecking of let-bindings, where the (polymorphic part of) a the
bound expression must be typed before the body of the let-expression.
We call it \emph{poly}-directional type inference.\Xdidier {let-directional?}

Initially introduced with semi-explicit polymorphism to \ML~\cite
{Garrigue-Remy/poly-ml} it has also been used for the overloading of record
fields. The user may define two records with overlapping fields:
\begin{program}[input]
type one = {x : int}
type 'a two = {x : 'a; y : int}
\end{program}
This can be thought of as defining two abstract types \ocaml{foo} and
\ocaml{bar} with two access \ocaml{(_:one).x} and \ocaml{(_:two).x} of
respective types \ocaml{(one -> int)} and \ocaml{(two -> int)}, among
others.
%
In the absence of overloading the latter definition would hide the former
and we fall back in \ML. With overloading enabled, both definitions are
visible and the compiler must statically choose one of them.
%
The question is which one to choose or, equivalently, which type to infer
in the following three expressions? 
\begin{program}[input]
let e_1 r = r.x
let e_2 = let r = {x = 1} in r.x
let e_3 = (fun r -> r.x) {x = 1} 
\end{program}
To be able to resolve the overloading, the type of the projection should be
\emph{known} to be either \ocaml{one} or \ocaml{'a two} when typing the
projection.  Expression \ocaml[indices]{e_1} is clearly ambiguous since
there is no clue on the type of \ocaml{r} and its typechecking
fails\footnote {In fact, \OCaml uses a default resolution strategy instead
of failing when the type is ambiguous, which is to use the last
definition. Here, this will amount to chose the type \ocaml{'a two}
still leading to a failure since the record passed to the function is
actually of type \ocaml{one}. But we shall ignore default resolution
strategies for the moment.}.
%
By contrast, the only possible
type for \ocaml{r} is \ocaml {one} in both \ocaml{e_2} and \ocaml {e_3}.
Still, there is a difference.  Indeed, the type of \ocaml{e_2} is considered
to be unambiguous, while the type of \ocaml{e_3} is ambiguous. To
understand why \ocaml{e_3} fails, consider the equivalent versions of
\ocaml{e_3} where \ocaml{@@} and \ocaml{|>} are the application and the
reverse application functions.
\begin{program}[escapechar={},input]
let e_4 = (fun r -> r.x) @@ {x = 1} 
let e_5 = {x = 1}  |> (fun r -> r.x) 
\end{program}
\OCaml  does not make any difference between
\ocaml[indices]{e_1}, \ocaml[indices]{e_2}, or \ocaml[indices]{e_3}
and consider all subexpressions in an application, including the function
and all arguments as being inferred simultaneously, until they are
let-bound. More precisely, polymorphic types are considered to be known
while monomorphic types are consider to be unknown (or rather
not-yet-known). In \ocaml{fun r -> r.x}, the type of \ocaml{r} in
in the subexpression \ocaml{r.x} is not-yet-known even if equal to
\ocaml{one} since this is the lambda-bound variable \ocaml{r}, hence in the
typing context, hence it is monomorphic.\footnote{Technically, all type
nodes are annotated with a special of variables called annotation variables
$\av$, so that we may distinguish between the polymorphic binding $\xa : 
\all \av. \t^\av$ that binds $r$ to the known (raw) type $\t$ and the monomorphic
binding $\xb : \t^\av$ that binds $\xa$ to the unknown (raw) type $\t$.}


\paragraph{Limitations of directional type inference}

Directional type inference have been in used in different languages and
somehow proven to work. Still, each of them is unsatisfactory.  The main
critics of bidirectional type inference is to make a somewhat arbitrary
choice in the specification of the flow of information, typically in
applications: should the the function be typed first and its codomain be
used to improve the typing of its argument or, on the opposite, should the
argument be typed first and be used as the type of the codomain of the
function?  There are examples when the former is a better choice and others
when the latter is preferable---but typing rules have to choose one of the
two alternative forever.  One may also allow information to flow between
multiple arguments passed to the same function, or on the opposite disallow
it entirety. Again, there is not best choice. We eventually have a complete
algorithm with respect to a spec that made somewhat arbitrary choices.

Poly-directional type inference seems a better first for \ML, since it
relies on polymorphism which is the essence of \ML. Still, it does not even
allow to propagate user-type annotations! Thus \OCaml, which use
poly-directional type inference as the primary mechanisms also added a weak
form of bidirectional propagation. Besides, the implementation of
poly-directional type inference has an algorithmic cost, since the addition
of variable annotations requires to reread dags as trees destroying the
sharing that naturally resulting from unification, which may increase the
size of types and the cost of type inference. For that reason, the implement
of \OCaml cheats and is incomplete by default and the user must explicitly
require the most expensive computation of principal types when desired.

\paragraph{Incompleteness of directional type inference}

Both bidirectional and poly-directional type inference rely on an ordering
for type propagation that is partially specified, explicitly or implicitly,
to take advantage of user-provided type annotations and already-inferred types
to alleviated the need for extra annotations.

While they come with complete algorithms with respect to their
specification, their specifications include some choices that are subjective
and may sometimes look arbitrary.

Indeed, they all reject examples as ambiguous when sometimes they would be a
unique well-typed solution, as seen in the limitation of direction type
inference.  The first sources of incompleteness results from the ordering
enforced between the resolution of core-language constraints that prevents
some core information to be taken into account when resolving ambiguities.

However, the resolution of each ambiguity is another source of
incompleteness, which is actually two-fold:
\begin{enumerate}
\item  
  the resolution is not always taking all contextual type information into
  account.  
\item
  each ambiguity is solved independently of other yet unresolved ones;
\end{enumerate}
To illustrate the latter, the following example fails as ambiguous with
poly-directional inference, while \ocaml{r} can only be of type
\ocaml{int two}.  
\begin{program}[error]
let e_6 r = (r.x = r.y)
\end{program}
Indeed, \ocaml{r} is monomorphic when typing the projection.  This source of
incompleteness should wishfully be eliminated.

On the opposite, the former is a reasonable design to keep type inference
effective, since the complexity of general overloading is NP-hard even in
the absence of let-polymorphism,a s show by an encoding of 3-SAT problem as
shown by~\citet {Chargueraud-Bodin-Dunfield-Riboulet/jfla2025}.


\paragraph{Omnidirectional type inference}

We propose to use omnidirectional type inference to eliminate the first
source of incompleteness.  Our proposal with omnidirectional type inference
will correctly type all examples above (from \ocaml{e_1} to
\ocaml{e_6})\ignorespaces
\INFO {See \url
{https://gitlab.inria.fr/remy/semiun/-/blob/main/su/examples/record_fields.ml}}
---without any default strategy mechanism.

A type inference system without polymorphism, would be solely based on
unification constraints, which can be solved in any order.  The difficulty
originates from \ML let-polymorphism for which all known implementations
choose to always infer the type of a let-binding, turn it into a type scheme
that is assigned to the let-bound variable to extend the typing environment
in which the body is then typed. Algorithm $W$, its variant $M$ or more
flexible constraint solving algorithms all follow this strategy.

However, it is not mandatory.  In fact, typing constraints as presented
in~\cite {Pottier-Remy/emlti} can be instantiated before they are fully
solved, although this comes at the cost of duplicating some computation.
Still, this shows that the let-binding ordering is a convenience, not a
requirement.

Allowing typing constraint to be solved in disorder, type inference become
\emph{omnidirectional}.  This allows to take all program core-expression
constraints into account before resolving ambiguities.  This requires two
technical devices, hence two problems to solve:
\begin{enumerate*}
\item
  we first must work with partial types schemes, \ie. we the ability
  to instantiate type schemes that are not yet fully determined and
  consequently revisit their instances when their are being refined,
  incrementally.
\item
  we then must allow \emph{suspended match constraints} as a way to suspend
  ambiguity resolution until sufficient information has been found from
  context so that they can be discharged and hopefully resolved.

\end{enumerate*}
Once the framework established, it just remains to be instantiated
to each use case,




\subsubsection* {Plan}

The rest of the paper is organized as follows.
sf\begin{enumerate*}[label={}]
\item In \cref{sec:constraints}, we give an overview of suspended constraints and their 
application to various extensions for \ML. 
\item
  In \cref{sec:semantics}, we describe the constructions for a
  novel denotational semantics of suspended constraints.
\item
  In \cref{sec:language}, we define an extension of \ML featuring static
  overloading and semi-explicit first-class polymorphism along with its
  typing rules.  Here we give the precise definition of the constraint
  generation translation and state the theorems of soundness and
  completeness.
\item
  In \cref{sec:solving}, we provide a formal definition of our constraint
  solver as a series of non-deterministic rewriting rules.
\item
  In \cref{sec:implementation}, we describe an efficient implementation of
  suspended constraints.
\item
  In \cref{sec:related-work} and \cref{sec:future-work}, we discuss related
  and futre work.
\end{enumerate*}

\Xdidier
{I don't quite unedestand the difference between
\cref{sec:solving} and
\cref{sec:implementation}.
For me \cref{sec:solving} is the algorithm.
and \cref{sec:implementation} are just implementation details which could
fit at the end of~\cref{sec:solving}. Or do you mean that~\cref
{sec:solving} is generic and \cref {sec:implementaion} is an particular
instance?}

\subsubsection* {Our constributions}

Our contribusions are 
\begin{enumerate*}
\item
  a new \emph{omnidirectional} novel type inference technique for
  extensions of \ML with advanced features, based on two new devices,
  partial type schemes and suspended constraints;

  %% that require propagation of both
  %% user-defined type annotations and already inferred types to make other
  %% annotations optional, which is
  %% \item
  %% This is based on two new deviced:
  %% \begin{enumerate*}[label=\textbf{(\theenumii)}]
  %% \item
  %%   We enable the ability to instantiate type scheme before they are fully
  %%   resolved and and their incremental refinement which is propagated to their
  %%   instances; 
  %% \item
  %%   We introduce suspended constraint, guarded by a type variable, that are
  %%   discharged when their guard gets instantiated.
  %% \end{enumerate*}

\item
  A new more permissible type system for overloading of record fields and
  semi-explicit first-class polymorphism, and

\item
  a complete yet efficient constraint-solving type inference algorithm.
  
\end{enumerate*}


\begin{version}{}
\Xdidier {These are ``leftovers'' from the old introduction. Delete if/when
useless}

\subsection {Suspended constraints}


TODO introduce suspended
constraints. But: generalization becomes tricky.
\begin{itemize}
\item easy case: the choice is resolved while typing the definition
\item hard case: an a-priori generalizable variable is captured in a frozen
   constraint 
\item question: what to do when the frozen variable is generalizable?
\item another difficulty: supporting default clauses in the failure case
\end{itemize}

Different kind of difficulties:
\begin{itemize}
\item finding a reasonable declarative semantics for the behavior we want
\item implementing a solver, which is quite hard
\end{itemize}
\end{version}

\Xalistair{Is this the right introduction? All the features we describe
below break principality usually. So our work is about principality. This is
a way to guarantee principal types for these advanced features.}
\Xdidier{All systems proposed have principal types. We just have more
principal types, \ie principal types with respect to a specification that is
more expressive} 

\Xalistair{For tuples we added a projection rule that gives the size of the
tuple. I imagine for polytypes there would be something like that.}
\Xdidier {Yes indeed: in the explicit language, we could require the skeleton
as a required annotation}

\section{Suspended constraints}
\label{sec:constraints}


\TODO{PLAN: Only the constraint syntax, not constraint-solving yet. The
point is to show examples in a concrete way.}
\Xalistair{Some of this probably belongs in the intro @Didier, but I didn't
want to conflict with your work}

\Xdidier {We need examples before we did in the technical details.
But since this section is still partly informal---no semantics, we may
then still consider as part of the intro and not described 
suspended constraints earlier} 

\Xdidier {The question is where do we put examples of programs that
do not typecheck in \OCaml, are expected to typecheck and that we can
typecheck (without explaining how they typecheck) ?  I gave one example for
record fields in the intro.  Should I also introduce semi-explicit
polymorphism and give an example in the intro?}

% Constraint-based approach

Our insights into directionality come by approaching type inference from a
\textit{constraint-based} \citep{TODO} perspective, in which type inference
is split into two stages, mediated by the language of \textit{constraints}
\citep{TODO}.  In the first stage, a term $\e$ is translated into a constraint
$\C$ which is satisfiable if and only if the term $e$ is well-typed. In the
second stage, the generated constraint $\C$ is solved (or shown to be
unsatisfiable).

% HMX 

A foundational approach to constraint-based type inference is \HMX
\cite{Odersky-Sulzmann-Wehr@tpos}---the Hindley-Milner system parameterized
by a constraint domain $X$, allowing constraints to appear within types. Our
work is inspired by the presentation of HMX by \citet{Pottier-Remy/emlti}
who fully decoupled constraint generation from constraint solving.  Their
key insight for the reduction of type inference to constraint solving is the
inclusion of term variables in the language of constraints.

% Life is hard
\Xalistair{This paragraph now feels out of place -- nonetheless I think we
need state how difficult this was :/}\Xdidier {It could be move to the
introduction}
Formulating a suitable constraint language for omnidirectional type
inference, a (provably) correct constraint translation for our extensions,
and an efficient, sound and complete solver involves several difficulties.
\Xdidier
{I think what is difficult is (1) the semantics of constraints (2) the
efficient implementation, rather than just a sound and complete
implementation. In particular, efficiency requires to use ranks and an
ordering on the way constraint are solve.}
\Xdidier
{Beside we have no proof that our implementation is efficient. In fact, I
think is not cheap: Assume that a type scheme of size $n$ is solved
incrementally by adding one node at a time and that every time we add a node
we propagate it to all its instantiations. Currently, each propagation with
redo the job, ignoring what had already be done ealier. Hence you get a
factor of $n$ complexity that need not necessarily be there. Plus
constraints are copied before being simplified. Another factor...}

\begin{mathparfig}[t]%
  {fig:constraint-syntax}%
  {Syntax of types and constraints}
\def \is{&::=&}
\def \crmid {\\&&\mid&}
\def \nois{&&}
\def \tag{&&}
\begin{tabular*}{\linewidth}{C~R=;;R;;LL~r}&
\cva, \cvb, \cvc \nois
\tag   Type variables
\\&
\t  \is
     \cv \mid \overline{\t} \Fapp ~ \mid \ldots
\tag   Constraint types
\\&
\Fapp \is
\tunit \mid \cdot \tarrow \cdot \mid \dots
\tag   Type constructors
\\[1ex]&
C  \is
        \ctrue
        \mid \cfalse
        \mid \Cone \cand \Ctwo
        \mid \cexists \cv C
        \mid \cunif \tone \ttwo
\tag Constraints
\crmid  \cletin x {\clam \cv \Cone} {\Ctwo}
        \mid  \cinst \x \t
      \crmid  \cmatch \cv \D  \hf
\\&
\sh \is \overline{\tv} \Fapp ~ \mid \ldots
\tag   Shapes
\end{tabular*}
\end{mathparfig}

% Syntax!

We begin by introducing the syntax of constraints
in~\Cref{fig:constraint-syntax}.  The constraint language contains
tautologic constraints ($\ctrue$), unsatisfiable constraints ($\cfalse$),
and logical conjunctions ($\Cone \cand \Ctwo$). The constraint form $\tone =
\ttwo$ asserts that the types $\tone$ and $\ttwo$ are equal.  The constraint
form $\cexists \tv C$ binds an existentially quantified type variable $\tv$
in $C$.

\Xdidier {What about the instantiation constraint? $\sigma \leq \t$?
(\string\leq\space should be macroified)}

% Constraint abstractions

There are two constructs that deal with the introduction and elimination of
constraint abstractions. As the name suggests, a constraint abstraction
$\cabs \tv C$ can simply be seen as a function which when applied to some
type $\t$ returns $C[\tv := \t]$. Constraint abstractions are introduced by
a let-construct $\cletin x {\cabs \tv \Cone} \Ctwo$ which binds the
constraint abstraction to the term variable $x$ in $\Ctwo$---additionally
ensuring the abstraction is satisfiable. They are eliminated using the
application constraint $\capp x \t$ which applies the type $\t$ to the
abstraction bound to $x$.

% Suspended match constraints

Finally, we introduce \textit{suspended match
constraints}\footnote{Previously dubbed `frozen constraints' \citep{TODO}}.
These constraints defer constraint generation until the \textit{shape}, \eg
its top-level constructor, of a type becomes known (through unification), at
which point they match on the shape to generate a new constraint. Formally,
a suspended constraint is written as $\cmatch \t \D \hf$, where:
\begin{enumerate}
\item
  The matchee $\t$ is a type. The constraint remains suspended until the
  shape of $\t$ is determined.  
\item 
  The handler $f$ is a meta function from shape ($\sh$) to constraints.
  Once the constraint is unblocked, this function is applied to generate the
  constraint to be solved.  Conceptually, $f$ represents a meta-level
  functionalization of the match's branches.
\item
  The closure $\D$ is the set of term and type variables $f$ is
  permitted to reference.  The role of $\D$ in constraint solving is
  discussed further in \cref{sec:solving}.
\end{enumerate}
\Xdidier {If $\D$ is a list of variables what not just write $\bar\tv$?}
\Xdidier {Why not write $\Lambda \D. \hf$ that $[\D] \hf$ since
$\hf$ is itself written as a function? It is because this $\Lambda$ should
be transparent while $\lambda$ is opaque? It you want a syntactic binder,
one that is not opaque, then I would write this $\Delta \bar\tv. \hf$ and
explain that $\Delta$ stands for $\Lambda$ while beeing transparent---we can
extra the list of variables $\bar\tv$ under that binder.}

\begin{mathparfig}[t]
{fig:shape-fn-def}
{Definition of $\shape(\t)$, a partial function from types to
shape-substitution pairs $\langle \sh, \theta \rangle$}
\shape(\tv) \eqdef
    \bot 

\shape(\overline{\t} \Fapp) \eqdef
        \langle \overline{\tv} \Fapp, [\overline{\tv := \t}] \rangle 
\end{mathparfig}

% Informal semantics

Informally, the semantics of $\cmatch \t \Delta \hf$ are as follows: the
constraint remains suspended until the shape of $\t$ is defined (through
unification), at which point the handler is applied to this shape $\hf(\sh)$
and the generated constraint is scheduled to be solved. If the shape of $\t$
is never determined, the constraint remains unsatisfiable.

% Applications

The remainder of this section illustrates the role of suspended constraints
in supporting language features that historically pose challenges to the
preservation of principal types.\Xdidier {The key here is not the
preservation of principal types in my opinion but that there are just
elimination of optional type annotations, which obviously requires the
use---hence also the propagation---of \emph{known} type information.  It is
then difficult to specify which annoations can be omitted.}
These include:
\begin{enumerate}
  \item Constructor and record label overloading for nominal algebraic
  datatypes;  
  \item Overloaded tuple project\Xdidier {projection?}, in the style of \SML; 
  \item Semi-explicit first-class polymorphism.
\end{enumerate}
We demonstrate how the typablity of each of these features can be elaborated
into constraints, formalized using a constraint generation function of the
form $\cinfer e \tv$, which, given a term $e$ and expected type $\tv$,
produces a constraint $C$ which is satisfiable if and only if $e$ is
well-typed. A formal account of the semantics of suspended constraints and
the declarative typing rules for these features is deferred to 
\cref{sec:semantics} and \cref{sec:language}, respectively.

% When solving $\cmatch \cv \Delta \hf$, the solver proceeds as follows:
% \begin{enumerate}
% \item
%   If $\cv$ is already unified to a non-variable type, apply the handler
%   $f$ immediately to the resolved type.
% \item
%   If $\cv$ is still unresolved, enqueue the suspended constraint to the
%   list of suspended constraints blocked by $\cv$.
% \item
%   During unification, whenever a type variable with a pending constraint
%   becomes resolved, the solver retrieves all blocked constraints on that
%   variable and schedules them.
% \end{enumerate}
% Once all constraints have been processed, any pending suspended constraints
% that remain unsolved indicate failure.

% \paragraph{Constraint-solving: an intuition}


% \paragraph{Applications}
% \begin{itemize}
% \item
%   type-based disambiguation of data constructors (and record fields)
% \item
%   polytypes (are we sure?) \\ ensuring that there is a unique polytype
%   skeleton
% \item
%   anti-unification-based overloading (are we sure?) \\ there is some
%   uniqueness of the head / variational type
% \end{itemize}
% \Xdidier {For me, the three examples are mandatory.
% disambiguation of data constructors and record fields is too simple...
% Also, polytypes may or may not include polyparams. Polyparams are a real
% challenge because most constraints will be delayed, hence the need for a
% strategy to resolve them. Static ad hoc overloading is more general.
% }

% \Xgabriel{I care most about the semantics and implementation of the
% constraint language.} 
% \Xdidier{It would be nice to have typing rules for the surface language, and
% show that they match the constraints generated.} 
% \Xalistair{I agree.}

% For each feature:
% \begin{itemize}
% \item propose *informal* surface-language typing rules
% \item elaborate them into the constraint language
% \end{itemize}

% We will provide *precise* declarative semantics in the ``Semantics of
% constraints'' section.

\subsection{Static overloading of constuctors and record labels}

% Intro to the feature

OCaml allows users to define data types with overloaded constructors or
record fields. 
\begin{program}
type m = L
type n = L
let x = L
\end{program}
The type of \code{x} is ambiguous since OCaml could infer either the type
\code{m} or the type \code{n}. This amounts to a principality issue, since
there is no most general type for \code{x}. \Xdidier {It is not just a
question of typing issue but of semantics: it should make a stratic choice
of the implementation. Just speaking of it as a type issue is a bit
misleading.} As such, OCaml gives the 
following error: ...

OCaml's type checker relies on type information provided by the
bidirectional\Xdidier{bidirectional is misleading here! In what sense ocaml
is a bidirectional typechecker? It is in fact poly-direction. (poly means
polymorphism here)} propagation of type
information to resolve these 
ambiguities. Annotating \code{x} with either \code{m} or \code{n} would
produce a well-typed program.

% Constructor disambiguation in suspended constraints
We propose an alternative approach: suspended constraints. For the ambiguous
constructor application $\econstr K e$, we generate the constraint:
\begin{align*}
\cinfer {\econstr K e} \cva &=
  \cexists \cvb \cinfer e \cvb
  \cand
  \cmatch \cva {\cva, \cvb}
    \parens
      {\clam {\overline{\cvc} \Fapp} 
        {\cletin \sigma {\D(\F)(\mathsf{K})} {\sigma \leq \cvb \to \cva}}
      }
\end{align*}
where the function $\clam {\overline{\cvc} \Fapp} {\cletin \sigma
{\D(\F)(\mathsf{K})} {\sigma \leq \cvb \to \cva}}$ looks up the type scheme
associated with the constructor using the type name $\F$ in the environment
($\Delta(\F)(\mathsf{K})$). \Xdidier{I don't really understand what you
mean. The prefix notation for types does not help. At least, we should write
$\lambda \parens{\bar \cvc \Fapp}. \ldots$. Still, I think it would help
to use a postfix notation in the formal presentation. In fact
$\bar \cvc(\Fapp)$ would not parse in \OCaml. And if $\D(\F)$ is supposed to
be a type $\t$, what does $\t(K)$ mean?
}\Xdidier {We haven't seen the instantiation constraint $\sigma \leq \cvb
\to \cva$ yet}

We require a unique type for each type constructor in a given algebraic data
type definition, thus there is no ambiguity with this mapping, despite there
potentially being many constructors in the environment named $\mathsf{K}$.

This not only correctly encodes OCaml's current behaviour, but does better.
\Xdidier {Omitting the final default pass called in case of failure to
disambiguate.}\Xdidier {behavior means sound an complete. Here you mean
that we succeed when OCaml does with the same answer, but we also succeed
with a correct answer when OCaml fails. Hence, we do not capture the OCaml behavior...}


\paragraph{Interaction with let-polymorphism}

%% Our intuition is not entirely correct here.

While the above constraint generation already improves on
OCaml's current inference approach, it is not powerful enough to capture the
simple idea that to resolve an ambiguous constructor we must use contextual
information to show there is a unique type constructor $\F$ for the
application.

We demonstrate this is with the following example:
\begin{program}
let foo =
  let f x = match x with L -> 1 in
  f (L : m)
\end{program}
With the above contraint generation, we would keep the type of \code{x}
as a generalizable variable, thus the application of \code{f} would not
resolve this ambiguity, requiring an annotation at the definition.

We require some way of splitting the head of the type from the type itself,
treating the head of the variable monomorphically and the rest
polymorphically.
\Xalistair{An explaination of kinded(1) types here }


\subsection{Semi-explicit first-class polymorphism}

% Polytypes (and some stuff about labelled types)

Semi-explicit first-class polymorphism \citep{Garrigue-Remy/poly-ml} uses
\textit{annotated types} to track the origins of polymorphic types.

The type constructor $\tpoly \ts \av$ that boxes a polymorphic type
$\ts$ turning it into a \textit{polytype} is actually annotated with
an annotation variable $\av$.  Once boxed, the polytype $\tpoly
\ts \av$ is considered a monotype.  This permits impredicative
polymorphism. Annotations variables may be generalized as over
variables and $\all \av \tpoly \ts \av$ is a type scheme.

The introduction form for polytypes is a boxing operator $\epoly
\e \ts$ with an explicitly polytype annotation $\ts$. The resulting expression
has type $\tpoly \ts \av$ when $\av$ is an arbitrary (typically fresh)
annotation variable and can thus be generalized. That is $\e$ can also be
assigned the type scheme $\all \av \tpoly \ts \av$.

Conversely, to instantiate an polytype expression, one must use an explicit
unboxing operator $\einst \e$ that need not any type annotation.
However, it requires $e$ to be of a polytype scheme $\all \av
\tpoly \ts \av$ and then gives $\einst \e$ a type $\t$ that is an instance of
$\ts$.  In particular, $\einst \e$ is ill-typed if $\e$ has type $\tpoly \ts
\av$ and $\av$ cannot be generalized.  It is the polymorphism of $\av$ that
ensures that the polytype is indeed known and not being inferred.

In particular $\efun \x {\einst \x}$ is not typable, since the
$\lambda$-bound variable $\x$ is assigned a monotype.  However, annotations
can be used to freshen annotation variables.  An annotation $e : \t$ behaves
as the application of a retyping function (one that behaves as the identity
$(\t)$ of type $\all \tvs \t \to \tp$ to the expression $\e$. We usually
omit annotation variables in type annotations, since they can be For
instance, $\efun {\x : \tpoly \ts {}} {\einst \x}$ which is syntactic sugar
for $\efun \x {\elet \x {\eapp {(\tpoly \ts {})} \x} {\einst \x}}$, is
well-typed since the polytype annotation allows the introduction of a fresh
variable annotation which can be generalized.

\begin{version}{}
%% Old version
Principality is tracked using generalization.  This is done by tracking the
polymorphism of labels. Type schemes quantify not only over type variables
but $\varepsilon$ variables. As a result, a term $e$ with the type $\tpoly
\ts \varepsilon$ for some fresh $\varepsilon$ may be generalized to
$\tfor \varepsilon {\tpoly \ts \varepsilon}$.  The typing rule for
$\einst e$ in fact requires $e$'s type to be $\tfor \varepsilon {\tpoly
\ts \varepsilon}$, not just a plain polytype $\tpoly \ts
\varepsilon$. The rational is that a label is polymorphic if and only if it
is principally known and not inferred.  This is why the introduction form
for polytypes $\epoly e \ts$ is able to introduce a \textit{fresh}
$\varepsilon$ variable.
\end{version}

\Xalistair[]{Something about how information can flow from let bindings to
use sites for principality checking}

The very purpose of annotation variables is to distinguish known polymorphic
polytypes from yet-unknown monomorphic ones as required by poly-directional
type ifnerence. 
%
With suspended constraint, any type information can be considered as known,
so that we do not need the annotation variables any longer.

Typechecking $\epoly e \ts$ is as simple as:
\begin{mathpar}
\cinfer {\epoly e {\tfor {\overline{\cvb}} \t}} \cva \eqdef
  \Parens{\cfor {\overline{\cvb}} \cinfer e \t}
\cand
   \cva = [\tfor {\overline{\cvb}} \t] 
\end{mathpar}
If $\e$ is already known to have the type $[\ts]$, then we can simply
instantiate it.  However, if the type of $\e$ is not yet know, \ie.  it is a
(possibly constrained) type variable $\tv$: then, we must wait.  So we
instead write the constraint as
\begin{mathpar}
\cinfer {\einst e} \cva \eqdef
    \cexists \cvb \cinfer e \cvb
\cand
    \cmatch  \cvb {\cva} (\lambda [\ts]. ~ \ts \leq \cva)
\end{mathpar}
since waiting is harmeless, as the syspended constraint may immediately be
discharged. 

By waiting for $e$'s type to be \emph{known}, we may ensure principal types
without annotation variables. However, we still need to explain what it
means for a variable to be \emph{known}, that is, give the semantics of
suspended constraints.


\section{Semantics of constraints}
\label{sec:semantics}


A na\"ive denotation semantics for suspended constraints would be to simply
apply the assignment of $\cva$ to $f$. More formally,
\begin{mathpar}
  \inferrule*
    {\semenv(\cva) = \overline{\t} \Fapp \\ \semenv_{\setminus
    \Delta}[\overline{\cvb := \t}] \vdash f(\overline{\cvb} \Fapp)} 
    {\semenv \vdash \cmatch \cva \Delta f}
\end{mathpar}

However, our informal solver would be incomplete with respect to this
semantics.  We show this by considering the constraint $\cexists \cv
\cmatch \cv \cv (\lambda \_.~ \cv = 1)$.  This constraint does not
unify $\cv$ outside the suspended constraint, but once it is scheduled,
the variable $\cv$ must be unified with the unit type $1$. So our
informal solver suggests that this constraint is unsatisfiable since
$\cv$ is never unified outside the match constraint. Yet we can use the
above rule to show that this constraint is satisfiable:
\begin{mathpar}
  \inferrule*
    {
      \inferrule*
        {
          (\cv \mapsto 1)(\cv) = 1 \\
          \inferrule*
            {(\cv \mapsto 1)(\cv) = 1} {\cv \mapsto 1 \vdash \cv
            = 1}
        }
        {\cv \mapsto 1 \vdash \cmatch \cv \cv (\lambda \_. ~ \cv
        = 1)}
    }
    {\cdot \vdash \cexists \cv \cmatch \cv \cv (\lambda \_.~ \cv = 1)}
\end{mathpar}

The dependency of suspended constraints can also be complex, as in the
following example: $\cexists {\cva, \cvb} \cmatch \cva \cvb {(\lambda
\_. ~ \cvb = 1)} \cand \cmatch \cvb \cva {(\lambda \_. ~ \cva = 1)}$.
Worse still, is that the binding location of $\cva$ may not even contain
the sufficient information to show that $\cva$ is realised.  For instance,
\begin{align*}
  \cexists \cvb \cvb = 1 \cand \cexists \cva \cva = \cvb \cand \cmatch \cva \Delta f
\end{align*}

Our conclusion is that the na\"ive semantics should be rejected in favour
for the semantics of our informal solver. Our intent with suspended
constraints is that the blocking variable should not be guessed out of thin
air, but deduced from the surrounding context without knowledge of the
constraint generated by the handler.

The idea, inspired by type systems that require principality (such as
FreezeML), is to stipulate that the only satisfying assignments for the
context of the suspended constraint requires $\cv$ to be assigned to some
type with a unique type constructor.
\Xalistair[]{This doesn't read entirely correctly, we want a unique
constructor for all assignments}. 

This motivates our next definition: the principal realisation of a type
variable $\cv$ by some constraint context $\cctx[-]$.

\begin{definition}
  A type variable $\cv$ is principally realised with the constructor $\F$
  by the context $\cctx[-]$, written $\cctx[\cv !] \rhd \F$,
  iff for all assignments $\semenv$, there exists an assignment $\semenvp$
  for the 
  hole such that $\semenv \vdash \cctx[\semenvp \vdash \ctrue]$ implies
  $\semenvp(\cv) = \overline{\t} \Fapp$
\end{definition}

\TODO
{Here, we not only rely on the definition of a constraint with a hole
$\mathcal{C}[-]$ but a judgement with a hole and some way to translate
between the two. We'll need formal definitions for this.}

\TODO
{Some examples of principal realisation}

As we've previously hinted, the semantics for a suspended constraint must
require that the surrounding context principally realises the matchee. This
is formally specified as:
\begin{mathpar}
\inferrule*
   {\cctx[\cva !] \rhd \F \\
    \semenv \vdash \cctx[\semenvp_{\setminus \Delta}[\overline{\cvb :=
      \t}] \vdash f(\overline{\cvb} \Fapp)] \\
    \semenvp(\cva) = \overline{\t} \Fapp}
   {\semenv \vdash \cctx[\semenvp \vdash \cmatch \cva \Delta f]}
\end{mathpar}

\TODO: precise declarative semantics of constraints. They should coincide
with the semantics of the typing rules.

Alistair has yet another proposal derived from his simplification of his
timestamp semantics.

\TODO
{derive precise declarative semantics for the language features of
Section 2.}

\begin{mathparfig}%
  {fig:constraint-semantics}%
  {Semantics of constraints (without suspended constraints)}
  \begin{array}{rrl@{\hspace{8em}}l}
    \semenv
      & ::=
      & \cdot \mid \semenv, \alpha := \tau
      & \text{Semantic environment}
    \\[2em]
    \square,\semmetav &&& \text{Linear constraint meta-variable}
    \\
    \semmetaenv
      & ::=
      & \cdot \mid \semmetaenv, (\semenv \vdash \semmetav)
      & \text{Linear meta-variable environment}
    \\[2em]
  \end{array}
\\
  \infer
  {\semenv(\tone) = \semenv(\ttwo)}
  {\cdot; \semenv \vdash \tone = \ttwo}

  \infer
  {\semmetaenv_1; \semenv \vdash \cone
   \and
   \semmetaenv_2; \semenv \vdash \ctwo}
  {\semmetaenv_1 \uplus \semmetaenv_2; \semenv \vdash \cone \cand \ctwo}

  \infer
  {\semmetaenv; \semenv[\alpha := \tau] \vdash \c}
  {\semmetaenv; \semenv \vdash \cexists \cv \c}

  \infer
  { }
  {(\semenv \vdash \semmetav) ; \semenv \vdash \semmetav}
\end{mathparfig}

Meta-substitution lemma: if $\semmetav$ occurs exactly once in $\c$, then
$\semmetaenv, (\semenvp \vdash \semmetav); \semenv \vdash C$ and $\semenvp
\vdash \cp$ if and only if $\semmetaenv; \semenv \vdash \c[\semmetav :=
\cp]$. 

Convention: we write $\c[\cp]$ for $\c[\square := \cp]$. In particular
$\c[\square] = \c$ when $\square$ occurs once in $\c$. 

Next steps:
\begin{itemize}
\item introduce the condition
  $\c[\cv !]$:
  \begin{mathpar}
    \begin{array}{l}
    \exists! \F, \quad \forall \semenv, \semenvp,\\ \qquad (\semenvp \vdash
    \square); \semenv \vdash \c[\square] \implies
    \mathsf{head}(\semenvp(\cv)) = \F 
    \end{array}
  \end{mathpar}
\item and finally the global rule
  \begin{mathpar}
    \infer
    {\c[\cva !]
      \and \semenvp(\cva) = \overline{\t} \Fapp
      \and \semmetaenv, (\semenvp, \overline{\cvb := \t} \vdash
      \square)\vdash \c[f(\overline{\cvb} \Fapp)] } 
    {\semmetaenv, (\semenvp \vdash \square); \semenv \vdash \c[\cmatch \cva \Delta f]}
  \end{mathpar}
\end{itemize}

The intuition for the judgment $\semmetaenv, (\semenvp \vdash \square);
\semenv \vdash \c[\cp]$ is that it is a proof of satisfiability of
$\c[\square := \cp]$ with valuation $\semenv$, where we furthermore know
that the $\cp$ sub-derivation has the valuation $\semenvp$. Note that, after
the the global rule is added, the derivations of this judgment are not
syntactically directed by $\c$ anymore.

\section{The language}
\label{sec:language}

% Running example: tuple projection disambiguation 

% We need a spec, but this itself is hard

In order to show our approach is sound and complete with respect to our
constraint generation translation, we must first define a formal
specification in the form of a calculus and accompanying type
system. Surprisngly, identifying an appropriate declarative type system to
use as a specification is itself an interesting problem!

% Why is it hard? Examples of prev work:

Much work has been done on various extensions of \ML with features such as
static overloading \citep{TODO} and first-class polymorphism
\citep{TODO}. However, many of these approaches either fail to ensure
principality or require intricate menachisms -- such as labelled types -- to
do so.

% Why do naive approaches not guarantee principal types. 

Na\"ive specifications, though accessible, often lack principal types. Take
overloaded tuple projects \textit{\`a la \SML}: an expression $\efield e j$
governed by the rule:
\begin{mathpar}
  \infer 
    {\Gamma \vdash e : \Pi_{i = 1}^n \ti \and 1 \leq j \leq n}
    {\Gamma \vdash \efield e l : \tj}
\end{mathpar}
admits many typings, as any tuple of at least size $j$ satisfies the
premise. This multiplicity undermines principality.

% Partial annotations (shapes) to the rescue!

Our first insight is that partial type annotations often sufficies to
recover principality. We refer to these annotations as \textit{shapes}. For
example, explicitly annotating the tuple projection with its arity, as in
$\exfield e n j$, disambiguates the expression:
\begin{mathpar}
   \inferrule* 
      {\Gamma \vdash e : \Pi_{i = 1}^n \ti \\ 1 \leq j \leq n }
      {\Gamma \vdash \exfield e n j : \tj}
\end{mathpar}
Yet, requiring users to write such annotations manually is impractical. 

% Inference of shapes

Our solution is to permit \textit{inference} of shapes, provided that the
inferred annotation is \textit{uniquely determined by context}.  This
approach ensures principality by construction and yields a clear
specification with predictable annotability requirements for the everyday
programmer.

% A note on stratified type inference 

The idea of leveraging a principled core language with inference of shapes
is not novel. \textit{Stratified type inference} [??]  pursued a similar
goal (in the context of generalized algebraic data types). It operates in
two phases: the first phase transforms the program into an intermediate
language and generates shapes using \textit{bidirectional typing}; the
second performs \HM type inference for the intermediate language. While the
second phase is sound, complete and enjoys principal types, the overall
system remains incomplete due to the heuristic nature of the first phase.

% How do we specify our intuition?

Specifying the notion of unique inferrability from context is
tricky. Luckily, we can leverage many of the formal methods developed in the
previous section -- in particular, the manipulation of contexts within the
type derivation -- to prove uniqueness of shapes. Allowing us to
systematically replace the ambiguous constructs with their disambiguated
counterparts:
\begin{mathpar}
  \inferrule* 
    {E[e \mathop{!} \Pi^n] \\ \Gamma \vdash E[\exfield e n j] : \t}
    {\Gamma \vdash E[\efield e j] : \t}
\end{mathpar}
The intuition here is that the context $E$ is sufficiently large to
constrain the shape of $e$'s type as a tuple of size $n$.
\Xalistair{Some notation trickiness here }

% Limitations

The cheif limitation of our approach is that disambiguation can still
require explicit annotations in inherently ambiguous scenarios. One solution
is the use of sensible `defaults', such as resolving $\efield e j$ by
assuming the tuple has arity $j$. While appealing, such defaults compromise
principality. We return to this pragmatic tradeoff in Section
\ref{sec:implementation}.

\[
\begin{bnf}
  \bnflabel{Terms} & 
    \begin{bnfalign}
      e \bnfis & x \mid () \mid \efun x e \mid \eapp \eone \etwo \mid \elet x \eone \etwo \mid (e : \t) \\
          \mid & \erecord { \overline{l = e} } \mid \efield e l \mid \exfield e \F l \\
          \mid & (\eone, \ldots, \en) \mid \efield e j \mid \exfield e n j \\
          \mid & \epoly e { \exists \overline{\tv}. ~ \sigma } \mid \einst e \mid \exinst e {\overline{\tv}} \sigma
    \end{bnfalign}
    \\
  \bnflabel{Types} & 
    \begin{bnfalign}
      \t \bnfis & \tv \mid 1 \mid \tone \to \ttwo \mid \overline{\t} \Fapp ~ \mid \Pi_{i = 1}^n \ti \mid [\sigma] 
    \end{bnfalign}
    \\ 
  \bnflabel{Type schemes} & \sigma \bnfis \t \mid \tfor \tv \sigma \\
  \bnflabel{Contexts} & \Gamma \bnfis \cdot \mid \Gamma, x : \sigma \mid \Gamma, \tv 
\end{bnf}
\]

\paragraph{Terms}

In Figure ??, we give the grammar for our calculus. Terms include all of the \ML calculus: 
variables $x$, the unit literal $\eunit$, lambda abstractions $\efun x
e$, applications $\eapp \eone \etwo$, annotations $(e : \t)$ and let
bindings $\elet x \eone \etwo$. Our extensions include:
\begin{enumerate}
  \item Constructor and record label disambiguation, modelled using record
  literals $\erecord { \labone = \eone; \ldots; \labn = \en }$ and field
  projections $\efield e l$.

  \item Tuples $(\eone, \ldots, \en)$ with overloaded tuple projections
  $\eproj e i$.  

  \item For semi-explicit first-class polymorphism, we have the boxing
  construct $\epoly e \sigma$ and the unboxing construct $\einst e$.
\end{enumerate}
Each construct that endangers principality has an explicitly annotated
counterpart.  

\paragraph{Types}

As usual, types are split into monotypes (or just types) and type
scehems. Types $\t$ include the variables $\tv$, the unit type $1$,
functions $\tone \to \ttwo$, $n$-ary products $\Pi_{i = 1}^n \ti$ and
nominal record types $\overline{\t} \Fapp$. Types schemes $\sigma$ extend
monotypes allowing the universal quantification of zero or more type
variables $\tfor {\overline{\tv}} \t$. We write $\tv, \beta, \gamma$,
etc. for type variables. We also have polytypes
\textit{without the labels} $[\sigma]$ as monotypes. 

\paragraph{Contexts}

Contexts $\Gamma$ are an ordered sequence of expression variable typings $x
: \sigma$ and universally quantified type variables. Our presentation of \ML
differs from most traditional ones that permit type variables to ``float
around''. Instead, we explicitly track the type variables in scope in the
type context $\Gamma$.

\paragraph{Typing}




\begin{mathpar}
  \inferrule*
    {x : \sigma \in \Gamma}
    {\Gamma \vdash x : \sigma}

  \inferrule*
    {\Gamma \vdash \tone \\ \Gamma, x : \tone \vdash e : \ttwo }
    {\Gamma \vdash \efun x e : \tone \to \ttwo}

  \inferrule* 
    {\Gamma \vdash \eone : \tone \to \ttwo \\ \Gamma \vdash \etwo : \tone}
    {\Gamma \vdash \eapp \eone \etwo : \ttwo}
  
  \inferrule* 
    {\;}
    {\Gamma \vdash () : 1}

  \inferrule*
    {\Gamma \vdash e : \t \\ \Gamma \vdash \t}
    {\Gamma \vdash (e : \t) : \t}

  \inferrule*
    {\Gamma, \tv \vdash e : \sigma \\ \tv \disjoint \Gamma}
    {\Gamma \vdash e : \tfor \tv \sigma}
  
  \inferrule*   
    {\Gamma \vdash e : \sigmaone \\ \Gamma \vdash \sigmaone \leq \sigmatwo}
    {\Gamma \vdash e : \sigmatwo}

  \inferrule* 
    {\Gamma \vdash \eone : \sigma \\ \Gamma, x : \sigma \vdash \etwo : \t}
    {\Gamma \vdash \elet x \eone \etwo : \t}

  \inferrule*   
    {\forall 1 \leq i \leq n, \; \Gamma \vdash \ei : \ti}
    {\Gamma \vdash (\eone, \ldots, \en) : \Pi_{i = 1}^{n} \ti}

  \inferrule* 
    {\Gamma \vdash e : \Pi_{i = 1}^n \ti \\ 1 \leq j \leq n}
    {\Gamma \vdash e.n.j : \tj}
    
  \inferrule* 
    {\Gamma \vdash e : \sigma[\overline{\tv := \t}] \\
     \Gamma \vdash \overline{\t}}
    {\Gamma \vdash \epoly e { \exists \overline{\tv}. ~ \sigma } :
      [\sigma[\overline{\tv := \t}]]}

  \inferrule* 
    {\Gamma \vdash e : [\sigma[\overline{\tv := \t}]] \\ \Gamma \vdash
    \sigma[\overline{\tv := \t}] \leq \tp} 
    {\Gamma \vdash \exinst e {\overline{\tv}} \sigma : \tp }

  \inferrule*    
    {E[e \mathop{!} \Pi^n] \\ \Gamma \vdash E[\exfield e n j] : \t}
    {\Gamma \vdash E[\efield e j] : \t}

  \inferrule* 
    {E[e \mathop{!} \exists \overline{\tv}. ~ \sigma] \\ \Gamma \vdash E[\exinst e {\overline{\tv}} \sigma] : \t}
    {\Gamma \vdash E[\einst e] : \t}
\end{mathpar}

The shape of a type scheme is $\exists \overline{\tv}. ~\sigma$, where
$\overline{\tv}$ gives the closure.  We write $\langle \overline{\t},
\exists \overline{\tv}. ~ \sigma \rangle$ for the type scheme
$\sigma[\overline{\tv := \t}]$.

We also add a well-typed hole judgement (this is analogous to using $\ctrue$
in suspended constraints semantics):
\begin{mathpar}
  \inferrule*   
    {\Gamma \vdash e : \tp}
    {\Gamma \vdash \{ e : \tp \} : \t} 
\end{mathpar}
Then we can define: 
\begin{align*}
  E[e \mathop{!} s] := \exists! s'.\; &s = s' \wedge \forall \Gamma,
  \Gamma', \t, \tp. \\ 
  &\quad \Gamma \vdash E[\Gamma' \vdash \{ e : \tp \} : \_] : \t \implies
  \textsf{shape}(\tp) = s' 
\end{align*}
What is a shape? 

\begin{align*}
  s \bnfis \Pi^n \mid \exists \overline{\tv}. ~ [\sigma]
\end{align*}
Shapes are equal modulo alpha equivalence / removal of redundant variables. 
Shapes are only defined for n-ary tuples and polytypes. 


\section{Solving constraints}
\label{sec:solving}

\TODO
{a declarative solver.}

\section{HM Prototype and discussion}
\label{sec:implementation}

\subsection{Default clauses}

\section{Semi-unification Prototype}

Rank-2 polymorphic type inference with semi-unification!

\Xdidier{When you do rank-2 unification, you don't need a 'let' anymore
because 'lambda' is enough. You have rank-2 types but you cannot bind
them. It's not as exciting as I thought it would be.}

\section{Related work}
\label{sec:related-work}

\subsection{Qualified types}

\Xdidier {The following paragaph is just take from the old introduction}

Qualified types~\citep*{TODO} represent additional knowledge on a type. They
are used in particular in Haskell type-classes, where a constraint
$\mathsf{Show}~\alpha$ represents the fact that a part of the inferred term
needs to print values of type $\alpha$. A constraint on a ground type such
as $\mathsf{Show}~\mathsf{Int}$ can be resolved to a known printer. But if
the undetermined variable $\alpha$ becomes generalizable in a type $\tau$,
we get a type-scheme $\tfor{(\alpha \mid \mathsf{Show}~\alpha)}{\tau}$ that
also includes the type-class constraint.

DRAFT: Type-classes let each use-site choose a different typeclass instance
-- but this implies dictionary-passing or specialization. Sometimes we want
the choice to be shared by all use-sites:


\subsection{Suspended constraints in \textsc{OutsideIn}}

\Xalistair{They first solve simple constraints (existentials,
unification). Then they solve ``... constraints'', with implication
constraints, which corresponds to the case of GADT matches. Crucially, they
abandon local let generalization.}

\subsection{Suspended constraints in dependent-type systems}


\subsection{Bi-directional type inference}

\Xgabriel{Our approach should have better properties for disambiguation, but
which ones?} 

In the simply-typed case our system works better. But with generalization
you can have more issues.

\Xdidier{Should we try to extend what we did to predicative polymorphism?}

\Xalistair{Not ready yet and would probably need too much space.}

\Xgabriel{CoreML + bidirectional disambiguation of constructors?}

\TODO
{do we understand what to say precisely about bidirectional type inference?}

\subsection{Principality tracking in OCaml}

\Xdidier{The point of that is to enforce a directional type inference that
is based on let-bindings. When we check that the level is generic, we check
that we already ``closed'' this thing, it is an earlier 'let'
binding. Because we are omni-directional, we don't have principality issues
anymore -- except with default rules. We are principal by construction, we
never make any choice.}

\Xdidier{Principality tracking is making the choice that we are going to
make a directional let-binding-based type inference. We get rid of that, we
are omni-directional, and don't have any principality issues.}

\Xgabriel{We should not claim too much if we don't understand default
clauses well enough. It could people the impression that we hide the issue
under the carpet.}

\Xdidier{First a declarative/principal system, and then non-principal
heuristics to refine it, a two-phase process. It's fine.}

\section{Future work}
\label{sec:future-work}

% \begin{acks}
% \end{acks}


%% \bibliographystyle{ACM-Reference-Format}
\bibliography{suspended}

\end{document}

% LocalWords:  omnidirectional typecheck polymorphism Hindley Milner kinded
% LocalWords:  GADTs typechecked codomain typechecking subexpressions Bodin
% LocalWords:  monomorphic subexpression Dunfield Riboulet jfla subtyping
% LocalWords:  greek Chargueraud typable monotype polytype Garrigue Remy
% LocalWords:  impredicative polytypes
