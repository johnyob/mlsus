%; whizzy section

%% Leave the above line for didier
%% No macros before \documentclass

\documentclass[acmsmall,screen,nonacm]{acmart}

\input{suspended.cfg}
\newcommand{\acmart}{\True}
\usepackage{suspended}

%% \Xfirstname defined in {mycomments}
%% Use either
%%   \Xfistname[text to comment]{your comment on the text}
%% or
%%   \Xfirstname{free comment}
%% Uncomment this line to hide all comments.
% \UNXXX{}

\usepackage{marginnote}

\title{Omnidirectional type inference
  for fragile \ML type features
   %% for \ML extensions
   %% with suspended constraints
   %% for static overloading and semi-explicit first-class polymorphism
}

\begin{document}

\begin{abstract}
We propose a new concept of \emph{omnidirectional} type inference, which is 
the ability to resolve \ML-style typing constraints in disorder, by
contrast with all known implementations that always typecheck the
bindings before the bodies of let-expressions.
%
This relies on two technical devices: \emph{partial type schemes}
and \emph{suspended match constraints}. Partial here means  the
possibility of taking an instances of a type scheme that is not yet
completely solved with a mechanism to tack and update their instances when
the type scheme is refined, incrementally.
\emph{Suspended} match constraints means a mechanism to delay the resolution of
some constraints which typically require non-local not-yet-available
contextual information and to be discharged then when we have learned enough
information from the resolution of other constraints, typically when some type
variables have been instantiated.
%
We illustrate the benefit of omnidirectional type inference for several
advanced features present in the \OCaml language, including static
overloading of record labels and datatype constructors on the one hand and
semi-explicit polymorphism on the other hand. 
\end{abstract}

\maketitle

\Xdidier
  {\emph{first-class polytypes} is not best,
   \emph{semi-explicit first-class polymorphism} is better but too long.
   \emph{semi-explicit polytmorphism} or 
   \emph{first-class polytmorphism} are both a but inexact....
   Which one should we use?
   Alistair says polytypes doesn't fit the definition of $\mathsf{rank}(k)$
  types and I agree.} 

\Xgabriel{I would use ``polytypes'' for concision, after defining the concept in more details on the first occurrence.}

\Xalistair
  {Also @Didier, what are you using to format / trim lines?
   Not sure this is the question, but I edit in emacs and use
   fill-paragraph with a length of 76.}

\section{Introduction}

ML polymorphism, also known as Hindley-Milner type system, has been a very
sweet spot in the design space of strongly typed programming languages, as
it enjoys the principal type property and more importantly is closed by
common program transformations such as let-contraction and let-expansion,
and permutation of the order or arguments.

Over the years, many extensions of \ML have been proposed. Some of the
new constructs, such as extensible records with row-polymorphism,
higher-kinded types, or dimensional types, fit nicely into the \ML
framework. Others such as GADTs, semi-explicit polymorphism,
first-class modules or static overloading of datatype constructors and
record fields, are \emph{fragile}, since they require to twist the
ML-style specification with some direct or indirect algorithmic flavor
in order to preserve \emph{principality}. This (more or less ad-hoc)
restriction in the specification is designed especially to reject
examples that would break completeness, but it also rejects many
others.\Xgabriel{I would like to explain more here, it is not obvious
 why completeness-breaking examples also endanger principality.}
Unsurprisingly, they have historically posed challenges to the
preservation of principal types.

Fragile conststructs have all in common that their syntax includes
explicit type annotations which may sometimes but not always be
omitted. The return type of overloaded data constructors can be
annotated; polymorphic expressions can be annotated with a type
scheme; first-class module expressions with a module signature; for
GADTs, the type of the \texttt{match} scrutinee can be annotated to
a rigid type which will be refined by type equalities in each
branch. To each fragile construct corresponds a robust construct where
this type annotations is mandatory. Those fit nicely into the \ML
framework, but are significantly more cumbersome to use. Fragile
constructs can be defined by elaboration into their robust
counterpart. The elaboration determines which annotations can be
omitted and rebuilt from context, a point of view already taken in
stratified type inference~\cite
{Pottier-Regis-Gianas/stratified@popl06}.
%
The difficulty of fragile constructs lies in finding a specification
that is sufficiently expressive, principled, intuitive for the user,
and for which we have a complete and effective elaboration algorithm.

The solutions proposed so far all enforce some ordering in which type
inference is performed, which can then be used to propagate both inferred
types and user-provided type annotations as \emph{known} types that can be
used for disambiguation and enable the omission of some annotations.

\paragraph{Bidirectional type inference}

Bidirectional type inference is a known alternative to unification for
propagating type information. It can be presented by adding an optional
expected type in addition to the expression to be typechecked and the
context in which it should be typechecked to the type inference algorithm.
Type inference is said in checking mode the expected type is present, and in
(the usual) inference mode when it is absent. The checking mode allows to
pass source type annotations but also previously inferred types such as the
codomain of a function in an application position during type inference of
the arguments.
%
Bidirectional type inference is a actually a framework that must be
instantiated by a particular choice of modes for each of the language
construct, while there is no optimal combination of modes for the whole set
of language constructs. Once modes have been fixed, there are usually
principal solutions to the type inference problem, but with respect to a
specification that made non-principal choices.
%
Bidirectional type inference has been largely used for languages with
higher-rank polymorphism or subtyping~\TODO{citations}. Still, both \OCaml
and \Haskell use a limited form of bidirectional type inference.
%
Besides, bidirection type 

\Xalistair{I think we could critise bidirectional more? Aside from a forced
order, it suffers from scalability issues (rule duplication and lack of
clear annotability spec)}\Xdidier{I'd be happy with more criticism, indeed.
I am not sure about what you mean by the the scalaby issue however.  Lack of
clear anotability specl is fragile: we could say the same with our proposal
in the remaining cases where it is incomplete. }


\paragraph{Poly-directional type inference}

\OCaml uses another mechanism for propagation of type
information, based on polymorphism, which actually enforces an ordering in
the typechecking of let-bindings, where the (polymorphic part of) a the
bound expression must be typed before the body of the let-expression.  \Xgabriel[We
call it \emph{poly-directional} type inference, which is to be understood as
the direction of \emph{polymorphism} and not in multiple directions.]{Yeah, not a good term. Use-directional? Def-use-directional? Instance-directional? Let-directional?}
Initially introduced with the extension of \ML with semi-explicit
first-class polymorphism~\cite {Garrigue-Remy/poly-ml}, it has also been used
for the overloading of record fields, which we use for illustration here
as it is simpler.\footnote {Semi-explicit polymorphism will be discussed
in \cref{sec/constraints/polytypes}.}

The user may define two records with overlapping fields:
\begin{program}[input]
type one = {x : int}
type 'a two = {x : 'a; y : int}
\end{program}
This can be thought of as defining two abstract types \ocaml{foo} and
\ocaml{bar} with two accessors \ocaml{(_:one).x} and \ocaml{(_:two).x} of
respective types \ocaml{(one -> int)} and \ocaml{(two -> int)}, among
others.
%
In the absence of overloading the latter definition would hide the former
and we fall back in \ML. With overloading enabled, both definitions are
visible and the compiler must statically choose one of them.
%
The question is which one to choose or, equivalently, which type to infer
in the following three expressions? 
\begin{program}[input]
let e_1 r = r.x
let e_2 = let r = {x = 1} in r.x
let e_3 = (fun r -> r.x) {x = 1} 
\end{program}
To be able to resolve the overloading, the type of the projection should be
\emph{known} to be either \ocaml{one} or \ocaml{'a two} when typing the
projection.  Expression \ocaml[indices]{e_1} is clearly ambiguous since
there is no clue on the type of \ocaml{r} and its typechecking
fails\footnote {In fact, \OCaml uses a default resolution strategy instead
of failing when the type is ambiguous, which is to use the last
definition. Here, this will amount to chose the type \ocaml{'a two}
still leading to a failure since the record passed to the function is
actually of type \ocaml{one}. But we shall ignore default resolution
strategies for the moment.}\footnote {When running examples in \OCaml,
\OCaml should be always be called with option \texttt{-principal}, since by
default \OCaml does not quite follow the theory for efficiency reasons.}.
%
By contrast, the only possible
type for \ocaml{r} is \ocaml {one} in both \ocaml{e_2} and \ocaml {e_3}.
Still, there is a difference.  Indeed, the type of \ocaml{e_2} is considered
to be unambiguous, while the type of \ocaml{e_3} is ambiguous. To
understand why \ocaml{e_3} fails, consider the equivalent versions of
\ocaml{e_3} where \texttt{@@} and \ocaml{|>} are the application and the
reverse application functions.
\begin{program}[input,escapechar={}]
let e_4 = (fun r -> r.x) @@ {x = 1} 
let e_5 = {x = 1} |> (fun r -> r.x) 
\end{program}
\OCaml  does not make any difference between
\ocaml[indices]{e_1}, \ocaml[indices]{e_2}, or \ocaml[indices]{e_3}
and consider all subexpressions in an application, including the function
and all arguments as being inferred simultaneously, until they are
let-bound. More precisely, polymorphic types are considered to be known
while monomorphic types are consider to be unknown (or rather
not-yet-known). In \ocaml{fun r -> r.x}, the type of \ocaml{r} in
in the subexpression \ocaml{r.x} is not-yet-known even if equal to
\ocaml{one} since this is the lambda-bound variable \ocaml{r}, hence in the
typing context, hence it is monomorphic.\footnote{Technically, all type
nodes are annotated with a special of variables called annotation variables
$\av$, so that we may distinguish between the polymorphic binding $\xa : 
\all \av. \t^\av$ that binds $r$ to the known (raw) type $\t$ and the monomorphic
binding $\xb : \t^\av$ that binds $\xa$ to the unknown (raw) type $\t$.}


\paragraph{Limitations of directional type inference}

Directional type inference have been in used in different languages and
somehow proven to work. Still, each of them is unsatisfactory.  The main
critics of bidirectional type inference is to make a somewhat arbitrary
choice in the specification of the flow of information, typically in
applications: should the the function be typed first and its codomain be
used to improve the typing of its argument or, on the opposite, should the
argument be typed first and be used as the type of the codomain of the
function?  There are examples when the former is a better choice and others
when the latter is preferable---but typing rules have to choose one of the
two alternative forever.  One may also allow information to flow between
multiple arguments passed to the same function, or on the opposite disallow
it entirety. Again, there is no best choice. We eventually have a complete
algorithm with respect to a spec that made somewhat arbitrary choices.

Poly-directional type inference seems a better fit for \ML, since it relies
on polymorphism which is the essence of \ML. Still, it does not even allow
to propagate user-provided type annotations!  \Xalistair{Examples here?}
Thus \OCaml, which use poly-directional type inference as the primary
mechanisms also added a weak form of bidirectional propagation.
\Xalistair{Example of bidirectionality in OCaml here?} Besides, the
implementation of poly-directional type inference has an algorithmic cost:
for technical reasons, variable annotations must un-share types
(from acyclic graphs as naturally produced by unification to trees), which
may increase the size of types and the cost of type inference. For that
reason, the implementation of \OCaml cheats and is incomplete by default and
the user must explicitly pass the \texttt{-principal} flag to require the
more expensive computation of principal types when desired.

\paragraph{Incompleteness of directional type inference}

Both bidirectional and poly-directional type inference rely on an ordering
for type propagation that is partially specified, explicitly or implicitly,
to take advantage of user-provided type annotations and already-inferred types
to alleviated the need for extra annotations.

While they come with complete algorithms with respect to their
specification, their specifications include some choices that are subjective
and may sometimes look arbitrary.

Indeed, they all reject examples as ambiguous when sometimes they would be a
unique well-typed solution, as seen in the limitation of direction type
inference.  The first sources of incompleteness results from the ordering
enforced between the resolution of core-language constraints that prevents
some core information to be taken into account when resolving ambiguities.

However, the resolution of each ambiguity is another source of
incompleteness, which is actually two-fold:
\begin{enumerate}
\item  
  the resolution is not always taking all contextual type information into
  account.  
\item
  each ambiguity is solved independently of other yet unresolved ones;
\end{enumerate}
Let us consider two more examples: 
\begin{program}[error]
let e_6 r = (r.x, r.y)
let e_7 r = r.x.x
\end{program}
\OCaml fails on both examples as ambiguous with poly-directional inference.
However, \ocaml{r} can only be of type \ocaml{int two} in \ocaml{e_6}, which
is an instance of the former case of incompleteness, which should wishfully
be eliminated. Our proposal with omnidirectional inference succeeds
\Xgabriel{Does it? My implementation with Olivier would reject this example,
  because it is not robust to the addition of additional type definitions in
  the context. Even where there is a single type declaration with a given
  field/constructor we require an annotation.}

\Xgabriel{I would prefer to have less questionable examples, where type information comes from the body of the let-binding rather than closed-world reasoning. This could also be helpful to give a concrete intuition of partial type schemes.}

Omnidirectional still fails on \ocaml{e_7}, which is an instance of the
latter case of incompleteness. This is actually a reasonable design to keep
type inference effective, since the complexity of general overloading is
NP-hard even in the absence of let-polymorphism, as show by an encoding of
3-SAT problem as shown by~\citet
{Chargueraud-Bodin-Dunfield-Riboulet/jfla2025}.


\paragraph{Omnidirectional type inference}

The idea of \emph{omnidirectional} type inference is to take all program
core-expression constraints into account before resolving ambiguities. This
requires typing constraint to be solved in disorder.

This approach eliminates the first source of incompleteness we mentioned
above. Our proposal with omnidirectional type inference will correctly type
all examples above (from \ocaml{e_1} to \ocaml{e_6})\ignorespaces \INFO {See
  \url
  {https://gitlab.inria.fr/remy/semiun/-/blob/main/su/examples/record_fields.ml}} ---without
any default strategy mechanism.

In absence of polymorphism, type inference is solely based on unification
constraints which can be solved in any order; omnidirectional inference is
natural and easy to implement. The difficulty originates from \ML
let-polymorphism for which all known implementations choose to always infer
the type of a let-binding, turn it into a type scheme that is assigned to
the let-bound variable to extend the typing environment in which the body is
then typed. The Hindley-Milner algorithm $\mathcal{J}$, one of its variant
$\mathcal{W}$ or $\mathcal{M}$, or more flexible constraint solving
algorithms all follow this strategy.

However, it is not mandatory. In fact, typing constraints as presented
in~\cite {Pottier-Remy/emlti} can be instantiated before they are fully
solved, although this comes at the cost of duplicating some computation.
Still, this shows that the let-binding ordering is a convenience, not a
requirement.

Omnidirectional type inference for \ML thus relies on two key technical devices:
\begin{enumerate*}
\item
  We introduce \emph{suspended match constraints} as a way to suspend
  ambiguity resolution until sufficient information has been found from
  context so that they can be discharged and hopefully resolved.
\item
  We work with partial types schemes, \ie we the ability to instantiate type
  schemes that are not yet fully determined and consequently revisit their
  instances when their are being refined, incrementally. This allows
  inferring parts of a \texttt{let}-body to disambiguate its definition,
  without duplicating constraint-solving work.
\end{enumerate*}

These technical devices are introduced once and for all -- in a general
framework of constraint-based type inference. Each fragile \ML construct can
then be implemented as suspended constraints that expand to its robust
counterpart once the annotation has been inferred. This generality comes at
a cost, which is that everything is hard:
\begin{itemize}
\item Implementing partial types schemes (without duplicating
  constraint-solving work) is hard.
\item Giving an adequate semantics for suspended constraints is hard, as we
  must capture declaratively the intuition that some type information must be
  \emph{known} rather than \emph{guessed}.
\end{itemize}
In return, we found that the declarative semantics of suspended constraints
immediately suggests a systematic way to present user-facing typing rules
for each fragile construct, for which the implementation is correct and
complete.

\subsubsection* {Plan}

The rest of the paper is organized as follows.
\begin{enumerate*}[label={}]
\item
  In \cref{sec:constraints}, we give an overview of suspended constraints
  and their application to various extensions for \ML.
\item
  In \cref{sec:semantics}, we describe suspended constraints and their semantics.
\item
  In \cref{sec:language}, we define an extension of \ML featuring static
  overloading and semi-explicit first-class polymorphism along with its
  typing rules. Here we give the precise definition of the constraint
  generation translation and state the theorems of soundness and
  completeness.
\item
  In \cref{sec:solving}, we provide a formal definition of our constraint
  solver as a series of non-deterministic rewriting rules.
\item In \cref{sec:implementation}, we describe an efficient implementation
  of suspended constraints. We discuss some extensions of suspended
  constraints that we have prototyped but whose theory is less clear.
\item
  In \cref{sec:related-work} and \cref{sec:future-work}, we discuss related
  and futre work.
\end{enumerate*}

\Xdidier {I think we should
\begin{itemize}
\item
  Move the typing rules before the semantic of constraints.  It is simpler
  and far less technical
\item
  Move the polytype (2.2) story to the introduction after the record field
  story. 
\end{itemize}
}

\subsubsection* {Our contributions}

Our contributions are 
\begin{enumerate*}
\item
  A novel \emph{omnidirectional} type inference technique for
  extensions of \ML with advanced features, based on two new devices,
  suspended constraints and partial type schemes;

\item A declarative semantics of suspended constraints that captures the
  idea that they wait on information that must be propagated from the
  context, not \emph{guessed}.

  This includes, in particular, a new declarative caracterisation of
  \emph{known} type information.

\item
  A complete yet efficient constraint-solving type inference algorithm.

\item New declarative type systems for static overloading of constructors
  and fields, and for semi-explicit first-class polymorphism, and their
  implementation using suspended constraints.
\end{enumerate*}

\section{Suspended constraints}
\label{sec:constraints}


\TODO{PLAN: Only the constraint syntax, not constraint-solving yet. The
point is to show examples in a concrete way.}
\Xalistair{Some of this probably belongs in the intro @Didier, but I didn't
want to conflict with your work}

\Xdidier {We need examples before we did in the technical details.
But since this section is still partly informal---no semantics, we may
then still consider as part of the intro and not described 
suspended constraints earlier} 

\Xdidier {The question is where do we put examples of programs that
do not typecheck in \OCaml, are expected to typecheck and that we can
typecheck (without explaining how they typecheck) ?  I gave one example for
record fields in the intro.  Should I also introduce semi-explicit
polymorphism and give an example in the intro?}

\Xalistair{I think it makes sense that this is the intro continued. As for 
examples, I think the intro has enough. We should put them in the relevant
subsections  in this section.}\Xdidier {Or the other way round: I think we
need examples early enough. Or keep it as is and say in the intro that we
leave polytype examples, which are more involved for this section.}

% Constraint-based approach

Our insights into directionality come by approaching type inference from a
\textit{constraint-based} \citep{TODO} perspective, in which type inference
is split into two stages, mediated by the language of \textit{constraints}
\citep{TODO}.  In the first stage, a term $\e$ is translated into a constraint
$\C$ which is satisfiable if and only if the term $e$ is well-typed. In the
second stage, the generated constraint $\C$ is solved (or shown to be
unsatisfiable).

% HMX 

A foundational approach to constraint-based type inference is \HMX
\cite{Odersky-Sulzmann-Wehr@tpos}---the Hindley-Milner system parameterized
by a constraint domain $X$, allowing constraints to appear within types. Our
work is inspired by the presentation of HMX by \citet{Pottier-Remy/emlti}
who fully decoupled constraint generation from constraint solving.  Their
key insight for the reduction of type inference to constraint solving is the
inclusion of term variables in the language of constraints.

% Life is hard
\Xalistair{This paragraph now feels out of place -- nonetheless I think we
need state how difficult this was :/}\Xdidier {It could be move to the
introduction}
Formulating a suitable constraint language for omnidirectional type
inference, a (provably) correct constraint translation for our extensions,
and an efficient, sound and complete solver involves several difficulties.
\Xdidier
{I think what is difficult is (1) the semantics of constraints (2) the
efficient implementation, rather than just a sound and complete
implementation. In particular, efficiency requires to use ranks and an
ordering on the way constraint are solve.}

\Xdidier
{Beside we have no proof that our implementation is efficient. In fact, I
think is not cheap: Assume that a type scheme of size $n$ is solved
incrementally by adding one node at a time and that every time we add a node
we propagate it to all its instantiations. Currently, each propagation with
redo the job, ignoring what had already be done ealier. Hence you get a
factor of $n$ complexity that need not necessarily be there. Plus
constraints are copied before being simplified. Another factor...}
\Xalistair{The spec for the solver and my solver impl does not suffer from this, 
it is able to track which new nodes need propagating and leaves the rest
untouched.}\Xdidier {This is not what you told me when we talk. There may be
misunderstanding. We should discuss it. However, this is not so important.
My point about efficient implementation is that we do not have anything to
validate that the implementation is efficient for the moment.}

\begin{bnffig}[t]%
  {fig:constraint-syntax}%
  {Syntax of types and constraints}
\entry[Type variables]{\cva, \cvb, \cvc}{}{}
\\
\entry[Constraint types]{\t}{
     \cv \mid \overline{\t} \Fapp ~ \mid \ldots
}\\
\entry [Type constructors]{\Fapp}{
         \tunit
  \and   \cdot \tarrow \cdot
  \and   \dots
}\\
\entry[Constraints]{\C}{
        \ctrue
  \and  \cfalse
  \and  \Cone \cand \Ctwo
  \and  \cexists \cv C
  \and  \cunif \tone \ttwo
  \nextline
  \and  \cletin x {\clam \cv \Cone} {\Ctwo}
  \and  \cinst \x \t
  \nextline
  \and  \cmatch \cv \D  \hf
}\\
\entry[Shapes] \sh {\tvs \F \and \ldots}
\\
\entry \D {\eset \and \D, \tv \and \D, \x}
\end{bnffig}

% Syntax!

We begin by introducing the syntax of constraints
in~\Cref{fig:constraint-syntax}.  The constraint language contains
tautologic constraints ($\ctrue$), unsatisfiable constraints ($\cfalse$),
and logical conjunctions ($\Cone \cand \Ctwo$). The constraint form $\tone =
\ttwo$ asserts that the types $\tone$ and $\ttwo$ are equal.  The constraint
form $\cexists \tv C$ binds an existentially quantified type variable $\tv$
in $C$.

\Xdidier {What about the instantiation constraint? $\sigma \leq \t$?
(\string\leq\space should be macroified)}

% Constraint abstractions

There are two constructs that deal with the introduction and elimination of
constraint abstractions. As the name suggests, a constraint abstraction
$\cabs \tv C$ can simply be seen as a function which when applied to some
type $\t$ returns $C[\tv := \t]$. Constraint abstractions are introduced by
a let-construct $\cletin x {\cabs \tv \Cone} \Ctwo$ which binds the
constraint abstraction to the term variable $x$ in $\Ctwo$---additionally
ensuring the abstraction is satisfiable. They are eliminated using the
application constraint $\capp x \t$ which applies the type $\t$ to the
abstraction bound to $x$.

% Suspended match constraints

Finally, we introduce \textit{suspended match
constraints}\footnote{Previously dubbed `frozen constraints' \citep{TODO}}.
These constraints defer constraint generation until the \textit{shape}, \eg
its top-level constructor, of a type becomes known (through unification), at
which point they match on the shape to generate a new constraint. Formally,
a suspended constraint is written as $\cmatch \t \D \hf$, where:
\begin{enumerate}
\item
  The matchee $\t$ is a type. The constraint remains suspended until the
  shape of $\t$ is determined.  
\item 
  The handler $f$ is a meta function from shape ($\sh$) to constraints.
  Once the constraint is unblocked, this function is applied to generate the
  constraint to be solved.  Conceptually, $f$ represents a meta-level
  functionalization of the match's branches.
\item
  The closure $\D$ is the set of both term and type variables that $f$ is
  permitted to reference.  The role of $\D$ in constraint solving is
  discussed further in \cref{sec:solving}.
\end{enumerate}

\begin{mathparfig}[t]
{fig:shape-fn-def}
{Definition of $\shape(\t)$, a partial function from types to
shape-substitution pairs $\langle \sh, \theta \rangle$}
\shape(\tv) \eqdef
    \bot 

\shape(\overline{\t} \Fapp) \eqdef
        \langle \overline{\tv} \Fapp, [\overline{\tv := \t}] \rangle 
\end{mathparfig}

% Informal semantics

Informally, the semantics of $\cmatch \t \Delta \hf$ are as follows: the
constraint remains suspended until the shape of $\t$ is defined (through
unification), at which point the handler is applied to this shape $\hf(\sh)$
and the generated constraint is scheduled to be solved. If the shape of $\t$
is never determined, the constraint remains unsatisfiable.

% Applications

The remainder of this section illustrates the role of suspended constraints
in supporting \emph{fragile} language features as defined above.
These include:
\begin{enumerate}
  \item Constructor and record label overloading for nominal algebraic
  datatypes;  
  \item Overloaded tuple projection in the style of \SML; 
  \item Semi-explicit first-class polymorphism.
\end{enumerate}
We demonstrate how the typablity of each of these features can be elaborated
into constraints, formalized using a constraint generation function of the
form $\cinfer e \tv$, which, given a term $e$ and expected type $\tv$,
produces a constraint $C$ which is satisfiable if and only if $e$ is
well-typed. A formal account of the semantics of suspended constraints and
the declarative typing rules for these features is deferred to 
\cref{sec:semantics} and \cref{sec:language}, respectively.

% When solving $\cmatch \cv \Delta \hf$, the solver proceeds as follows:
% \begin{enumerate}
% \item
%   If $\cv$ is already unified to a non-variable type, apply the handler
%   $f$ immediately to the resolved type.
% \item
%   If $\cv$ is still unresolved, enqueue the suspended constraint to the
%   list of suspended constraints blocked by $\cv$.
% \item
%   During unification, whenever a type variable with a pending constraint
%   becomes resolved, the solver retrieves all blocked constraints on that
%   variable and schedules them.
% \end{enumerate}
% Once all constraints have been processed, any pending suspended constraints
% that remain unsolved indicate failure.

% \paragraph{Constraint-solving: an intuition}


% \paragraph{Applications}
% \begin{itemize}
% \item
%   type-based disambiguation of data constructors (and record fields)
% \item
%   polytypes (are we sure?) \\ ensuring that there is a unique polytype
%   skeleton
% \item
%   anti-unification-based overloading (are we sure?) \\ there is some
%   uniqueness of the head / variational type
% \end{itemize}
% \Xdidier {For me, the three examples are mandatory.
% disambiguation of data constructors and record fields is too simple...
% Also, polytypes may or may not include polyparams. Polyparams are a real
% challenge because most constraints will be delayed, hence the need for a
% strategy to resolve them. Static ad hoc overloading is more general.
% }

% \Xgabriel{I care most about the semantics and implementation of the
% constraint language.} 
% \Xdidier{It would be nice to have typing rules for the surface language, and
% show that they match the constraints generated.} 
% \Xalistair{I agree.}

% For each feature:
% \begin{itemize}
% \item propose *informal* surface-language typing rules
% \item elaborate them into the constraint language
% \end{itemize}

% We will provide *precise* declarative semantics in the ``Semantics of
% constraints'' section.

\subsection{Static overloading of constuctors and record labels}

% Intro to the feature

OCaml allows users to define data types with overloaded constructors or
record fields. 
\begin{program}
type m = L
type n = L
let x = L
\end{program}
The type of \code{x} is ambiguous since OCaml could infer either the type
\code{m} or the type \code{n}. This amounts to a principality issue, since
there is no most general type for \code{x}. \Xdidier {It is not just a
question of typing issue but of semantics: it should make a stratic choice
of the implementation. Just speaking of it as a type issue is a bit
misleading.} \Xalistair{I'll add something about how this ambiguity could have 
runtime implications} As such, OCaml gives the 
following error: ...

OCaml's type checker relies on type information provided by the
bidirectional\Xdidier{bidirectional is misleading here! In what sense ocaml
is a bidirectional typechecker? It is in fact poly-direction. (poly means
polymorphism here)} \Xalistair{Agreed, will change} propagation of type
information to resolve these 
ambiguities. Annotating \code{x} with either \code{m} or \code{n} would
produce a well-typed program.

% Constructor disambiguation in suspended constraints
We propose an alternative approach: suspended constraints. For the ambiguous
constructor application $\econstr K e$, we generate the constraint:
\begin{align*}
\cinfer {\econstr K e} \cva &=
  \cexists \cvb \cinfer e \cvb
  \cand
  \cmatch \cva {\cva, \cvb}
    \parens
      {\clam {(\overline{\cvc} \Fapp)} 
        {\cletin \sigma {\D(\F)(\mathsf{K})} {\sigma \leq \cvb \to \cva}}
      }
\end{align*}
where the function $\clam {(\overline{\cvc} \Fapp)} {\cletin \sigma
{\D(\F)(\mathsf{K})} {\sigma \leq \cvb \to \cva}}$ looks up the type scheme
associated with the constructor using the type name $\F$ in the environment
($\Delta(\F)(\mathsf{K})$). \Xdidier{I don't really understand what you
mean. The prefix notation for types does not help. At least, we should write
$\lambda \parens{\bar \cvc \Fapp}. \ldots$. Still, I think it would help
to use a postfix notation in the formal presentation. In fact
$\bar \cvc(\Fapp)$ would not parse in \OCaml. And if $\D(\F)$ is supposed to
be a type $\t$, what does $\t(K)$ mean?
}
\Xalistair
{$\D(\F)(\K)$ looks up the type scheme associated with the constructor $\K$
in the definition of $\F$ in the context $\D$ (although this metavar has now
been taken by closures...)}\Xdidier {I agree with what it should mean. But
then $\D(F)$ should be explained. Mathematically, $\D$ is a list and
$\F$ is just a constructor so $\D(\F)$ does not mean anything.}

\Xdidier {We haven't seen the instantiation constraint $\sigma \leq \cvb
\to \cva$ yet}

We require a unique type for each type constructor in a given algebraic data
type definition, thus there is no ambiguity with this mapping, despite there
potentially being many constructors in the environment named $\mathsf{K}$.

This not only correctly encodes OCaml's current behaviour, but does better.
\Xdidier {Omitting the final default pass called in case of failure to
disambiguate.}\Xdidier {behavior means sound an complete. Here you mean
that we succeed when OCaml does with the same answer, but we also succeed
with a correct answer when OCaml fails. Hence, we do not capture the OCaml behavior...}

\Xgabriel{It would be nice to be able to show some code examples and explain how they work in our setting, informally -- or, sometimes, how the naive/simple elaboration is not enough.}

\subsection{Semi-explicit first-class polymorphism}

\label {sec/constraints/polytypes}

% Polytypes (and some stuff about labelled types)

Semi-explicit first-class polymorphism \citep{Garrigue-Remy/poly-ml} uses
\textit{annotated types} to track the origins of polymorphic types.

The type constructor $\tapoly \ts \av$ that boxes a polymorphic type
$\ts$ turning it into a \textit{polytype} is actually annotated with
an annotation variable $\av$.  Once boxed, the polytype $\tapoly
\ts \av$ is considered a monotype.  This permits impredicative
polymorphism. Annotations variables may be generalized as over
variables and $\all \av \tapoly \ts \av$ is a type scheme.

The introduction form for polytypes is a boxing operator $\expoly
\e \ts$ with an explicitly polytype annotation $\ts$. The resulting expression
has type $\tapoly \ts \av$ when $\av$ is an arbitrary (typically fresh)
annotation variable and can thus be generalized. That is $\e$ can also be
assigned the type scheme $\all \av \tapoly \ts \av$.

Conversely, to instantiate a polytype expression, one must use an explicit
unboxing operator $\einst \e$ that need not any type annotation.
However, it requires $e$ to be of a polytype scheme $\all \av
\tapoly \ts \av$ and then gives $\einst \e$ a type $\t$ that is an instance of
$\ts$.  In particular, $\einst \e$ is ill-typed if $\e$ has type $\tapoly \ts
\av$ and $\av$ cannot be generalized.  It is the polymorphism of $\av$ that
ensures that the polytype is indeed known and not being inferred.

In particular $\efun \x {\einst \x}$ is not typable, since the
$\lambda$-bound variable $\x$ is assigned a monotype.  However, annotations
can be used to freshen annotation variables.  An annotation $e : \t$ behaves
as the application of a retyping function (one that behaves as the identity
$(\t)$ of type $\all \tvs \t \to \tp$ to the expression $\e$. We usually
omit annotation variables in type annotations, since they can be freshen. For
instance, $\efun {\x : \tapoly \ts {}} {\einst \x}$ which is syntactic sugar
for $\efun \x {\elet \x {\eapp {(\tapoly \ts {})} \x} {\einst \x}}$, is
well-typed since the polytype annotation allows the introduction of a fresh
variable annotation which can be generalized.

\begin{version}{}
%% Old version
Principality is tracked using generalization.  This is done by tracking the
polymorphism of labels. Type schemes quantify not only over type variables
but $\varepsilon$ variables. As a result, a term $e$ with the type $\tapoly
\ts \varepsilon$ for some fresh $\varepsilon$ may be generalized to
$\tfor \varepsilon {\tapoly \ts \varepsilon}$.  The typing rule for
$\einst e$ in fact requires $e$'s type to be $\tfor \varepsilon {\tapoly
\ts \varepsilon}$, not just a plain polytype $\tapoly \ts
\varepsilon$. The rational is that a label is polymorphic if and only if it
is principally known and not inferred.  This is why the introduction form
for polytypes $\expoly e \ts$ is able to introduce a \textit{fresh}
$\varepsilon$ variable.
\end{version}

\Xalistair[]{Something about how information can flow from let bindings to
use sites for principality checking}\Xdidier{Not sure of what you have in
mind}

The very purpose of annotation variables is to distinguish known, polymorphic
polytypes from yet-unknown, monomorphic ones as required by poly-directional
type ifnerence. 
%
With suspended constraints, any type information can be considered as known,
so that we do not need the annotation variables any longer.

Typechecking $\expoly e \ts$ is, intuitively:
\begin{mathpar}
\cinfer {\expoly e {\tfor {\overline{\cvb}} \t}} \cva \wide\eqdef
  \Parens{\cfor {\overline{\cvb}} \cinfer e \t}
\cand
   \cva = [\tfor {\overline{\cvb}} \t] 
\end{mathpar}
If $\e$ is already known to have the type $[\ts]$, then we can simply
instantiate it.  However, if the type of $\e$ is not yet known, \ie  it is a
(possibly constrained) type variable $\tv$: then, we must wait.  So we
instead defined the typing constraint as:
\begin{mathpar}
\cinfer {\einst e} \cva \wide\eqdef
    \cexists \cvb \cinfer e \cvb
\cand
    \cmatch  \cvb {\cva} (\lambda [\ts]. ~ \ts \leq \cva)
\end{mathpar}
since waiting when $\cvb$ is known is harmeless, as the suspended constraint
may immediately be discharged.

By waiting for $e$'s type to be \emph{known}, we may ensure principal types
without annotation variables. However, we still need to explain what it
means for a variable to be \emph{known}, that is, give the semantics of
suspended constraints.


\section{Semantics of constraints}
\label{sec:semantics}

\subsection{Standard semantics}

\begin{mathparfig}%
  {fig:constraint-semantics}%
  {Semantics of constraints (without suspended constraints)}
  \begin{array}{rrl@{\hspace{8em}}l}
    \semenv
      & ::=
      & \cdot
        \mid \semenv, \alpha := \tau
        \mid \semenv, x := \cabs \tv \c
      & \text{Semantic environment}
  \end{array}
\\
  \infer{ }
  {\semenv \vdash \ctrue}

  \infer
  {\semenv \vdash \cone
   \and
   \semenv \vdash \ctwo}
  {\semenv \vdash \cone \cand \ctwo}

  \infer
  {\semenv, \tv := \tau \vdash \c}
  {\semenv \vdash \cexists \cv \c}

  \infer
  {\semenv(\tone) = \semenv(\ttwo)}
  {\semenv \vdash \tone = \ttwo}
\\
  \infer
  {\semenv \vdash \exists \tv. \cone
   \\
   \semenv, x :=  \cabs \tv \cone \vdash \ctwo}
  {\semenv \vdash \cletin x {\cabs \tv \cone} \ctwo}

  \infer
  {\semenv(x) = \cabs \tv \c
   \\
   \semenv, \alpha := \t \vdash \c}
  {\semenv \vdash \cinst x \t}
\end{mathparfig}

The semantics of constraints is given, as is standard, by a satisfiability
judgment of the form $\semenv \vdash \c$: the constraint $\c$ is satisfied
by the solution, or valuation, $\semenv$. The semantic environment $\semenv$
maps type variables to types, and term variables to constraint abstractions.

The judgment is defined, for all constraint-formers except suspended
constraints, in \cref{fig:constraint-semantics}. $\ctrue$ is satisfied in
any environment, and $\cfalse$ in none. An environment $\semenv$ satisfies
$\cone \cand \ctwo$ if it satisfies both $\cone$ and $\ctwo$. The
existential rule $\semenv \vdash \exists \tv \c$ requires that $\semenv$
provides a ``solution'' for $\tv$: it maps it to a type $\tau$ that
satisfies the expected constraints in $\c$. Finally, a unification
constraint holds if both sides are mapped to the exact same type.

This semantics feels a bit tautological -- the rules above, which omit
suspended constraints, are fairly obvious. It is a useful exercise to ensure
that we can give an explanation of each constraint-former in our
mathematical meta-language that is reasonably simple and declarative. It
will be used to specify the soundness and completeness of the constraint
generator, and to reason about the correctness of the solver.

Closed constraints are either satisfiable in the empty semantic environment
$(\cdot)$ or unsatisfiable. For example, consider the constraint $\exists
\tv. \tv = \mathsf{int}$, its satisfiability is established by the following
derivation:
\begin{mathpar}
  \infer
  {\infer
   {\mathsf{int} = \mathsf{int}}
   {\cdot, \alpha := \mathsf{int} \vdash \tv = \mathsf{int}}}
  {\cdot \vdash \exists \tv. \tv = \mathsf{int}}
\end{mathpar}

\subsection{Semantics for suspended constraints}

To extend these semantics to our suspended constraints, a first natural idea
is to propose the following rule -- that we will call the ``natural
semantics'' of suspended constraints.

\begin{mathpar}
  \infer
  {\shape(\semenv(\tau)) = \sh,\theta
    \\
   \semenv_{\mid \Delta}, \theta \vdash f(\sh)}
  {\semenv \vdash \cmatch \tau \Delta f}
\end{mathpar}

This rule says that a suspended constraint is satisfied by $\phi$ whenever
$\phi$ can guess a way to complete $\tau$ into a non-variable type with some
shape $\sh$, and the constraint $f(\sh)$ is satisfied by $\semenv$ --
restricted to the variables explicitly listed in the closure environment
$\Delta$.

For example, the constraint $\exists \tv \tvb. \tv = \tint \cand \cmatch \tv
{} {(\lambda \wild. \ctrue)}$ is satisfied as follows:
\begin{mathpar}
  \infer
  {\infer
   {\infer{\tint = \tint}{\cdot, \alpha := \tint \vdash \alpha = \tint} \\
    \infer{\shape(\tint) = \tint
     \\
     \infer*{ }{\cdot, \tv := \tint \vdash \ctrue}
    }
    {\cdot, \alpha := \tint \vdash \cmatch \tv {} {(\lambda \wild. \ctrue)}}
    }
   {\cdot, \alpha := \tint \vdash
      \tv = \tint
      \cand
      \cmatch \tv {} {(\lambda \wild. \ctrue)}}
 }
  {\cdot \vdash
     \exists \tv. \tv = \tint
     \cand
     \cmatch \tv {} {(\lambda \wild. \ctrue)}}
\end{mathpar}

Unfortunately, this semantics is not satisfying because it does not match
our intution for suspended constraints, which is that they should resolve to
a concrete constraint when its matchee $\t$ becomes \emph{known} from the
rest of constraint. Consider for example the constraint $\cexists \cv
\cmatch \cv \cv (\lambda \wild.~ \cv = \tint)$.  This constraint does not
unify $\cv$ outside the suspended constraint, but once it is scheduled,
the variable $\cv$ must be unified with the type $\tint$. So our
informal solver suggests that this constraint is unsatisfiable since
$\cv$ is never unified outside the match constraint. Yet we can use the
above rule to show that this constraint is satisfiable:
\begin{mathpar}
  \inferrule*
    {
      \inferrule*
        {
          \tint = \tint \\
          \inferrule*
            {\tint = \tint} {\cv := \tint \vdash \cv
            = \tint}
        }
        {\cv := \tint \vdash \cmatch \cv \cv (\lambda \wild. ~ \cv
        = \tint)}
    }
    {\cdot \vdash \cexists \cv \cmatch \cv \cv (\lambda \wild.~ \cv = \tint)}
\end{mathpar}

This semantics let us \emph{guess} types instead of requiring that they are
\emph{known}. A reasonable solver (that does not attempt to guess or
backtrack) cannot match it. It also breaks principality: consider the
function $\efun x (x.2)$, which projects on its second argument, the
semantics of the generated constraint would let us ``guess'' any size for
the tuple type of $x$, and there is no most general choice.\Xalistair{Maybe
we could show a bit of the constraint, simplified.}

Instead we propose \emph{contextual} semantics to capture a non-guessing
semantics for suspended constraints. First we introduce a simplified version
of the natural rule
  \begin{mathpar}
    \infer[susp-open]
    {\shape(\t) = \sh, \theta
     \\
     \semenv_{\mid \Delta}, \theta \vdash f(\sh)
    }
    {\semenv \vdash \cmatch \t \Delta f}
\end{mathpar}
which only applies when $\tau$ already has a shape. In the natural rule we
would take the shape of $\semenv(\tau)$, which contains parts that have been
``guessed'' in $\semenv$; here the shape must already be known in the
constraint.

The interesting part comes with the \emph{contextual} rule that follows,
which is not syntax-directed:
  \begin{mathpar}
    \infer[susp-ctx]
    {\c[\tv !]
     \\
      \semenv \vdash \C[\tv = \t \cand \cmatch \t \Delta f]
    }
    {\semenv \vdash \C[\cmatch \tv \Delta f]}
  \end{mathpar}

This rule lets us substitute the matchee variable $\tv$ by a more defined
type $\t$, whenever the side-condition $C[\alpha !]$ holds. This condition,
whose definition follows (and is rather technical), expresses that the
context $\C$ constrains the solutions of $\tv$ to have a \emph{unique}
shape. In other words, we can guess a shape, as in the natural semantics,
but only if the constraint context around it $\C[\square]$ enforces that
there is a unique solution.

Consider our previous examples:
\begin{mathpar}
\exists \tv. \tv = \tint
  \cand
  \cmatch \tv {} {(\lambda \wild. \ctrue)}

\cexists \cv \cmatch \cv \cv (\lambda \wild.~ \cv = \tint)
\end{mathpar}

In the first example, we apply the contextual rule with the context $\C :=
(\exists \tv. \tv = \tint \cand \square)$. Any solution $\phi$ of this part
of the constraint necessarily has $\phi(\tv) = \tint$, therefore the shape
of $\tv$ is uniquely determined. The contextual rule is thus applicable and
the example is satisfiable.

In the second example, the context around the suspended constraint is empty
($\C[\square] = \square$), so any solution $\semenv$ satisfies it, and
$\alpha$ can take an arbitrary shape in this context. The contextual rule is
therefore not applicable, and the example is not satisfiable, as we
intend. Let us now define the $\C[\alpha!]$ condition.

\begin{definition}
  A type variable $\cv$ is \emph{uniquely known} within the context
  $\cctx[-]$, written $\cctx[\cv !]$, iff there exists a unique shape $\sh$
  such that, for all assignments $\semenv, \semenvp$, then $\semenv \vdash
  \cctx[\semenvp \vdash \ctrue]$ implies that $\shape(\semenvp(\tv))$ is
  $(\sh, \theta)$ for some $\theta$.

  In other words:
  \begin{mathpar}
    \begin{array}{l}
    \exists! \sh, \quad \forall \semenv, \semenvp,
      \quad \exists \theta, \\ \qquad
      \semenv \vdash \c[\semenvp \vdash \ctrue] \implies
    \shape(\semenvp(\cv)) = (\sh, \theta)
    \end{array}
  \end{mathpar}

  The notation $\semenv \vdash \c[\semenvp \vdash \cp]$ means that there is
  a derivation of the satisfiability judgment for $\c[\cp]$ where, the
  subderivation at the hole of $\c[\square]$ is a derivation of $\semenvp
  \vdash \cp$. We define it formally in \cref{fig:TODO}.
\end{definition}

Notice that the rule \Rule{susp-ctx} has $\C[\tv!]$ as a premise, which in
turn uses the judgment $\semenv \vdash \c[\semenvp \vdash \ctrue]$ in
negative position, so we must check that our satisfiability judgment remains
well-founded. Informally, the reason for well-foundedness is that the
conclusion of \Rule{susp-ctx}, which is $\semenv \vdash \C[\cmatch \tv
\Delta f]$, is strictly larger than the negatively-occurring judgment
$\semenv \vdash \C[\ctrue]$. We discuss well-foundedness in more details in
\cref{sec:TODO}.

\subsection{Examples}

Consider for example the constraint
$$
  \exists \tva \tvb.
  \Parens{\begin{array}{l}
    \cmatch \tva {\tvb} {(\cabs \wild {\tvb = \tbool})} \\
    {} \cand \cmatch \tvb {} {(\cabs \wild \ctrue)} \\
    {} \cand \tva = \tint
  \end{array}}
$$

To prove that this constraint is satisfiable we can use the \Rule{susp-ctx}
twice, once with a hole around the suspension on $\tva$, and once with a
hole around the suspension on $\tvb$. In fact this can be done in any order:
\begin{itemize}
\item
  If you focus on $\tva$ first, it is easy to prove that the context around
  it, $(\square \cand \cmatchdots \tvb \cand \tva = \tint)$, imposes a
  unique solution thanks to $\tva = \tint$. Then we use the
  $\Rule{susp-ctx}$ rule in the context $(\cmatch \tint {\beta} {(\cabs
  \wild {\beta = \tbool})}) \cand \square \cand \tva = \tint)$, and we have
  to prove that $\tvb$ is uniquely determined in this context. This is in
  turn proved by using the conjunction rule followed by \Rule{susp-open},
  which gives us access to the unification $\tvb = \tbool$.

\item
  If you focus on $\tvb$ first, proving that $\tvb$ is uniquely determined
  in the context $(\cmatch \tva \tvb {(\cabs \wild {\tvb = \tbool})} \cand
  \square \cand \tva = \tint)$ requires using the \Rule{susp-ctx} again as
  in the case above. Then we end up with the constraint $(\cmatchdots \tva
  \cand \cmatchdots \tbool \cand \tva = \tint)$, and we can use
  \Rule{susp-ctx} yet another time on $\alpha$.

\end{itemize}

On the other hand, the following constraint has circular dependencies:
$$
  \exists \tva \tvb.
  \left(\begin{array}{l}
    \cmatch \tva {\tvb} {(\cabs \wild {\tvb = \tbool})} \\
    {} \cand \cmatch \tvb {\tva} {(\cabs \wild {\tva = \tint})}
  \end{array}\right)
$$

This constraint can be proved satisfiable with the ``natural semantics''
that we proposed earlier, by guessing the instantiation $\tva := \tint, \tvb
:= \tbool$. Our solver fails on it, and so does our contextual
semantics. Without loss of generality, suppose that we try to use
\Rule{susp-ctx} on $\tva$ first: we get to prove $\C[\tva !]$ for the
context $\C := (\square \cand \cmatch \tvb \tva {(\cabs \wild {\tva =
\tint})})$: we have to prove that there exists a unique head structure for
$\alpha$ (presumably $\tint$) such that $\C[\ctrue]$ is satisfiable. But
this does not hold, as the constraint $(\cmatch \tvb \tva {(\cabs \wild
{\tva = \tint})})$ cannot be solved: we cannot use the \Rule{susp-open} rule
yet as $\beta$ has no head, and we cannot use the \Rule{susp-ctx} rule as
the empty context $\square$ does not uniquely constrain $\tvb$.

\subsection{Technical details}

\TODO{Explain the notation $\semenv \vdash \C[\semenvp \vdash \c]$.}

\TODO{A figure that summarizes everything about the contextual rules so far.}

\TODO{Discuss well-foundedness.}


\Xgabriel{(We stopped here with Alistair.)}



\Xgabriel{Maybe reuse what's below.}
Our conclusion is that the na\"ive semantics should be rejected in favour
for the semantics of our informal solver. Our intent with suspended
constraints is that the blocking variable should not be guessed out of thin
air, but deduced from the surrounding context without knowledge of the
constraint generated by the handler.

The idea, inspired by type systems that require principality (such as
FreezeML), is to stipulate that the only satisfying assignments for the
context of the suspended constraint requires $\cv$ to be assigned to some
type with a unique type constructor.
\Xalistair[]{This doesn't read entirely correctly, we want a unique
constructor for all assignments}. 


\TODO{Show the natural-semantics rule for suspended constraints,
  explain that (1) it is not our intention for this typing construct (it
  admits ``guesses out of thin air'') (2) a non-guessing solver cannot be
  complete with respect to it.}

\TODO{Show the contextual semantics,
  first an informal explanation of $\c[\tv !]$,
  then show the full definition, then discuss the fine-grained
  points on examples.
}

\TODO{Discuss well-foundedness.}

\Xdidier{We are missing an introduction to the semantics of constraints}


A na\"ive denotation semantics for suspended constraints would be to simply
apply the assignment of $\cva$ to $f$. More formally,
\begin{mathpar}
  \inferrule*
    {\semenv(\cva) = \overline{\t} \Fapp \\ \semenv_{\setminus
    \Delta}[\overline{\cvb := \t}] \vdash f(\overline{\cvb} \Fapp)} 
    {\semenv \vdash \cmatch \cva \Delta f}
\end{mathpar}
However, our informal solver would be incomplete with respect to this
semantics.  We show this by considering the constraint $\cexists \cv
\cmatch \cv \cv (\lambda \_.~ \cv = 1)$.  This constraint does not
unify $\cv$ outside the suspended constraint, but once it is scheduled,
the variable $\cv$ must be unified with the unit type $1$. So our
informal solver suggests that this constraint is unsatisfiable since
$\cv$ is never unified outside the match constraint. Yet we can use the
above rule to show that this constraint is satisfiable:
\begin{mathpar}
  \inferrule*
    {
      \inferrule*
        {
          (\cv \mapsto 1)(\cv) = 1 \\
          \inferrule*
            {(\cv \mapsto 1)(\cv) = 1} {\cv \mapsto 1 \vdash \cv
            = 1}
        }
        {\cv \mapsto 1 \vdash \cmatch \cv \cv (\lambda \_. ~ \cv
        = 1)}
    }
    {\cdot \vdash \cexists \cv \cmatch \cv \cv (\lambda \_.~ \cv = 1)}
\end{mathpar}
The dependency of suspended constraints can also be complex, as in the
following example: $\cexists {\cva, \cvb} \cmatch \cva \cvb {(\lambda
\_. ~ \cvb = 1)} \cand \cmatch \cvb \cva {(\lambda \_. ~ \cva = 1)}$.
Worse still, is that the binding location of $\cva$ may not even contain
the sufficient information to show that $\cva$ is realised.  For instance,
\begin{align*}
  \cexists \cvb \cvb = 1 \cand \cexists \cva \cva = \cvb \cand \cmatch \cva \Delta f
\end{align*}

Our conclusion is that the na\"ive semantics should be rejected in favour
for the semantics of our informal solver. Our intent with suspended
constraints is that the blocking variable should not be guessed out of thin
air, but deduced from the surrounding context without knowledge of the
constraint generated by the handler.

The idea, inspired by type systems that require principality (such as
FreezeML), is to stipulate that the only satisfying assignments for the
context of the suspended constraint requires $\cv$ to be assigned to some
type with a unique type constructor.
\Xalistair[]{This doesn't read entirely correctly, we want a unique
constructor for all assignments}. 

This motivates our next definition: the principal realisation of a type
variable $\cv$ by some constraint context $\cctx[-]$.

\begin{definition}
  A type variable $\cv$ is principally realised with the constructor $\F$
  by the context $\cctx[-]$, written $\cctx[\cv !] \rhd \F$,
  iff for all assignments $\semenv$, there exists an assignment $\semenvp$
  for the 
  hole such that $\semenv \vdash \cctx[\semenvp \vdash \ctrue]$ implies
  $\semenvp(\cv) = \overline{\t} \Fapp$
\end{definition}

\TODO
{Here, we not only rely on the definition of a constraint with a hole
$\mathcal{C}[-]$ but a judgement with a hole and some way to translate
between the two. We'll need formal definitions for this.}

\TODO
{Some examples of principal realisation}

As we've previously hinted, the semantics for a suspended constraint must
require that the surrounding context principally realises the matchee. This
is formally specified as:
\begin{mathpar}
\inferrule*
   {\cctx[\cva !] \rhd \F \\
    \semenv \vdash \cctx[\semenvp_{\setminus \Delta}[\overline{\cvb :=
      \t}] \vdash f(\overline{\cvb} \Fapp)] \\
    \semenvp(\cva) = \overline{\t} \Fapp}
   {\semenv \vdash \cctx[\semenvp \vdash \cmatch \cva \Delta f]}
\end{mathpar}

\TODO: precise declarative semantics of constraints. They should coincide
with the semantics of the typing rules.

Alistair has yet another proposal derived from his simplification of his
timestamp semantics.

\TODO
{derive precise declarative semantics for the language features of
Section 2.}


Meta-substitution lemma: if $\semmetav$ occurs exactly once in $\c$, then
$\semmetaenv, (\semenvp \vdash \semmetav); \semenv \vdash C$ and $\semenvp
\vdash \cp$ if and only if $\semmetaenv; \semenv \vdash \c[\semmetav :=
\cp]$. 

Convention: we write $\c[\cp]$ for $\c[\square := \cp]$. In particular
$\c[\square] = \c$ when $\square$ occurs once in $\c$. 

Next steps:
\begin{itemize}
\item introduce the condition
  $\c[\cv !]$:
  \begin{mathpar}
    \begin{array}{l}
    \exists! \F, \quad \forall \semenv, \semenvp,\\ \qquad (\semenvp \vdash
    \square); \semenv \vdash \c[\square] \implies
    \mathsf{head}(\semenvp(\cv)) = \F 
    \end{array}
  \end{mathpar}
\item and finally the global rule
  \begin{mathpar}
    \infer
    {\c[\cva !]
      \and \semenvp(\cva) = \overline{\t} \Fapp
      \and \semmetaenv, (\semenvp, \overline{\cvb := \t} \vdash
      \square)\vdash \c[f(\overline{\cvb} \Fapp)] } 
    {\semmetaenv, (\semenvp \vdash \square); \semenv \vdash \c[\cmatch \cva \Delta f]}
  \end{mathpar}
\end{itemize}

The intuition for the judgment $\semmetaenv, (\semenvp \vdash \square);
\semenv \vdash \c[\cp]$ is that it is a proof of satisfiability of
$\c[\square := \cp]$ with valuation $\semenv$, where we furthermore know
that the $\cp$ sub-derivation has the valuation $\semenvp$. Note that, after
the the global rule is added, the derivations of this judgment are not
syntactically directed by $\c$ anymore.

\section{The language}
\label{sec:language}

% Running example: tuple projection disambiguation 

% We need a spec, but this itself is hard

In order to show our approach is sound and complete with respect to our
constraint generation translation, we must first define a formal
specification in the form of a calculus and accompanying type
system. Surprisngly, identifying an appropriate declarative type system to
use as a specification is itself an interesting problem!

% Why is it hard? Examples of prev work:

Much work has been done on various extensions of \ML with features such as
static overloading \citep{TODO} and first-class polymorphism
\citep{TODO}. However, many of these approaches either fail to ensure
principality or require intricate menachisms -- such as labelled types -- to
do so.

% Why do naive approaches not guarantee principal types. 

Na\"ive specifications, though accessible, often lack principal types. Take
overloaded tuple projects \textit{\`a la \SML}: an expression $\efield e j$
governed by the rule:
\begin{mathpar}
  \infer 
    {\Gamma \vdash e : \Pi_{i = 1}^n \ti \and 1 \leq j \leq n}
    {\Gamma \vdash \efield e j : \tj}
\end{mathpar}
admits many typings, as any tuple of at least size $j$ satisfies the
premise. This multiplicity undermines principality.

% Partial annotations (shapes) to the rescue!

Our first insight is that partial type annotations often sufficies to
recover principality. We refer to these annotations as \textit{shapes}. For
example, explicitly annotating the tuple projection with its arity, as in
$\exfield e n j$, disambiguates the expression:
\begin{mathpar}
   \inferrule* 
      {\Gamma \vdash e : \Pi_{i = 1}^n \ti \\ 1 \leq j \leq n }
      {\Gamma \vdash \exfield e n j : \tj}
\end{mathpar}
Yet, requiring users to write such annotations manually is impractical. 

% Inference of shapes

Our solution is to permit \textit{inference} of shapes, provided that the
inferred annotation is \textit{uniquely determined by context}.  This
approach ensures principality by construction and yields a clear
specification with predictable annotability requirements for the everyday
programmer.

% A note on stratified type inference 

The idea of leveraging a principled core language with inference of shapes
is not novel. \textit{Stratified type inference} [??]  pursued a similar
goal (in the context of generalized algebraic data types). It operates in
two phases: the first phase transforms the program into an intermediate
language and generates shapes using \textit{bidirectional typing}; the
second performs \HM type inference for the intermediate language. While the
second phase is sound, complete and enjoys principal types, the overall
system remains incomplete due to the heuristic nature of the first phase.

% How do we specify our intuition?

Specifying the notion of unique inferrability from context is
tricky. Luckily, we can leverage many of the formal methods developed in the
previous section -- in particular, the manipulation of contexts within the
type derivation -- to prove uniqueness of shapes. Allowing us to
systematically replace the ambiguous constructs with their disambiguated
counterparts:
\begin{mathpar}
  \inferrule* 
    {E[e \mathop{!} \Pi^n] \\ \Gamma \vdash E[\exfield e n j] : \t}
    {\Gamma \vdash E[\efield e j] : \t}
\end{mathpar}
The intuition here is that the context $E$ is sufficiently large to
constrain the shape of $e$'s type as a tuple of size $n$.
\Xalistair{Some notation trickiness here }

% Limitations

The cheif limitation of our approach is that disambiguation can still
require explicit annotations in inherently ambiguous scenarios. One solution
is the use of sensible `defaults', such as resolving $\efield e j$ by
assuming the tuple has arity $j$. While appealing, such defaults compromise
principality. We return to this pragmatic tradeoff in Section
\ref{sec:implementation}.

\begin{bnffig}{fig/syntax}{Syntax of types and terms}
\entry[Terms]{\e}{
  x \and
  () \and
  \efun x e \and
  \eapp \eone \etwo \and
  \elet x \eone \etwo \and
  (e : \t) \andcr
  \erecord {\overline{\elab = e} } \and
  \efield e \elab \and
  \exfield e \F \elab \andcr
   (\eone, \ldots, \en) \and
   \efield e j \and
   \exfield e n j \andcr
   \epoly e \and
   \epoly[\exi \tvs. \ts] e  \and
   \einst e \and
   \exinst e \tvs \ts
}\\[1ex]
\entry[Types]{\t}{
   \tv \and
   1 \and
   \tya \to \tyb \and
   \ts \F \and
   \Pi_{i = 1}^n \ti \and
   \tpoly {\exi \tvs \ts}
}\\
\entry[Type schemes]{\ts}{
     \t \and
     \tfor \tv \ts 
}\\
\entry[Contexts]{\G}{
   \eset \and
   \G, x : \ts \and
   \G, \tv 
}\\
\end{bnffig}

\paragraph{Terms}

\Xdidier{Records are in the syntax, but not in the typing rules}

In \cref {fig/syntax}, we give the grammar for our calculus. Terms
include all of the
\ML calculus: variables $x$, the unit literal $\eunit$, lambda abstractions
$\efun x e$, applications $\eapp \eone \etwo$, annotations $(e : \t)$ and
let bindings $\elet x \eone \etwo$. Our extensions include:
\begin{enumerate}
\item
  Constructor and record label disambiguation, modelled using record
  literals $\erecord { \labone = \eone; \ldots; \labn = \en }$ and field
  projections $\efield e l$.

\item
  Tuples $(\eone, \ldots, \en)$ with overloaded tuple projections
  $\efield e i$.  

\item
  For semi-explicit first-class polymorphism, we have the boxing construct
  $\epoly \e$  and the unboxing construct $\einst e$.

\end{enumerate}
Each construct that endangers principality (written $\e^i$) has an
explicitly annotated counterpart (written $\e^x$) that is $\eproj [\F] \e
\elab$, $\eproj [n] \e j$, $\epoly [\exi \tvs \ts] \e$, or $\einst [\exi
\tvs \ts] \e$.

\paragraph{Types}

As usual, types are split into monotypes (or just types) and type
scehems. Types $\t$ include the variables $\tv$, the unit type $1$, function
types $\tone \to \ttwo$, $n$-ary product types $\Pi_{i = 1}^n \ti$ and
nominal record types $\tys \F$. Type schemes $\sigma$ extend monotypes
allowing the universal quantification of zero or more type variables $\all
\tvs \t$. We write $\tv, \tvb, \tvc$, \etc for type variables.  We also have
polytypes \textit{without the labels} $[\ts]$ as monotypes.

\paragraph{Typing contexts}

Tying contexts $\Gamma$ are an ordered sequence of expression variable
typings $x : \sigma$ and universally quantified type variables. Our
presentation of \ML explicitly tracks the type variables in scope in the type
context $\G$. (This may differ from traditional presentations that permit type
variables to ``float around''.)


\paragraph{Typing}

\Xdidier
{I added implicit creation of polytypes. This is required for polyparams. It
also allows to illustrate a constryct whose implicitness is from the context
rather than from $e$. }


% Shape of judgements (Well-formedness, instantiation and typing)

% ML explainations: variables, unit, functions, let binders, instantiation 

% Annotations

% Static overloading 

% Shape principality 
% Abuse of notation remark -- well foundedness 
% Judgement indexing 

% Semi-explicit first-class polymorphism 
% Shapes of polytypes -- explain canonicalization / defunctionalization process 

\begin{mathparfig}{fig/typing}{Typing rules}
  \inferrule[Var]
    {x : \sigma \in \G}
    {\G \th x : \sigma}

  \inferrule[Fun]
    {\G \th \tone \\
     \G, x : \tone \th e : \ttwo }
    {\G \th \efun x e : \tone \to \ttwo}

  \inferrule[App]
    {\G \th \eone : \tone \to \ttwo \\
     \G \th \etwo : \tone}
    {\G \th \eapp \eone \etwo : \ttwo}
  
  \inferrule[Unit]
    {}
    {\G \th () : 1}

  \inferrule[Annot]
    {\G \th e : \t\where {\tvs \is \ts}}
    {\G \th (e : \exi \tvs \t) : \t\where {\tvs \is \ts}}

  \inferrule[Gen]
    {\G, \tv \th e : \sigma \\ \tv \disjoint \G}
    {\G \th e : \tfor \tv \sigma}
  
  \inferrule[Inst]
    {\G \th e : \sigmaone \\
     \G \th \sigmaone \leq \sigmatwo}
    {\G \th e : \sigmatwo}

  \inferrule[Let]
    {\G \th \eone : \sigma \\
     \G, x : \sigma \th \etwo : \t}
    {\G \th \elet x \eone \etwo : \t}

  \inferrule[Tuple]
    {\parens{\G \th \ei : \ti}\iton}
    {\G \th (\eone, \ldots, \en) : \Pi\iton \ti}

  \inferrule[Proj-X]
    {\G \th \e : \Pi\iton \ti \\
     1 \leq j \leq n}
    {\G \th \exfield \e n j : \tj}

  \inferrule[Proj-I]
    {\eshape E e {\Pi^n} \\
     \G \th E[\exfield e n j] : \t}
    {\G \th E[\efield e j] : \t}
    
  \inferrule [Poly-X]
    {\G \th \e : \ts\where {\tvs \is \tys}}
    {\G \th \epoly[\exi \tvs \ts] \e : \tpoly {\ts \where {\tvs \is \tys}}}

  \inferrule [Poly-I]
    {\Eshape \E \e {\tpoly {\exi \tvs \ts}} \\
     \G \th \epoly[\exi \tvs \ts] \e : \tpoly {\ts \where {\tvs \is \tys}}}
    {\G \th \epoly \e : \tpoly {\ts \where {\tvs \is \tys}}}

  \inferrule [Use-X]
    {\G \th \e : \ts \where {\tvs \is \tys}}
    {\G \th \exinst e {\overline{\tv}} \sigma : \ts \where {\tvs \is \tys}}

  \inferrule [Use-I]
    {\eshape E  \e {\tpoly{\exi \tvs \ts}} \\
     \G \th E[\exinst e \tvs \ts] : \t}
    {\G \th E[\einst e] : \t}
\end{mathparfig}

As usual, the typing judgment $\G \th \e : \ts$ states that in context
$\G$, expression $\e$ has type scheme $\ts$.  Typing rules are given on
\cref {fig/typing}. Typing rules for fully annotated terms are unsurprising.

However, typing rules for terms with omitted type annotations are
non-compositional as they depends on a surrounding one-hold context
$\E$. Hence, they assert that an expression $\G \th \E \where {\e^i}: \t$
where $\e^i$ is an expression with an implicit type annotation.
%
We first request a typing for the expression with an explicit annotation $\G
\th \E \where {\e^i}: \t$ where $\e^i$.  We then requests that (the
shape of) the annotation is fully determined from context, either from the
type of the expression, which we write $\eshape \E \e s$, or from the type
of the hole, which we write $\Eshape \E \e s$.

Shapes are one of the following form:n
\begin{mathpar}
s \is \exi \tvs \ts \mid \Pi^n \mid \F \mid \sbot
\end{mathpar}
where $\exi \tvs \ts$ is a shape (as defined above).  The shape of a
variable is undefined (which we write $\sbot$).  The shape of a non-variable
monotype $\tys \F$ is $\F$.
%
The shape of a polytype $\tpoly \ts$ is the \emph{polytype skeleton} of
$\tpoly \ts$ abstracted over the monomorphic parts. It is the minimal
closed type expression $\exi \tvas {\tpoly {\all \tvbs \t}}$ (up to the
ordering of variables) such that there exists types $\tys$ such that
$\t\where {\tvas \is \tys}$ is equal to $\ts$.
%
Closed, means that $\tvas$ is equal to the free type variables
$\ftv \ts$ of $\ts$. The minimality condition is equivalent to the fact that
any suffix of a non-variable node in $\t$ leads to a polymorphic type
variable, \ie a variable in $\tvbs$.
%
When $\exi \tvs \ts$ is a polytype skeleton, we write $\cpoly \tys \ts$ for
the polytype $\tpoly \ts \where {\tvs \is \tys}$.

\Xdidier {We may wish to extend the definition of shapes to all non-variable
types, letting it be the top constructor. This will carry a better
intuition, even though the current presentation is correct.}

In order to describe the judgments $\eshape \E \e s$ and
$\Eshape \E \e s$, we introduce a cast operation $\ecase \e \t \tp$
which allows an expression $\e$ of type $\t$ to be seen with type $\tp$.
That is the typing rule for casts is 
\begin{mathpar}
  \inferrule[Cast]  
    {\G \th e : \t}
    {\G \th \ecast e \t \tp : \tp} 
\end{mathpar}
Casts are not allowed on source terms and are just a device for the
definition of unavoidable shapes. Finally, we define it means for a shape
to be determined from the context or the expression: 
\begin{mathpar}
\def \Eqdef {&\eqdef&}
{\begin{tabular}{RCL}
\eshape E \e s \Eqdef
  \forall \G, \t, \tp, \uad
  \G \th E \where {\ecast \e \typ \typp} : \t \implies \tshape \typ = s
\\[1ex]
\Eshape E \e s \Eqdef
  \forall \G, \t, \tp, \uad  \G \th E\where{\ecast \e \typ \typp} : \t \implies
  \tshape \typp = s
\end{tabular}}
\end{mathpar}
These states that the shape $s$ of expression $\e$ in context $\E$ is
determined by the expression $\e$, in the former case, or by the context
$\E$ in the latter case.

Shapes are equal modulo alpha equivalence and the removal of useless
polymorphic type variables. There do not have useless existential variables.
Shapes are only defined for n-ary tuples and polytypes.


\Xdidier {In fact, the current proposal does not include overloading
of nominal record fields... I thought we wanted to do this.
Do we then need to change the examples in the introduction?
Besides, I don't think we can reproduce the examples analogous to $e_6$ and
$e_7$ with structural tuples.}

The following lemma shows that we can always take a larger context
$\E$ for implicit rules \Rule {Proj-I}, \Rule {Use-I} and \Rule {Poly-I}. 
\begin{lemma}
\newcommand {\Eab}{\parens{\Ea\where \Eb}}
Assume $\eshape \Eb e s$ and $\G \th \Ea \Where {\Ga \th \Eb \where {\e^x} :
\t} \t$, then $\eshape \Eab e s$ and $\G \th \Eab\where {\e^i} \t$.
\end{lemma}
\begin{proof}[Proof Sketch]
Formally, can be proved by induction on $\Ea$. 
\end{proof}
That is, there is always a derivation using only toplevel contexts. 

We now illustrate the typing of implicit constructs with a few examples.
\begin{example}
To illustrate a simple case of non-typability, we show that the expression $e$
equal to $\efun \x {\eproj \x k}$ is ambiguous, \ie that it does not
typecheck.
%
Let $e_n$ be the explicitly annotated version $\efun r
{\eproj[n] \x i}$. for $n \le k$.  If there is a derivation of $\efun r
{\eproj \x i}$ then there must be one of the form:
\begin{mathpar}
\infer*[Right=Proj-I]{
                        \eshape E \x {\Pi\iton} \\
                \eset \th E \where {\eproj[n] \x k} : \t_k
}{%             -------------------------------------------
                  \eset \th E \where {\eproj \x k} : \t_k
}
\end{mathpar}
where $E$ is the term $\efun \x \ehole$, which is the largest possible
context. (If there is one with a smaller $E$, there is also one with a
larger $E$.)
%
We have  the following derivation:
\begin{mathpar}
\infer* [Right=Fun]{
          \infer*[Right=Proj-X]
                {\x : \Pi\iton \ti \th \x : \Pi\iton \ti}
                {r : \Pi\iton \ti \th \eproj[n] \x k : \t_k}
}{%             --------------------------------------------
                 \eset \th E \where{\eproj[n] \x k} : \t_k
}
\end{mathpar}
However, $\eshape E  \x {\Pi\iton}$ does not hold.
Indeed, we have $\eset \th E \where {\emagic {\x :\t_n}} : \wild$
for any $\t_n$ of the form $\Pi\iton \tys$ and $n \ge k$. 
Hence, $\Pi\iton[k]$ and $\Pi\iton[k+1]$ are two possible shapes
for the type of $\x$. 
\end{example}

\begin{example}
\locallabelreset
We now illustrate a non-ambiguous example, showing that the
expression $e$ equal to $\th \eapp {(\efun \x {\eproj
\x  k})} {(1, 2)} : \tint$.

Let $k$ and $n$ be $1$ and $2$.
Let $E$ be the context $\th \eapp {(\efun \x \ehole)} {(1, 2)} : \tint$.  We
have the derivation:
\begin{mathpar}
\infer* [Right=Proj-I]{
                        \eshape E \x {\Pi\iton} \\
                \eset \th E \where {\eproj[n] \x k} : \t_k
}{%             ------------------------------------------- 
                  \eset \th E \where {\eproj \x k} : \t_k
}
\end{mathpar}
Indeed, $\eset \th E \where {\eproj[n] \x k} : \t_k$ obviously holds.
Therefore, it just remains to show $\eshape E \x {\Pi\iton}$~\llabel C.
Assume $\eset \th \E \where{\ecast \x \typ \typp} : \t$. Since
$\x: \Pi\iton (\tint,\tint)$ is bound in the context at the hole $E$,
there is no other choice but $\tp$ equal to $\Pi\iton (\tint,\tint)$,
hence $\tshape \t = \Pi\iton$, which proves~\lref C. 
\end{example}

The following example of non-typability illustrates is how the typing rules
still forces to reject typing of expressions whose annotations would
be unambiguous. This is intended, to prevent us from having to focus
at several annotation simultaneously. Our typing rules enforces the
resolution of annotation, locally, one construct at a time. 

\begin{example}
\newcommand{\tyid}{\ty_{\kwd{id}}}
\newcommand {\epid}[1][]{\epoly[#1]{\efun z z}}
Let $\tyid$ be $\tpoly{\all \tv \tv \to \tv}$. 
%
We show that the expression $e$ equal to $\elet \x {\epoly {\efun z z}}
{(\eapp {\einst \x} 1, \eapp {\einst \x} \eunit)}$ is rejected as ambiguous.
Let $\tyid$ be $\tpoly {\all \tv \tv \to \tv}$.  Clearly, we have $\elet \x
{\epoly [\tyid] {\efun z z}} {(\eapp {\einst[\tyid] \x} 1, \eapp
{\einst[\tyid] \x} \eunit)}$.  This is actually the only possible fully
annotated derivation.
%
To show that $e$ is typable, we must be able to make all annotations
optional, sequentially.  Therefore, the final step, which will eliminate the
last annotation has a single point of focus $\E{\e^x}$, which could be any
of the three positions with an annotation.  We consider each case
independently, and show that it is actually not typable.
\proofcase
{$\E$ is $\elet \x \ehole (\eapp {\einst \x} 1, \eapp {\einst \x}\eunit)$}
%
If this holds, we should have a derivation that ends with
\begin{mathpar}
\infer*[Right=Poly-I]{
                         \Eshape \E \epid \tyid \\
                  \eset \th \E \where {\epid [\tyid]}: \t
}{%               --------------------------------------- 
                       \eset \th \E \where \epid : \t
}
\end{mathpar}
However, $\Eshape \E {\epoly {\efun z z}} {\tpoly \tyid}$ does not hold.
Indeed, the following judgment $\eset \th \E \where {\epid [s]}: \t$ where
$s$ is either $\all \tv \tv \to \tv$ or $\all \tv (\tv\to\tv) \to
\tv\to\tv$. Hence, this case cannot occur. 

\proofcase
{$\E$ is
 $\elet \x \epid (\eapp {\einst \ehole} 1, \eapp {\einst \x} \eunit)$}
%
The derivation must end with:
\begin{mathpar}
\infer*[Right=Proj-X]{
                          \eshape \E \x \tyid \\
                \eset \th \E \where {\einst[\tyid] \x} : \t
}{%             ------------------------------------------- 
                    \eset \th \E \where {\einst \x} : \t
}
\end{mathpar}
However, $\eshape \E \x \tyid$ does not hold (the proof is similar to the
previous case). 

\proofcase {$\E$ is  $\elet \x \epoly {\efun z z} (\eapp {\einst \ehole} 1, \eapp
{\einst \x} \eunit)$} This is symmetric to the previous case, which cannot
hold either.
\end{example}

\section{Solving constraints}
\label{sec:solving}

\TODO
{a declarative solver.}

% Intro 

% Shape of solver 

% Constraint contexts (and their use in solving)

% Solved forms 

% Unification 

% Solver rules 

% Explainations for the main rules 

% Term variable constraints

% Suspended match constraints 

% Metatheory 


\section{HM Prototype and discussion}
\label{sec:implementation}

% Generalization trees 

% Lazy generalization 

% Schedulers 

% Partial generics and instances 

\paragraph{Interaction with let-polymorphism}
\Xgabriel{This was moved from the introduction, it's related to ``choice points'' below.}

%% Our intuition is not entirely correct here.

While the above constraint generation already improves on
OCaml's current inference approach, it is not powerful enough to capture the
simple idea that to resolve an ambiguous constructor we must use contextual
information to show there is a unique type constructor $\F$ for the
application.

We demonstrate this is with the following example:
\begin{program}
let foo =
  let f x = match x with L -> 1 in
  f (L : m)
\end{program}
With the above contraint generation, we would keep the type of \code{x}
as a generalizable variable, thus the application of \code{f} would not
resolve this ambiguity, requiring an annotation at the definition.

We require some way of splitting the head of the type from the type itself,
treating the head of the variable monomorphically and the rest
polymorphically.
\Xalistair{An explaination of kinded(1) types here }


\subsection{Choice points}

\subsection{Default clauses}


\section{Semi-unification Prototype}

Rank-2 polymorphic type inference with semi-unification!

\Xdidier{When you do rank-2 unification, you don't need a 'let' anymore
because 'lambda' is enough. You have rank-2 types but you cannot bind
them. It's not as exciting as I thought it would be.}

\section{Related work}
\label{sec:related-work}

\subsection{Qualified types}

\Xdidier {The following paragaph is just take from the old introduction}

Qualified types~\citep*{TODO} represent additional knowledge on a type. They
are used in particular in Haskell type-classes, where a constraint
$\mathsf{Show}~\alpha$ represents the fact that a part of the inferred term
needs to print values of type $\alpha$. A constraint on a ground type such
as $\mathsf{Show}~\mathsf{Int}$ can be resolved to a known printer. But if
the undetermined variable $\alpha$ becomes generalizable in a type $\tau$,
we get a type-scheme $\tfor{(\alpha \mid \mathsf{Show}~\alpha)}{\tau}$ that
also includes the type-class constraint.

DRAFT: Type-classes let each use-site choose a different typeclass instance
-- but this implies dictionary-passing or specialization. Sometimes we want
the choice to be shared by all use-sites:


\subsection{Suspended constraints in \textsc{OutsideIn}}

\Xalistair{They first solve simple constraints (existentials,
unification). Then they solve ``... constraints'', with implication
constraints, which corresponds to the case of GADT matches. Crucially, they
abandon local let generalization.}

\subsection{Suspended constraints in dependent-type systems}


\subsection{Bi-directional type inference}

\Xgabriel{Our approach should have better properties for disambiguation, but
which ones?} 

In the simply-typed case our system works better. But with generalization
you can have more issues.

\Xdidier{Should we try to extend what we did to predicative polymorphism?}

\Xalistair{Not ready yet and would probably need too much space.}

\Xgabriel{CoreML + bidirectional disambiguation of constructors?}

\TODO
{do we understand what to say precisely about bidirectional type inference?}

\subsection{Principality tracking in \OCaml}

\Xdidier{The point of that is to enforce a directional type inference that
is based on let-bindings. When we check that the level is generic, we check
that we already ``closed'' this thing, it is an earlier 'let'
binding. Because we are omni-directional, we don't have principality issues
anymore -- except with default rules. We are principal by construction, we
never make any choice.}

\Xdidier{Principality tracking is making the choice that we are going to
make a directional let-binding-based type inference. We get rid of that, we
are omni-directional, and don't have any principality issues.}

\Xgabriel{We should not claim too much if we don't understand default
clauses well enough. It could people the impression that we hide the issue
under the carpet.}

\Xdidier{First a declarative/principal system, and then non-principal
heuristics to refine it, a two-phase process. It's fine.}

\section{Future work}
\label{sec:future-work}

% \begin{acks}
% \end{acks}


%% \bibliographystyle{ACM-Reference-Format}
\bibliography{suspended}

\end{document}

% LocalWords:  omnidirectional typecheck polymorphism Hindley Milner kinded
% LocalWords:  GADTs typechecked codomain typechecking subexpressions Bodin
% LocalWords:  monomorphic subexpression Dunfield Riboulet jfla subtyping
% LocalWords:  greek Chargueraud typable monotype polytype Garrigue Remy
% LocalWords:  impredicative polytypes minimality RCL ary Proj toplevel
% LocalWords:  typability
