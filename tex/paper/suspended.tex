%; whizzy section

%% Leave the above line for didier
%% No macros before \documentclass

\documentclass[acmsmall,screen,nonacm]{acmart}

\input{suspended.cfg}
\newcommand{\acmart}{\True}
\usepackage{suspended}

%% \Xfirstname defined in {mycomments}
%% Use either
%%   \Xfistname[text to comment]{your comment on the text}
%% or
%%   \Xfirstname{free comment}
%% Uncomment this line to hide all comments.
% \UNXXX{}

\usepackage{marginnote}

\title{Omnidirectional type inference for \ML extensions}

\begin{document}

\begin{abstract}
We propose a new concept of \emph{omnidirectional} type inference, which is
the ability to resolve \ML-style typing constraints in disorder, by
contrast with all known implementations that always fully typecheck the
bindings before the bodies of let-expressions.
%
This relies on two technical devices: \emph{partial type schemes}
and \emph{suspended match constraints}. Partial here means  the
possibility of taking an instances of a type scheme that is not yet
completely solved with a mechanism to tack and update their instances when
the type scheme is refined, incrementally.
\emph{Suspended} match constraints means a mechanism to delay the resolution of
some constraints which typically require non-local not-yet-available
contextual information and to be discharged then when we have learned enough
information from the resolution of other constraints, typically when some type
variables have been instantiated.
%
We illustrate the benefit of omnidirectional type inference for several
advanced features present in the \OCaml language, including static
overloading of record labels and datatype constructors on the one hand and
semi-explicit polymorphism on the other hand.
\end{abstract}

\maketitle


\section{Introduction}

ML polymorphism, also known as Hindley-Milner type system, has been a very
sweet spot in the design space of strongly typed programming languages, as
it enjoys the principal type property and more importantly is closed by
common program transformations such as let-contraction and let-expansion,
and permutation of the order or arguments.

Over the years, many extensions of \ML have been proposed.  While some of
them, such as flexible records, higher-kinded types, or dimensional types
fits nicely into the \ML framework, others such as GADTs or semi-explicit
polymorphism, or overloading of datatype constructors and record fields, are
less satisfactory, since they require to twist the ML-style specification
with some direct or indirect algorithmic flavor in order to preserve
principality.

All borderline extensions have in common that they introduce new construct
that require explicit type annotations that may sometimes but not always be
omitted. They can actually be views as an elaboration into the slightly weaker
language that makes annotations mandatory, but itself first nicely into
the \ML framework. The elaboration just defines which annotations can be
omitted and rebuilds them from context.
%
The difficulty lies in finding a clear such specification that comes with a
complete effective elaboration algorithm.

The solutions proposed in those cases so far all enforce some ordering in
which type inference is performed, which can then be used to propagate both
inferred types and user-provided type annotations as known types that can be
used for disambiguation and all alleviate the need for some annotations.

\paragraph{Bidirectional type inference}

Bidirectional type inference is a known alternative to unification for
propagating type information. Type inference then receives an optional
expected type in addition to the expression to be typechecked and the
context in which it should be typechecked, When omitted, this correspond to
the usual type inference and when present it becomes a checking mode.  This
mechanisms allows to pass source type annotations but also inferred types
such as the codomain of a function in an application position during type
inference of its argument.  The typing rules must specify which type
information is passed as expected returned types which is considered as
\emph{known}.  Hence bidirectional type inference is a framework that can be
instantiated differently, depending on the choice of typing rules.  While
\OCaml uses a very limited form of bidirectional type inference, other
languages, such as \Haskell, make a more extensive use of it.


\paragraph{Poly-directional type inference}

\OCaml uses another mechanism for propagation of type
information, based on polymorphism, which actually enforces an ordering in
the typechecking of let-bindings, where the (polymorphic part of) a the
bound expression must be typed before the body of the let-expression.
We call it \emph{poly}-directional type inference.\Xdidier {let-directional?}

Let us illustrate this mechanism with the overloading of record fields in
\ML. The user may define two records with overlapping fields:
\begin{program}[input]
type one = {x : int}
type 'a two = {x : 'a; y : int}
\end{program}
This can be thought of as defining two abstract types \ocaml{foo} and
\ocaml{bar} with two access \ocaml{(_:one).x} and \ocaml{(_:two).x} of
respective types \ocaml{(one -> int)} and \ocaml{(two -> int)}, among
others.
%
In the absence of overloading the latter definition would hide the former
and we fall back in \ML. With overloading enabled, both definitions are
visible and the compiler must statically choose one of them.
%
The question is which one to choose or, equivalently, which type to infer
in the following three expressions? 
\begin{program}[input]
let e_1 r = r.x
let e_2 = let r = {x = 1} in r.x
let e_3 = (fun r -> r.x) {x = 1} ;;
\end{program}
To be able to resolve the overloading, the type of the projection should be
\emph{known} to be either \ocaml{one} or \ocaml{'a two} when typing the
projection.  Expression \ocaml[indices]{e_1} is clearly ambiguous since
there is no clue on the type of \ocaml{r} and its typechecking
fails\footnote {In fact, \OCaml uses a default resolution strategy instead
of failing when the type is ambiguous, which is to use the last
definition. Here, this will amount to chose the type \ocaml[greek]{'a two}
still leading to a failure since the record passed to the function is
actually of type \ocaml{one}. But we shall ignore default resolution
strategies for the moment.}.
%
By contrast, the only possible
type for \ocaml{r} is \ocaml {one} in both \ocaml{e_2} and \ocaml {e_3}.
Still, there is a difference.  Indeed, the type of \ocaml{e_2} is considered
to be unambiguous, while the type of \ocaml{e_3} is ambiguous. To
understand why \ocaml{e_3} fails, consider the equivalent versions of
\ocaml{e_3} where \ocaml{@@} and \ocaml{|>} are the application and the
reverse application functions.
\begin{program}[escapechar={}]
let e_4 = (fun r -> r.x) @@ {x = 1} ;;
let e_5 = {x = 1}  |> (fun r -> r.x) ;;
\end{program}
\OCaml  does not make any difference between
\ocaml[indices]{e_1}, \ocaml[indices]{e_2}, or \ocaml[indices]{e_3}
and consider all subexpressions in an application, including the function
and all arguments as being inferred simultaneously, until they are
let-bound. More precisely, polymorphic types are considered to be known
while monomorphic types are consider to be unknown (or rather
not-yet-known). In \ocaml{fun r -> r.x}, the type of \ocaml{r} in
in the subexpression \ocaml{r.x} is not-yet-known even if equal to
\ocaml{one} since this is the lambda-bound variable \ocaml{r}, hence in the
typing context, hence it is monomorphic.\footnote{Technically, all type
nodes are annotated with special $\epsilon$ variables, so that we may
distinguish between the polymorphic binding $\xa : \all \kv. \t^\kv$ that
binds $r$ to the known (raw) type $\t$ and the monomorphic binding $\xb :
\t^\kv$ that binds $\xa$ to the unknown (raw) type $\t$.}


\paragraph{Limitations of directional type inference}

Directional type inference have been in used in different languages and
somehow proven to work. Still, each of them is unsatisfactory.  The main
critics of bidirectional type inference is to make a somewhat aribtrary
choice in the specification of the flow of information, typically in
applications: should the the function be typed first and its codomain be
used to improve the typing of its argument or, on the opposite, should the
argument be typed first and be used as the type of the codomain of the
function?  There are examples when the former is a better choice and others
when the latter is preferable---but typing rules have to choose one of the
two alternative forever.  One may also allow information to flow between
multiple arguments passed to the same function, or on the opposite disallow
it entirery. Again, there is not best choice. We eventually have a complete
algorthim with respect to a spec that made somewhat arbitray choices.

Poly-directional type inference seems a better first for \ML, since it
relies on polymorphism which is the essence of \ML. Still, it does not even
allow to propagate user-type annotations! Thus \OCaml, which use
poly-directional type inference as the primary mechanisms also added a weak
form of bidirectional propagration. Besides, the implementation of
poly-directional type inference has an algorithmic cost, since the addition
of variable annotations requires to reread dags as trees distroying the
sharing that naturally resulting from unification, which may increase the
size of types and the cost of type inference. For that reason, the implement
of \OCaml cheats and is incomplete by default and the user must explicitly
require the most expensive computation of principal types when desired.

\paragraph{Omnidirectional type inference}

Both bidirectional and poly-directional type inference rely on an ordering
for type propagation that is partially specified, explicitly or implicitly,
to take advantage of user-provided type annotations and already-inferred types
to alleviated the need for extra annotations.

While they come provide complete algorithms with respect to their
specification, their specifications consist of choices that are principled,
but subjective and not principal.

Indeed, they all reject examples as ambiguous when sometimes they would be a
unique well-typed solution, as seen in the limitation of direction type
inference.  There are actually two sources of incompleteness: 
\begin{enumerate*}
\item
  the ordering between the resolution of core-language constraints that
  prevents some core information to be taken into account when resolving
  ambiguities.
\item 
  the resolution of each ambiguity, independently of others and sometimes
  just not taking all contextual type information into account. 
\end{enumerate*}
Indeed, taking overloading of record fields, for example, the following
example fails as ambiguous, while the only solution is obviously that
\ocaml{r} be \ocaml[greek]{'a two}.
\begin{program}
let fail r = (r.x = r.y)
\end{program}

Complete resolution of ambiguity may actually be impractical as the
complexity of overloading is in general is NP-hard even in the absence of
let-polymorphism,a s show by an encoding of 3-SAT problem as shown by~\citet
{Chargueraud-Bodin-Dunfield-Riboulet/jfla2025}.

We propose to use omnidirectional type inference to eliminated the first
source of incompleteness.

\paragraph{Omnidirectional type inference}

A type inference system without polymorphism, would be solely based on
unification constraints, which can be solved in any order.  The difficulty
originates from \ML let-polymorphism, all known implementations of which,
choose to infer type of let-bindings, turn then into type schemes that is
assigned to the let-bound variable to extend the typing environment in which
the body is typed. Algorithm $W$, its variant $M$ or more agnostics
constraint solving all follow this strategy.

However, it is not mandatory. In fact, type constraints can be instantiated
before being solved, but at the price of some work duplication.  Still,
this shows that the let-binding ordering is a convenience, not a
requirement.

We this allow typing constraint to be solved in disorder, which we call
omnidirectional type inference. By this, we mean that we are able to take
into account all program core-expression constraints before resolving
ambiguities.  There are two difficulties:
\begin{enumerate*}
\item
  we first must work with partial types schemes, \ie. we the ability
  to instantiate type schemes that are not yet fully determined and
  consequently revisit their instances when their are being refined,
  incrementally.
\item
  we then must allow \emph{suspended match constraints} as a way to suspend ambiguity
  resolution until sufficient information has been found from context so
  that they can be discharged and hopefully resolved.

\end{enumerate*}
Finally, once the framework established, it just remains to instantiate it
to each use case.

\Xdidier {We should continue with a brief description of both mechanisms}
\Xalistair {Should we? I think the discussions of both mechanisms could be left to later sections}

\subsection {Refinement of instantiation constraints}

\subsection {Suspended constraints} 

\Xalistair{Is this the right introduction? All the features we describe
below break principality usually. So our work is about principality. This is
a way to guarantee principal types for these advanced features.}

\Xdidier{We would like to elaborate features with difficult type-checking
into a core language where things fit exactly the ML type-inference
approach. The source language has less annotations, the target language has
more precise annotations.}

\Xalistair{For tuples we added a projection rule that gives the size of the
tuple. I imagine for polytypes there would be something like that.}



\paragraph{Structure of the paper} We proceed as follows:
\begin{itemize}
\item In Section \ref{sec:constraints}, we give an overview of suspended constraints and their 
application to various extensions for \ML. 
\item In Section \ref{sec:semantics}, we describe the constructions for a
  novel denotational semantics of suspended constraints.
\item In Section \ref{sec:language}, we define an extension of the \ML calculus featuring
  static overloading and semi-explicit first-class polymorphism along with its typing rules.
  Here we give the precise definition of the constraint generation translation and state the theorems 
  of soundness and completeness. 
\item In Section \ref{sec:solving}, we provide a formal definition of our constraint solver 
  as a series of non-deterministic rewriting rules. 
\item In Section \ref{sec:implementation}, we discuss an efficient implementation of suspended constraints.
\item In Section \ref{sec:related-work}, we discuss related work, and in
  Section \ref{sec:future-work}, we discuss future work.
\end{itemize}

\section{Suspended constraints}
\label{sec:constraints}


\TODO{PLAN: Only the constraint syntax, not constraint-solving yet. The point is to
show examples in a concrete way.}
\Xalistair{Some of this probably belongs in the intro @Didier, but I didn't want to conflict with your work}

% Constraint-based approach
Our insights into directionality come by approaching type inference from a \textit{constraint-based} \citep{TODO} 
perspective, in which type inference is split into two stages, mediated by the language of \textit{constraints} \citep{TODO}. 
In the first stage, terms $e$ are translated into constraints $C$ which are satisfiable if and only if 
the $e$ is well-typed. In the second stage, the generated constraint is solved (or shown to be unsatisfiable). 

% HMX 
A foundational approach to constraint-based type inference is $\HM(X)$ \citep{TODO} -- the Hindley-Milner system parameterized by a 
constraint domain $X$, allowing constraints to appear within types. Our work is inspired by the presentation of $\HM(X)$ by 
Pottier and R\'emy \citep{TODO}, who fully decoupled of constraint generation from constraint solving. 
Their key insight for the reduction of type inference to constraint solving is the inclusion of term variables in the language 
of constraints.

% Life is hard
\Xalistair{This paragraph now feels out of place -- nonetheless I think we need state how difficult this was :/}
Formulating a suitable constraint language for omnidirectional type inference, 
a (provably) correct constraint translation for our extensions, and an efficient, sound and 
complete solver involves several difficulties. 

\begin{mathparfig}%
  {fig:constraint-syntax}%
  {Syntax of types and constraints}
  \begin{array}{rrl@{\hspace{8em}}l}
    \cva, \cvb, \cvc &&& \text{Type variables}
    \\
    \t & ::=
         & \cv \mid \overline{\t} \Fapp ~ \mid \ldots
         & \text{Constraint types}
    \\
    \Fapp & ::=
          & \tunit \mid \cdot \tarrow \cdot \mid \dots
          & \text{Type constructors}
    \\[2em]

    C & ::=
      & \ctrue
        \mid \cfalse
        \mid C_1 \cand C_2
        \mid \cexists \cv C
        \mid \cunif \tone \ttwo
      & \text{Constraints}
      \\
      & \mid
      & \cletin x {\clam \cv {C_1}} {C_2}
        \mid \cinst x \t
      &
      \\
      & \mid
      & \cmatch \cv \Delta f
    \\
    \sh
      & ::=
      & \overline{\tv} \Fapp ~ \mid \ldots
      & \text{Shapes}
  \end{array}
\end{mathparfig}

% Syntax!
We begin by introducing the syntax of constraints in Figure ??. The constraint language contains 
tautologic constraints ($\ctrue$), unsatisfiable constraints ($\cfalse$) and logical conjunctions 
($\Cone \cand \Ctwo$). $\tone = \ttwo$ asserts that the types $\tone$ and $\ttwo$ are equal. 
The constraint form $\cexists \tv C$ binds an existentially quantified type variable in $C$.

% Constraint abstractions
There are two constructs that deal with the introduction and elimination of constraint
abstractions. As the name suggests, a constraint abstraction $\cabs \tv C$ can simply be seen as a
function which when applied to some type $\t$ returns $C[\tv := \t]$. Constraint abstractions 
are introduced by a let-construct $\cletin x {\cabs \tv \Cone} \Ctwo$  which binds the constraint abstraction
to the term variable $x$ in $\Ctwo$ -- additionally ensuring the abstraction is satisfiable. They are
eliminated using application constraints $\capp x \t$ which applies the type $\t$ to the abstraction
bound to $x$.

% Suspended match constraints
Finally, we introduce \textit{suspended match constraints}\footnote{Previously dubbed `frozen constraints' \citep{TODO}}.  
These constraints defer constraint generation until the \textit{shape}, \eg its top-level constructor, of a type becomes known (through unification), at which point 
they match on the shape to generate a new constraint. Formally, a suspended constraint is written as 
$\cmatch \t \Delta f$, where:
\begin{enumerate}
\item
  The matchee $\t$ is a type. The constraint remains suspended until the shape of $\t$ is determined. 
\item 
  The handler $f$ is a meta function from shape ($\sh$) to constraints.
  Once the constraint is unblocked, this function is applied to generate the constraint to be solved. 
  Conceptually, $f$ represents a meta-level functionalization of the match's branches.
\item
  The closure $\Delta$ is the set of term and type variables $f$ is permitted to reference. 
  The role of $\Delta$ in constraint solving is discussed further in Section \ref{sec:solving}. 
\end{enumerate}

\begin{figure}[!htpb]
  \begin{align*}
    \shape(\tv) &= \bot \\ 
    \shape(\overline{\t} \Fapp) &= \langle \overline{\tv} \Fapp, [\overline{\tv := \t}] \rangle
  \end{align*} 
  \caption{Definition of $\shape(\t)$, a partial function from types to shape-substitution pairs $\langle \sh, \theta \rangle$}
  \label{fig:shape-fn-def}
\end{figure}

% Informal semantics
Informally, the semantics of $\cmatch \t \Delta f$ are as follows: the
constraint remains suspended until the shape of $\t$ is defined (through unification), at which point
the handler is applied to this shape $f(\sh)$ and the
generated constraint is scheduled to be solved. If the shape of $\t$ is never
determined, the constraint remains unsatisfiable.

% Applications

The remainder of this section illustrates the role of suspended constraints in supporting 
language features that historically pose challenges to the preservation of principal types. 
These include:
\begin{enumerate}
  \item Constructor and record label overloading for nominal algebraic datatypes; 
  \item Overloaded tuple project, in the style of \SML; 
  \item Semi-explicit first-class polymorphism
\end{enumerate}
We demonstrate how the typablity of each of these features can be elaborated into 
constraints, formalized using a constraint generation function of the form $\cinfer e \tv$, 
which, given a term $e$ and expected type $\tv$, produces a constraint $C$ which is satisfiable if 
and only if $e$ is well-typed. A formal account of the semantics of suspended constraints 
and the declarative typing rules for these features is deferred to Sections \ref{sec:semantics}
and \ref{sec:language}, respectively. 

% When solving $\cmatch \cv \Delta f$, the solver proceeds as follows:
% \begin{enumerate}
% \item
%   If $\cv$ is already unified to a non-variable type, apply the handler
%   $f$ immediately to the resolved type.
% \item
%   If $\cv$ is still unresolved, enqueue the suspended constraint to the
%   list of suspended constraints blocked by $\cv$.
% \item
%   During unification, whenever a type variable with a pending constraint
%   becomes resolved, the solver retrieves all blocked constraints on that
%   variable and schedules them.
% \end{enumerate}
% Once all constraints have been processed, any pending suspended constraints
% that remain unsolved indicate failure.

% \paragraph{Constraint-solving: an intuition}


% \paragraph{Applications}
% \begin{itemize}
% \item
%   type-based disambiguation of data constructors (and record fields)
% \item
%   polytypes (are we sure?) \\ ensuring that there is a unique polytype
%   skeleton
% \item
%   anti-unification-based overloading (are we sure?) \\ there is some
%   uniqueness of the head / variational type
% \end{itemize}
% \Xdidier {For me, the three examples are mandatory.
% disambiguation of data constructors and record fields is too simple...
% Also, polytypes may or may not include polyparams. Polyparams are a real
% challenge because most constraints will be delayed, hence the need for a
% strategy to resolve them. Static ad hoc overloading is more general.
% }

% \Xgabriel{I care most about the semantics and implementation of the
% constraint language.} 
% \Xdidier{It would be nice to have typing rules for the surface language, and
% show that they match the constraints generated.} 
% \Xalistair{I agree.}

% For each feature:
% \begin{itemize}
% \item propose *informal* surface-language typing rules
% \item elaborate them into the constraint language
% \end{itemize}

% We will provide *precise* declarative semantics in the ``Semantics of
% constraints'' section.

\subsection{Static overloading of constuctors and record labels}

% Intro to the feature
OCaml permits on to define data types with overloaded constructor or field
names.
\begin{lstlisting}
type m = L;; type n = L;;

let x = L;;
\end{lstlisting}
The type of \code{x} is ambiguous since OCaml could either infer \code{m} or
\code{n}. This amounts to a principality issue, since there is no most
general type for \code{x}. As such, OCaml gives the following error: ...

OCaml's type checker relies of type information provided by the
bidirectional propagation of type information to resolve these
ambiguities. Thus annotating \code{x} with either \code{m} or \code{n} would
be produce a well-typed program.

% Constructor disambiguation in suspended constraints
We propose an alternative approach: suspended constraints. For the ambiguous
constructor application $\econstr K e$, we generate the constraint:
\begin{align*}
\cinfer {\econstr K e} \cva &= \cexists \cvb \cinfer e \cvb \cand \cmatch \cva {\cva, \cvb} (\lambda \overline{\cvc} \Fapp. ~ \Let ~ \sigma = \Delta(\F)(\mathsf{K}) ~ \In ~ \sigma \leq \cvb \to \cva )
\end{align*}
where the function looks up the type scheme associated with the constructor
using the type name $\F$ in the environment ($\Delta(\F)(\mathsf{K})$).

We require a unique type for each type constructor in a given algebraic data
type definition, thus there is no ambiguity with this mapping, despite there
potentially being many constructors in the environment named $\mathsf{K}$.

This not only correctly encodes OCaml's current behaviour, but does better.


\paragraph{Interaction with let-polymorphism}

Our intuition is not entirely correct here. While the above will improve on
OCaml's current inference approach, it is not powerful enough to capture the
simple idea that to resolve an ambiguous constructor we must use contextual
information to show there is a unique type constructor $\F$ for the
application.

We demonstrate this is with the following example:
\begin{lstlisting}
let foo =
  let f x = match x with L -> 1 in
  f (L : m)
;;
\end{lstlisting}
Our current approach would keep the type of \code{x} as a generalizable
variable, thus the application of \code{f} would not resolve this ambiguity,
requiring an annotation at the definition.

We require some way of splitting the head of the type from the type itself,
treating the head of the variable monomorphically and the rest
polymorphically.
\Xalistair{An explaination of kinded(1) types here }

\subsection{Semi-explicit first-class polymorphism}

% Polytypes (and some stuff about labelled types)

Semi-explicit first-class polymorphism \citep{TODO} uses \textit{annotated
types} to track the origins of polymorphic types.

The type constructor $\tpoly \sigma \varepsilon$ that boxes a polymorphic
type $\sigma$ turning it into a \textit{polytype}. Each polytype is annotated
with an $\varepsilon$ variable.  Once boxed, polytypes are considered to be
monotypes. This permits impredicative polymorphism.

To instantiate a polytype, one must use an explicit unboxing operator
$\einst e$ which instantiates the top-level quantifiers of a polytype if $e$
has the type $\tpoly \sigma \varepsilon$.

Conversely, the introduction form for polytypes is a boxing operator $\epoly
e \sigma$ with an explicity polytype annotation $\sigma$ and an annotation
$\varepsilon$, has the type $\tpoly \sigma \varepsilon$ for some arbitrary 
$\varepsilon$ if $e$ has the type $\sigma$.

Principality is tracked using generalization.  This is done by tracking the
polymorphism of labels. Type schemes quantify not only over type variables
but $\varepsilon$ variables. As a result, a term $e$ with the type $\tpoly
\sigma \varepsilon$ for some fresh $\varepsilon$ may be generalized to
$\tfor \varepsilon {\tpoly \sigma \varepsilon}$.  The typing rule for
$\einst e$ in fact requires $e$'s type to be $\tfor \varepsilon {\tpoly
\sigma \varepsilon}$, not just a plain polytype $\tpoly \sigma
\varepsilon$. The rational is that a label is polymorphic if and only if it
is principally known and not inferred.  This is why the introduction form
for polytypes $\epoly e \sigma$ is able to introduce a \textit{fresh}
$\varepsilon$ variable.

Additionally, type annotations can be used to freshen labels.  This permits
us to generalize the labels of an annotated term, indicating that the type
is principally known.

\Xalistair[]{Something about how information can flow from let bindings to use sites for principality checking}

Instead of using labelled types to infer whether the type is principally
known, we may use suspended constraints.  Typechecking $\epoly e \sigma$ is
simple:
\begin{align*}
  \cinfer {\epoly e {\tfor {\overline{\cvb}} \t}} \cva &= \left(\cfor {\overline{\cvb}} \cinfer e \t \right) \cand \cva = [\tfor {\overline{\cvb}} \t]
\end{align*}

If $e$ is known to have the type $[\sigma]$, then we can simply instantiate
it. However, if the type of $e$ is not principally known, then we must
wait. This is how our approach differs from OCaml's / labelled types.

By waiting for $e$'s type to be known, we guarantee principal types without
the machinery of labelled types.  So we can simply write the constraint
generated by $\einst e$ as:
\begin{align*}
  \cinfer {\einst e} \cva &= \cexists \cvb \cinfer e \cvb \cand \cmatch
  \cvb {\cva} (\lambda [\sigma]. ~ \sigma \leq \cva)
\end{align*}

\section{Semantics of constraints}
\label{sec:semantics}


A na\"ive denotation semantics for suspended constraints would be to simply
apply the assignment of $\cva$ to $f$. More formally,
\begin{mathpar}
  \inferrule*
    {\semenv(\cva) = \overline{\t} \Fapp \\ \semenv_{\setminus \Delta}[\overline{\cvb := \t}] \vdash f(\overline{\cvb} \Fapp)}
    {\semenv \vdash \cmatch \cva \Delta f}
\end{mathpar}

However, our informal solver would be incomplete with respect to this
semantics.  We show this by considering the constraint $\cexists \cv
\cmatch \cv \cv (\lambda \_.~ \cv = 1)$.  This constraint does not
unify $\cv$ outside the suspended constraint, but once it is scheduled,
the variable $\cv$ must be unified with the unit type $1$. So our
informal solver suggests that this constraint is unsatisfiable since
$\cv$ is never unified outside the match constraint. Yet we can use the
above rule to show that this constraint is satisfiable:
\begin{mathpar}
  \inferrule*
    {
      \inferrule*
        {
          (\cv \mapsto 1)(\cv) = 1 \\
          \inferrule*
            {(\cv \mapsto 1)(\cv) = 1} {\cv \mapsto 1 \vdash \cv
            = 1}
        }
        {\cv \mapsto 1 \vdash \cmatch \cv \cv (\lambda \_. ~ \cv
        = 1)}
    }
    {\cdot \vdash \cexists \cv \cmatch \cv \cv (\lambda \_.~ \cv = 1)}
\end{mathpar}

The dependency of suspended constraints can also be complex, as in the
following example: $\cexists {\cva, \cvb} \cmatch \cva \cvb {(\lambda
\_. ~ \cvb = 1)} \cand \cmatch \cvb \cva {(\lambda \_. ~ \cva = 1)}$.
Worse still, is that the binding location of $\cva$ may not even contain
the sufficient information to show that $\cva$ is realised.  For instance,
\begin{align*}
  \cexists \cvb \cvb = 1 \cand \cexists \cva \cva = \cvb \cand \cmatch \cva \Delta f
\end{align*}

Our conclusion is that the na\"ive semantics should be rejected in favour
for the semantics of our informal solver. Our intent with suspended
constraints is that the blocking variable should not be guessed out of thin
air, but deduced from the surrounding context without knowledge of the
constraint generated by the handler.

The idea, inspired by type systems that require principality (such as
FreezeML), is to stipulate that the only satisfying assignments for the
context of the suspended constraint requires $\cv$ to be assigned to some
type with a unique type constructor.
\Xalistair[]{This doesn't read entirely correctly, we want a unique constructor for all assignments}.

This motivates our next definition: the principal realisation of a type
variable $\cv$ by some constraint context $\cctx[-]$.

\begin{definition}
  A type variable $\cv$ is principally realised with the constructor $\F$
  by the context $\cctx[-]$, written $\cctx[\cv !] \rhd \F$,
  iff for all assignments $\semenv$, there exists an assignment $\semenvp$ for the
  hole such that $\semenv \vdash \cctx[\semenvp \vdash \ctrue]$ implies
  $\semenvp(\cv) = \overline{\t} \Fapp$
\end{definition}

\TODO
{Here, we not only rely on the definition of a constraint with a hole
$\mathcal{C}[-]$ but a judgement with a hole and some way to translate
between the two. We'll need formal definitions for this.}

\TODO
{Some examples of principal realisation}

As we've previously hinted, the semantics for a suspended constraint must
require that the surrounding context principally realises the matchee. This
is formally specified as:
\begin{mathpar}
\inferrule*
   {\cctx[\cva !] \rhd \F \\
    \semenv \vdash \cctx[\semenvp_{\setminus \Delta}[\overline{\cvb :=
      \t}] \vdash f(\overline{\cvb} \Fapp)] \\
    \semenvp(\cva) = \overline{\t} \Fapp}
   {\semenv \vdash \cctx[\semenvp \vdash \cmatch \cva \Delta f]}
\end{mathpar}

\TODO: precise declarative semantics of constraints. They should coincide
with the semantics of the typing rules.

Alistair has yet another proposal derived from his simplification of his
timestamp semantics.

\TODO
{derive precise declarative semantics for the language features of
Section 2.}

\begin{mathparfig}%
  {fig:constraint-semantics}%
  {Semantics of constraints (without suspended constraints)}
  \begin{array}{rrl@{\hspace{8em}}l}
    \semenv
      & ::=
      & \cdot \mid \semenv, \alpha := \tau
      & \text{Semantic environment}
    \\[2em]
    \square,\semmetav &&& \text{Linear constraint meta-variable}
    \\
    \semmetaenv
      & ::=
      & \cdot \mid \semmetaenv, (\semenv \vdash \semmetav)
      & \text{Linear meta-variable environment}
    \\[2em]
  \end{array}
\\
  \infer
  {\semenv(\tone) = \semenv(\ttwo)}
  {\cdot; \semenv \vdash \tone = \ttwo}

  \infer
  {\semmetaenv_1; \semenv \vdash \cone
   \and
   \semmetaenv_2; \semenv \vdash \ctwo}
  {\semmetaenv_1 \uplus \semmetaenv_2; \semenv \vdash \cone \cand \ctwo}

  \infer
  {\semmetaenv; \semenv[\alpha := \tau] \vdash \c}
  {\semmetaenv; \semenv \vdash \cexists \cv \c}

  \infer
  { }
  {(\semenv \vdash \semmetav) ; \semenv \vdash \semmetav}
\end{mathparfig}

Meta-substitution lemma: if $\semmetav$ occurs exactly once in $\c$, then $\semmetaenv, (\semenvp \vdash \semmetav); \semenv \vdash C$ and $\semenvp \vdash \cp$ if and only if $\semmetaenv; \semenv \vdash \c[\semmetav := \cp]$.

Convention: we write $\c[\cp]$ for $\c[\square := \cp]$. In particular $\c[\square] = \c$ when $\square$ occurs once in $\c$.

Next steps:
\begin{itemize}
\item introduce the condition
  $\c[\cv !]$:
  \begin{mathpar}
    \begin{array}{l}
    \exists! \F, \quad \forall \semenv, \semenvp,\\ \qquad (\semenvp \vdash \square); \semenv \vdash \c[\square] \implies \mathsf{head}(\semenvp(\cv)) = \F
    \end{array}
  \end{mathpar}
\item and finally the global rule
  \begin{mathpar}
    \infer
    {\c[\cva !]
      \and \semenvp(\cva) = \overline{\t} \Fapp
      \and \semmetaenv, (\semenvp, \overline{\cvb := \t} \vdash \square)\vdash \c[f(\overline{\cvb} \Fapp)] }
    {\semmetaenv, (\semenvp \vdash \square); \semenv \vdash \c[\cmatch \cva \Delta f]}
  \end{mathpar}
\end{itemize}

The intuition for the judgment $\semmetaenv, (\semenvp \vdash \square); \semenv \vdash \c[\cp]$ is that it is a proof of satisfiability of $\c[\square := \cp]$ with valuation $\semenv$, where we furthermore know that the $\cp$ sub-derivation has the valuation $\semenvp$. Note that, after the the global rule is added, the derivations of this judgment are not syntactically directed by $\c$ anymore.

\section{The language}
\label{sec:language}

% Running example: tuple projection disambiguation 

% We need a spec, but this itself is hard
In order to show our approach is sound and complete with respect to our
constraint generation translation, we must first define a formal specification in 
the form of a calculus and accompanying type system. Surprisngly, identifying an appropriate 
declarative type system to use as a specification is itself an interesting problem! 

% Why is it hard? Examples of prev work:
Much work has been done on various extensions of \ML with features such as static overloading \citep{TODO} 
and first-class polymorphism \citep{TODO}. However, many of these approaches either fail to ensure 
principality or require intricate menachisms -- such as labelled types -- to do so.  

% Why do naive approaches not guarantee principal types. 
Na\"ive specifications, though accessible, often lack principal types. Take 
overloaded tuple projects \textit{\`a la \SML}: an expression $\efield e j$ governed by the rule: 
\begin{mathpar}
  \infer 
    {\Gamma \vdash e : \Pi_{i = 1}^n \ti \and 1 \leq j \leq n}
    {\Gamma \vdash \efield e l : \tj}
\end{mathpar}
admits many typings, as any tuple of at least size $j$ satisfies the premise. This multiplicity undermines principality. 

% Partial annotations (shapes) to the rescue!
Our first insight is that partial type annotations often sufficies to recover principality. We refer 
to these annotations as \textit{shapes}. For example, explicitly annotating the tuple projection with 
its arity, as in $\exfield e n j$, disambiguates the expression:
\begin{mathpar}
   \inferrule* 
      {\Gamma \vdash e : \Pi_{i = 1}^n \ti \\ 1 \leq j \leq n }
      {\Gamma \vdash \exfield e n j : \tj}
\end{mathpar}
Yet, requiring users to write such annotations manually is impractical. 

% Inference of shapes
Our solution is to permit \textit{inference} of shapes, provided that the inferred annotation is \textit{uniquely determined by context}. 
This approach ensures principality by construction and yields a clear specification with predictable annotability requirements for the 
everyday programmer. 

% A note on stratified type inference 
The idea of leveraging a principled core language with inference of shapes is not novel. \textit{Stratified type inference} [??] 
pursued a similar goal (in the context of generalized algebraic data types). It operates in two phases: 
the first phase transforms the 
program into an intermediate language and generates shapes using \textit{bidirectional typing}; the second performs \HM type inference for 
the intermediate language. While the second phase is sound, complete and enjoys principal types,  
the overall system remains incomplete due to the heuristic nature of the first phase. 

% How do we specify our intuition?
Specifying the notion of unique inferrability from context is tricky. Luckily, we can leverage many of the formal methods developed in 
the previous section -- in particular, the manipulation of contexts within the type derivation -- to prove uniqueness of shapes. Allowing 
us to systematically replace the ambiguous constructs with their disambiguated counterparts: 
\begin{mathpar}
  \inferrule* 
    {E[e \mathop{!} \Pi^n] \\ \Gamma \vdash E[\exfield e n j] : \t}
    {\Gamma \vdash E[\efield e j] : \t}
\end{mathpar}
The intuition here is that the context $E$ is sufficiently large to constrain the shape of $e$'s type 
as a tuple of size $n$. 
\Xalistair{Some notation trickiness here }

% Limitations
The cheif limitation of our approach is that disambiguation can still require explicit annotations 
in inherently ambiguous scenarios. One solution is the use of sensible `defaults', such as resolving $\efield e j$ 
by assuming the tuple has arity $j$. While appealing, such defaults compromise principality. We return to this pragmatic 
tradeoff in Section \ref{sec:implementation}.

\[
\begin{bnf}
  \bnflabel{Terms} & 
    \begin{bnfalign}
      e \bnfis & x \mid () \mid \efun x e \mid \eapp \eone \etwo \mid \elet x \eone \etwo \mid (e : \t) \\
          \mid & \erecord { \overline{l = e} } \mid \efield e l \mid \exfield e \F l \\
          \mid & (\eone, \ldots, \en) \mid \efield e j \mid \exfield e n j \\
          \mid & \epoly e { \exists \overline{\tv}. ~ \sigma } \mid \einst e \mid \exinst e {\overline{\tv}} \sigma
    \end{bnfalign}
    \\
  \bnflabel{Types} & 
    \begin{bnfalign}
      \t \bnfis & \tv \mid 1 \mid \tone \to \ttwo \mid \overline{\t} \Fapp ~ \mid \Pi_{i = 1}^n \ti \mid [\sigma] 
    \end{bnfalign}
    \\ 
  \bnflabel{Type schemes} & \sigma \bnfis \t \mid \tfor \tv \sigma \\
  \bnflabel{Contexts} & \Gamma \bnfis \cdot \mid \Gamma, x : \sigma \mid \Gamma, \tv 
\end{bnf}
\]

\paragraph{Terms}

In Figure ??, we give the grammar for our calculus. Terms include all of the \ML calculus: 
variables $x$, the unit literal $\eunit$, lambda abstractions $\efun x
e$, applications $\eapp \eone \etwo$, annotations $(e : \t)$ and let
bindings $\elet x \eone \etwo$. Our extensions include:
\begin{enumerate}
  \item Constructor and record label disambiguation, modelled using record literals $\erecord { \labone = \eone; \ldots; \labn = \en }$
  and field projections $\efield e l$. 

  \item Tuples $(\eone, \ldots, \en)$ with overloaded tuple projections $\eproj e i$. 

  \item For semi-explicit first-class polymorphism, we have the boxing construct $\epoly
e \sigma$ and the unboxing construct $\einst e$.
\end{enumerate}
Each construct that endangers principality has an explicitly annotated counterpart. 

\paragraph{Types} As usual, types are split into monotypes (or just types) and type scehems. Types $\t$ include the variables $\tv$, the unit type $1$, functions $\tone \to \ttwo$, $n$-ary products $\Pi_{i = 1}^n \ti$ and 
nominal record types $\overline{\t} \Fapp$. Types schemes $\sigma$ extend monotypes allowing the universal quantification of zero or more type 
variables $\tfor {\overline{\tv}} \t$. We write $\tv, \beta, \gamma$, etc. for type variables. We also have polytypes 
\textit{without the labels} $[\sigma]$ as monotypes. 

\paragraph{Contexts} Contexts $\Gamma$ are an ordered sequence of expression variable typings $x : \sigma$ and 
universally quantified type variables. Our presentation of \ML differs from most traditional ones that permit type variables 
to ``float around''. Instead, we explicitly track the type variables in scope in the type context $\Gamma$. 

\paragraph{Typing}




\begin{mathpar}
  \inferrule*
    {x : \sigma \in \Gamma}
    {\Gamma \vdash x : \sigma}

  \inferrule*
    {\Gamma \vdash \tone \\ \Gamma, x : \tone \vdash e : \ttwo }
    {\Gamma \vdash \efun x e : \tone \to \ttwo}

  \inferrule* 
    {\Gamma \vdash \eone : \tone \to \ttwo \\ \Gamma \vdash \etwo : \tone}
    {\Gamma \vdash \eapp \eone \etwo : \ttwo}
  
  \inferrule* 
    {\;}
    {\Gamma \vdash () : 1}

  \inferrule*
    {\Gamma \vdash e : \t \\ \Gamma \vdash \t}
    {\Gamma \vdash (e : \t) : \t}

  \inferrule*
    {\Gamma, \tv \vdash e : \sigma \\ \tv \disjoint \Gamma}
    {\Gamma \vdash e : \tfor \tv \sigma}
  
  \inferrule*   
    {\Gamma \vdash e : \sigmaone \\ \Gamma \vdash \sigmaone \leq \sigmatwo}
    {\Gamma \vdash e : \sigmatwo}

  \inferrule* 
    {\Gamma \vdash \eone : \sigma \\ \Gamma, x : \sigma \vdash \etwo : \t}
    {\Gamma \vdash \elet x \eone \etwo : \t}

  \inferrule*   
    {\forall 1 \leq i \leq n, \; \Gamma \vdash \ei : \ti}
    {\Gamma \vdash (\eone, \ldots, \en) : \Pi_{i = 1}^{n} \ti}

  \inferrule* 
    {\Gamma \vdash e : \Pi_{i = 1}^n \ti \\ 1 \leq j \leq n}
    {\Gamma \vdash e.n.j : \tj}
    
  \inferrule* 
    {\Gamma \vdash e : \sigma[\overline{\tv := \t}] \\ \Gamma \vdash \overline{\t}}
    {\Gamma \vdash \epoly e { \exists \overline{\tv}. ~ \sigma } : [\sigma[\overline{\tv := \t}]]}

  \inferrule* 
    {\Gamma \vdash e : [\sigma[\overline{\tv := \t}]] \\ \Gamma \vdash \sigma[\overline{\tv := \t}] \leq \tp}
    {\Gamma \vdash \exinst e {\overline{\tv}} \sigma : \tp }

  \inferrule*    
    {E[e \mathop{!} \Pi^n] \\ \Gamma \vdash E[\exfield e n j] : \t}
    {\Gamma \vdash E[\efield e j] : \t}

  \inferrule* 
    {E[e \mathop{!} \exists \overline{\tv}. ~ \sigma] \\ \Gamma \vdash E[\exinst e {\overline{\tv}} \sigma] : \t}
    {\Gamma \vdash E[\einst e] : \t}
\end{mathpar}

The shape of a type scheme is $\exists \overline{\tv}. ~\sigma$, where $\overline{\tv}$ gives the closure. 
We write $\langle \overline{\t}, \exists \overline{\tv}. ~ \sigma \rangle$ for the type scheme $\sigma[\overline{\tv := \t}]$. 

We also add a well-typed hole judgement (this is analogous to using $\ctrue$ in suspended constraints semantics): 
\begin{mathpar}
  \inferrule*   
    {\Gamma \vdash e : \tp}
    {\Gamma \vdash \{ e : \tp \} : \t} 
\end{mathpar}
Then we can define: 
\begin{align*}
  E[e \mathop{!} s] := \exists! s'.\; &s = s' \wedge \forall \Gamma, \Gamma', \t, \tp. \\
  &\quad \Gamma \vdash E[\Gamma' \vdash \{ e : \tp \} : \_] : \t \implies \textsf{shape}(\tp) = s'
\end{align*}
What is a shape? 

\begin{align*}
  s \bnfis \Pi^n \mid \exists \overline{\tv}. ~ [\sigma]
\end{align*}
Shapes are equal modulo alpha equivalence / removal of redundant variables. 
Shapes are only defined for n-ary tuples and polytypes. 


\section{Solving constraints}
\label{sec:solving}

\TODO
{a declarative solver.}

\section{HM Prototype and discussion}
\label{sec:implementation}

\subsection{Default clauses}

\section{Semi-unification Prototype}

Rank-2 polymorphic type inference with semi-unification!

\Xdidier{When you do rank-2 unification, you don't need a 'let' anymore
because 'lambda' is enough. You have rank-2 types but you cannot bind
them. It's not as exciting as I thought it would be.}

\section{Related work}
\label{sec:related-work}


\subsection{Suspended constraints in \textsc{OutsideIn}}

\Xalistair{They first solve simple constraints (existentials,
unification). Then they solve ``... constraints'', with implication
constraints, which corresponds to the case of GADT matches. Crucially, they
abandon local let generalization.}

\subsection{Suspended constraints in dependent-type systems}


\subsection{Bi-directional type inference}

\Xgabriel{Our approach should have better properties for disambiguation, but
which ones?}

In the simply-typed case our system works better. But with generalization
you can have more issues.

\Xdidier{Should we try to extend what we did to predicative polymorphism?}

\Xalistair{Not ready yet and would probably need too much space.}

\Xgabriel{CoreML + bidirectional disambiguation of constructors?}

\TODO
{do we understand what to say precisely about bidirectional type inference?}

\subsection{Principality tracking in OCaml}

\Xdidier{The point of that is to enforce a directional type inference that
is based on let-bindings. When we check that the level is generic, we check
that we already ``closed'' this thing, it is an earlier 'let'
binding. Because we are omni-directional, we don't have principality issues
anymore -- except with default rules. We are principal by construction, we
never make any choice.}

\Xdidier{Principality tracking is making the choice that we are going to
make a directional let-binding-based type inference. We get rid of that, we
are omni-directional, and don't have any principality issues.}

\Xgabriel{We should not claim too much if we don't understand default
clauses well enough. It could people the impression that we hide the issue
under the carpet.}

\Xdidier{First a declarative/principal system, and then non-principal
heuristics to refine it, a two-phase process. It's fine.}

\section{Future work}
\label{sec:future-work}

% \begin{acks}
% \end{acks}


%% \bibliographystyle{ACM-Reference-Format}
\bibliography{suspended}

\end{document}

% LocalWords:  omnidirectional typecheck polymorphism Hindley Milner kinded
% LocalWords:  GADTs typechecked codomain typechecking subexpressions Bodin
% LocalWords:  monomorphic subexpression Dunfield Riboulet jfla
