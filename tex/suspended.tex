%; whizzy section

%% Leave the above line for didier
%% No macros before \documentclass

\documentclass[acmsmall,screen,nonacm]{acmart}

\input{suspended.cfg}
\newcommand{\acmart}{\True}
\usepackage{suspended}

%% \Xfirstname defined in {mycomments}
%% Use either
%%   \Xfistname[text to comment]{your comment on the text}
%% or
%%   \Xfirstname{free comment}
%% Uncomment this line to hide all comments.
% \UNXXX{}

\usepackage{marginnote}

\title{Omnidirectional type inference
   for fragile \ML type features
   %% : fragile yet principal
   %% for \ML extensions
   %% for a better propagation of type annotations
   %% for principality
   %% with suspended constraints
   %% for static overloading and semi-explicit first-class polymorphism
}

\begin{document}

\begin{abstract}
We propose a new concept of \emph{omnidirectional} type inference, which is
the ability to resolve \ML-style typing constraints in disorder, by
contrast with all known implementations that always typecheck the
bindings before the bodies of let-expressions.
%
This relies on two technical devices: \emph{partial type schemes}
and \emph{suspended match constraints}. Partial here means  the
possibility of taking an instances of a type scheme that is not yet
completely solved with a mechanism to tack and update their instances when
the type scheme is refined, incrementally.
\emph{Suspended} match constraints means a mechanism to delay the resolution of
some constraints which typically require non-local not-yet-available
contextual information and to be discharged then when we have learned enough
information from the resolution of other constraints, typically when some type
variables have been instantiated.
%
We illustrate the benefit of omnidirectional type inference for several
advanced features present in the \OCaml language, including static
overloading of record labels and datatype constructors on the one hand and
semi-explicit polymorphism on the other hand.
\end{abstract}

\maketitle

\Xgabriel{On the title, I still prefer ``for ML extensions'' or ``with
suspended constraint'' to the mouthful that we have here.}
\Xdidier{\emph{Fragile \ML features} does not mean much---even though we define
these constraints as fragile later on, this is not a know concept}
\Xdidier{We could replace \emph{for fragile \ML features}
by \emph{\ML extensions}}
\Xdidier{and add \emph{that require source type annotations}}

\Xdidier{Other options:
\begin{itemize}
\item
introduce the world \emph{principality}: all extensions are
elaboration like: the all allow a more complete approximation of the natural
deifnition. That is they allow to be principal for a more principal
definition of principality.
\item
introduce \emph{type annotation propagation}, which is another common
feature of all the extensions.
\end{itemize}}

\Xalistair{Veto ``with suspended constraints'' -- omnidirectional is the new
word we're introducing in the title (we should minimize new jargon in the
title)}\Xdidier{ok}


\Xdidier
  {\emph{first-class polytypes} is not best,
   \emph{semi-explicit first-class polymorphism} is better but too long.
   \emph{semi-explicit polytmorphism} or
   \emph{first-class polytmorphism} are both a but inexact....
   Which one should we use?
   Alistair says polytypes doesn't fit the definition of $\mathsf{rank}(k)$
  types and I agree.}

\Xgabriel{I would use ``polytypes'' for concision, after defining the
concept in more details on the first occurrence.}
\Xdidier{Then, we could complete the title with \emph{such as static
overloading and polytypes} }

\begin{version}[Titles]{\blue\True}
\newcommand {\Optional}[1]{{\em [#1]}}
\newcommand {\likes}[1]{\hfill\textbf{#1}}
Vote by capitalizing or uncapitalizing
your initial in the macro \likes{a, d, g}.

  \Xalistair{I assume capitialized means you like the title}

\medskip
\noindent \textbf{Omnidirectional type inference}
\begin{itemize}
\item : Principality revisited for \Optional{advanced} \ML extensions
        \Optional {with optional type annotations}
    \likes {A, D, g}
    \Xalistair{without the optional suffix}


\item for more principal types
      \Optional{in extensions of \ML with optional type annotations}
\likes {a, D, g}

\item for a more principal principal-type property
\likes {a, d, g}

\item for fragile \ML type features
\likes {a, d,G}

\item for \Optional{fragile} \ML extensions \Optional {with optional type annotations}
\likes {A, D, g}

\item for improved typing of static overloading and polytypes
\likes {a, D, g}

\end{itemize}

\end{version}

\section{Introduction}

ML polymorphism, also known as Hindley-Milner type system, has been a very
sweet spot in the design space of strongly typed programming languages, as
it enjoys the principal type property and, more importantly, is closed by
common program transformations such as let-contraction and let-expansion,
and permutation of the order or arguments.

Over the years, many extensions of \ML have been proposed. Some of the new
constructs, such as extensible records with row-polymorphism, higher-kinded
types, or dimensional types, fit perfectly into the \ML framework. Others
such as GADTs, semi-explicit polymorphism, first-class modules or static
overloading of datatype constructors and record fields, are
\emph{fragile}, as they sometimes require explicit type
annotations.
In order to minimize the need for type anotations, their specifications have
been twisted with some direct or indirect algorithmic flavor in order to
preserve \emph{principality}.
%
These (more or less ad-hoc) restrictions in the specification are designed
especially to reject examples that would break completeness with respect to
the type-checker implementations, but they also simultaneously reject other
examples whose type could easily be guessed.\Xgabriel{I would like to
explain more here, it is not obvious why completeness-breaking examples also
endanger principality.}  Unsurprisingly, they have historically posed
challenges to the preservation of principal types.

Optional type annotations may sometimes but not always be omitted: the
return type of overload data constructors can be annotated; polymorphic
expressions can be annotated with a type scheme; first-class module
expressions with a module signature; for GADTs, the type of the
\texttt{match} scrutinee can be annotated to a rigid type which will be
refined by type equalities in each branch. To each fragile construct
corresponds a robust construct where this type annotation is mandatory. The
robust constructs fit perfectly into the \ML framework, but are
significantly more cumbersome to use, as they always require explicit type
annotations.  Fragile constructs can be defined by elaboration into their
robust counterpart. The elaboration determines which annotations can be
omitted and rebuilt from context, a point of view already taken by~\citet
{Pottier-Regis-Gianas/stratified@popl06} in their work on stratified type
inference.
%
The difficulty of fragile constructs lies in finding a specification
that is sufficiently expressive, principled, intuitive for the user,
and for which we have a complete and effective elaboration algorithm.

The solutions proposed so far all enforce some ordering in which type
inference is performed, which can then be used to propagate both inferred
types and user-provided type annotations as \emph{known} types that can be
used for disambiguation and enable the omission of some annotations.

\paragraph{Bidirectional type inference}

Bidirectional type inference is a known alternative to unification for
propagating type information. It can be presented by adding to the type
inference algorithm an optional expected type in addition to the expression
to be typechecked and the context in which it should be typechecked.  Type
inference is said in \emph{checking mode} when the expected type is present,
and in (the usual) \emph{inference mode} when it is absent. The checking
mode allows to pass source type annotations but also previously inferred
types such as the codomain of a function in an application position during
type inference of the arguments.
%
Bidirectional type inference is actually a framework that must be
instantiated by a particular choice of modes for each of the language
construct, while there is no optimal combination of modes for the whole set
of language constructs. Once modes have been fixed, there are usually
principal solutions to the type inference problem, but with respect to a
specification that made non-principal choices.
%
Bidirectional type inference has been largely used for languages with
higher-rank polymorphism or subtyping~\TODO{citations}. Still, both \OCaml
and \Haskell only use a limited form of bidirectional type inference to
limit its downsides.


\paragraph{\Geninst-directional type inference}
\Xdidier {See other choices in \texttt{./abbreviations.sty}}
\OCaml uses another mechanism for propagation of type information,
based on polymorphism, which actually enforces an ordering in the
typechecking of let-bindings, where the (polymorphic part of) a the
bound expression must be typed before the body of the let-expression.
We call it \textbf{\geninst}-directional type inference, to mean that
polymorphic expressions must be typed before their
instances.  Initially introduced with the extension of \ML with
semi-explicit first-class polymorphism~\cite {Garrigue-Remy/poly-ml},
it has also been used for the overloading of record fields, which we
use for illustration here as it is simpler.\footnote {Semi-explicit
  polymorphism will be discussed in \cref{sec/constraints/polytypes}.}

The user may define two records with overlapping fields:
\begin{program}[input]
type 'a one = {x : 'a}
type two = {x : int; y : int}
\end{program}
This can be thought of as defining two abstract types \ocaml{'a foo} and
\ocaml{bar} with two accessor functions \ocaml{(_ : 'a one).x} and
\ocaml{(_:two).x} of respective types \ocaml{('a one -> 'a)} and
\ocaml{(two -> int)}, among others.
%
In the absence of overloading the latter definition would hide the former
and we fall back in \ML. With overloading enabled, both definitions are
visible and the compiler must statically choose one of them.
%
The question is which one to choose or, equivalently, which type to infer
in the following three expressions?
\begin{program}[input]
let e_1 r = r.x
let e_2 = let r = {x = 1} in r.x
let e_3 = (fun r -> r.x) {x = 1}
\end{program}
To be able to resolve the overloading, the type of the projection should be
\emph{known} to be either (an instance of) \ocaml{'a one} or \ocaml{one}
when typing the projection.  Expression \ocaml[indices]{e_1} is clearly
ambiguous since there is no clue on the type of \ocaml{r} and its
typechecking fails\footnote {In fact, \OCaml uses a default resolution
strategy instead of failing when the type is ambiguous, which is to use the
last definition. Here, this will amount to chose the type \ocaml{two}
still leading to a failure since the record passed to the function is
actually of type \ocaml{'a one}. But we shall ignore default resolution
strategies for the moment.}\footnote {When running examples in \OCaml,
\OCaml should be always be called with option \texttt{-principal}, since by
default \OCaml does not quite follow the theory for efficiency reasons.}.
%
By contrast, the only possible type for \ocaml{r} is \ocaml {'a one} in both
\ocaml{e_2} and \ocaml {e_3}.  Still, there is a difference.  Indeed, the
type of \ocaml{e_2} is considered to be unambiguous, while the type of
\ocaml{e_3} is ambiguous.
%
To understand why \ocaml{e_3} fails, consider the equivalent versions of
\ocaml{e_3} where \texttt{@@} and \ocaml{|>} are the application and the
reverse application functions.
\begin{program}[input,escapechar={}]
let e_4 = (fun r -> r.x) @@ {x = 1}
let e_5 = {x = 1} |> (fun r -> r.x)
\end{program}
\OCaml  does not make any difference between
\ocaml[indices]{e_1}, \ocaml[indices]{e_2}, or \ocaml[indices]{e_3}
and consider all subexpressions in an application, including the function
and all arguments as being inferred simultaneously, until they are
let-bound. More precisely, polymorphic types are considered to be known
while monomorphic types are consider to be unknown (or rather
not-yet-known). In \ocaml{fun r -> r.x}, the type of \ocaml{r} in
in the subexpression \ocaml{r.x} is not-yet-known even if equal to
\ocaml{one} since this is the lambda-bound variable~\ocaml{r}, hence in the
typing context, and therefore it is monomorphic.\footnote{Technically, all
type nodes are annotated with a special variable $\av$ called an annotation
variable, so that we may distinguish between the polymorphic binding $\xa :
\all \av. \t^\av$ that binds $r$ to the known (raw) type $\t$ and the
monomorphic binding $\xb : \t^\av$ that binds $\xa$ to the unknown (raw)
type $\t$.}


\paragraph{Limitations of directional type inference}

Directional type inference have been in used in different languages and
somehow proven to work. Still, each of them is unsatisfactory.  The main
critics of bidirectional type inference is to make a somewhat arbitrary
choice in the specification of the flow of information, typically in
applications: should the function be typed first and its codomain be
used to improve the typing of its argument or, on the opposite, should the
argument be typed first and be used as the type of the codomain of the
function?  There are examples when the former is a better choice and others
when the latter is preferable---but typing rules have to choose one of the
two alternative forever.  One may also allow information to flow between
multiple arguments passed to the same function, or on the opposite disallow
it entirety. Again, there is no best choice. We eventually have a complete
algorithm with respect to a spec that made somewhat arbitrary choices.

\Geninst-directional type inference seems a better fit for \ML, since it relies
on polymorphism which is the essence of \ML. Still, it does not even allow
to propagate user-provided type annotations from a function to its argument!
For example, the following would be rejected as ambiguous with
\geninst-directinal type inference alone:
\begin{program}[error]
let g : ('a one -> int) -> int = fun f -> f {x = 1} in g (fun r -> r.x)
\end{program}
However,, \OCaml, which use \geninst-directional type inference as the primary
mechanism, also uses a weak form of bidirectional propagation,
by passing the expected type of the argument so that
\ocaml{(fun r -> r.x)} is typed with the expected type
\ocaml{('a one -> int)} and this considered non-ambiguous.
%
Besides, the implementation of \geninst-directional type inference has an
algorithmic cost: for technical reasons, variable annotations must un-share
types (from acyclic graphs as naturally produced by unification to trees),
which may increase the size of types and the cost of type inference. For
that reason, the implementation of \OCaml cheats and is incomplete by
default. The user must explicitly pass the \texttt{-principal} flag to
require the more expensive computation of principal types when desired.

\paragraph{The relative completeness of directional type inference}

Both bidirectional and \geninst-directional type inference rely on an
ordering for type propagation that is partially specified, explicitly or
implicitly, to take advantage of user-provided type annotations and
already-inferred types to alleviated the need for extra annotations.
%
While they come with \emph{complete} algorithms, this is with respect to
their specifications, which include some choices that are subjective and may
sometimes look arbitrary.

Indeed, they all reject examples as ambiguous when sometimes they would be a
unique well-typed solution, as seen in the limitation of directional type
inference.  The first source of incompleteness results from the ordering
enforced between the resolution of core-language constraints that prevents
some core information to be taken into account when resolving ambiguities.

However, the resolution of each ambiguity is another source of
incompleteness, which is actually three-fold:
\begin{enumerate}
\item\label {item/order}
  the resolution is not considering type information from all ambigusous
  nodes into account.
\item\label {item/info}
  the resolution is not taking all contextual type information from
  each ambiguous node into account.
\item\label {item/simultaneous}
  each ambiguity node is solved independently of other, yet unresolved ones;
\end{enumerate}
Let us consider three examples:
%% let e_6 f = let g () = (f ()).x in (f : unit -> one); g ();;
%% OCaml succeeds on e_6---and so should we!
\begin{program}[error]
let e_6 r = (r.x, r.y)
let e_7 r = (r.x : bool)
let e_8 r = r.x.x
\end{program}
All examples are ambiguous for \OCaml\footnote{Its succeeds on \ocaml{e_6},
using the default choice, but it fails if we inverse the order of the type
declarations.}.  However, \ocaml{r} can only be of type
\ocaml{two} in \ocaml{e_6}. Indeed, considering the second projection first,
we should learn that \ocaml{r} is of type \ocaml{two} and since it is
$\lambda$-bound, this should then make the first projection unambiguous.
The failure is a matter of solving the constraint in the right order, which
should wishfully be eliminated by omnidirectional type inference.
This illustrates case~\ref {item/order}.
%
In
\ocaml{e_7}, the only possible type is \ocaml{one} if we take into account
the return of the projection. This illustrates case~\ref {item/info}.
However, there may be a design choice of whether we allow the return type of
the projection into account before we have resolved the projection as non
amnbiguous.

Our proposal with omnidirectional type inference\Xdidier {Mine} succeeds
on both \ocaml{e_6} and \ocaml{e_7}.
\Xgabriel{Does it? My implementation with Olivier would reject this example,
  because it is not robust to the addition of additional type definitions in
  the context. Even where there is a single type declaration with a given
  field/constructor we require an annotation.}
\Xgabriel{I would prefer to have less questionable examples, where type
information comes from the body of the let-binding rather than closed-world
reasoning. This could also be helpful to give a concrete intuition of
partial type schemes.}
\Xdidier{I disagree. We should show examples in increasing order of
difficulties, even if we don't solve them at the end. We should not cheat
and let the example we solve them all.  We should also do better than OCaml
wihout default clauses (which is the case).}

Omnidirectional still fails on \ocaml{e_8}, which is an instance of the
case~\ref {item/simultaneous} of incompleteness. Indeed, it requires to
consider both projections simultaneously to conclude that the type of
\ocaml{r} must be \ocaml{one}.

Requiring an order in which overloarding sites could be solved one after the
other is actually a reasonable design to keep type inference effective,
since the complexity of general overloading without such a restriction is
NP-hard, even in the absence of let-polymorphism, as shown by an encoding of
3-SAT problem as shown by~\citet
{Chargueraud-Bodin-Dunfield-Riboulet/jfla2025}.

\paragraph{Omnidirectional type inference}

The idea of \emph{omnidirectional} type inference is to take all program
core-expression constraints into account before resolving ambiguities. This
requires typing constraint to be solved in disorder.
%
This approach eliminates the source~\ref {item/order} of incompleteness.  It
leave design space for the source~\ref {item/info}, but intendedly keeps
source~\ref {ref/simulteanous} of incompleteness by design.

Our proposal with omnidirectional type inference will
correctly type all examples above (from \ocaml{e_1} to
\ocaml{e_7})\ignorespaces
\INFO
{See \url
{https://gitlab.inria.fr/remy/semiun/-/blob/main/su/examples/record_fields.ml}}
---without any default strategy mechanism.

In absence of polymorphism, type inference is solely based on unification
constraints which can be solved in any order; omnidirectional inference is
then natural and easy to implement.  The difficulty originates from \ML
let-polymorphism for which all known implementations choose to always infer
the type of a let-binding, turn it into a type scheme that is assigned to
the let-bound variable to extend the typing environment in which the body is
then typed. The Hindley-Milner algorithm $\mathcal{J}$, one of its variant
$\mathcal{W}$ or $\mathcal{M}$~\cite {Lee_Yi/algoM@toplas1998}, or more
flexible constraint solving algorithms all follow this strategy.

However, it is not mandatory. In fact, typing constraints as presented
by~\citet {Pottier-Remy/emlti} can be instantiated before they are fully
solved, although this comes at the cost of duplicating some computation.
Still, this shows that the let-binding ordering is a convenience, not a
requirement.

Omnidirectional type inference for \ML thus relies on two key technical devices:
\begin{enumerate*}
\item
  We introduce \emph{suspended match constraints} as a way to suspend
  ambiguity resolution until sufficient information has been found from
  context so that they can be discharged and hopefully resolved.
\item
  We work with partial types schemes, \ie we the ability to instantiate type
  schemes that are not yet fully determined and consequently revisit their
  instances when they are being refined, incrementally. This allows
  inferring parts of a \texttt{let}-body to disambiguate its definition,
  without duplicating constraint-solving work.
\end{enumerate*}

These technical devices are introduced once and for all---in a general
framework of constraint-based type inference. Each fragile \ML construct can
then be implemented as suspended constraints that expand to its robust
counterpart once the annotation has been inferred. This generality comes at
a cost, which is that everything is hard:
\begin{itemize}
\item Implementing partial types schemes (without duplicating
  constraint-solving work) is hard.
\item Giving an adequate semantics for suspended constraints is hard, as we
  must capture declaratively the intuition that some type information must be
  \emph{known} rather than \emph{guessed}.
\end{itemize}
In return, we found that the declarative semantics of suspended constraints
immediately suggests a systematic way to present user-facing typing rules
for each fragile construct, for which the implementation is correct and
complete.

\subsubsection* {Plan}

The rest of the paper is organized as follows.
\begin{enumerate*}[label={}]
\item
  In \cref{sec:constraints}, we give an overview of suspended constraints
  and their application to various extensions for \ML.
\item
  In \cref{sec:semantics}, we describe suspended constraints and their semantics.
\item
  In \cref{sec:language}, we define an extension of \ML featuring static
  overloading and semi-explicit first-class polymorphism along with its
  typing rules. Here we give the precise definition of the constraint
  generation translation and state the theorems of soundness and
  completeness.
\item
  In \cref{sec:solving}, we provide a formal definition of our constraint
  solver as a series of non-deterministic rewriting rules.
\item In \cref{sec:implementation}, we describe an efficient implementation
  of suspended constraints. We discuss some extensions of suspended
  constraints that we have prototyped but whose theory is less clear.
\item
  In \cref{sec:related-work} and \cref{sec:future-work}, we discuss related
  and futre work.
\end{enumerate*}

\Xdidier {I think we should
\begin{itemize}
\item
  Move the typing rules before the semantic of constraints.  It is simpler
  and far less technical
\item
  Move the polytype (2.2) story to the introduction after the record field
  story.
\end{itemize}
}

\subsubsection* {Our contributions}

Our contributions are
\begin{enumerate*}
\item
  A novel \emph{omnidirectional} type inference technique for
  extensions of \ML with advanced features, based on two new devices,
  suspended constraints and partial type schemes;

\item A declarative semantics of suspended constraints that captures the
  idea that they wait on information that must be propagated from the
  context, not \emph{guessed}.

  This includes, in particular, a new declarative caracterisation of
  \emph{known} type information.

\item
  A complete yet efficient constraint-solving type inference algorithm.

\item New declarative type systems for static overloading of constructors
  and fields, and for semi-explicit first-class polymorphism, and their
  implementation using suspended constraints.
\end{enumerate*}

\section{Suspended constraints: an overview}
\label{sec:constraints}

\begin{version}{}
% Old comments
\TODO{PLAN: Only the constraint syntax, not constraint-solving yet. The
point is to show examples in a concrete way.}
\Xalistair{Some of this probably belongs in the intro @Didier, but I didn't
want to conflict with your work}

\Xdidier {We need examples before we did in the technical details.
But since this section is still partly informal---no semantics, we may
then still consider as part of the intro and not described
suspended constraints earlier}

\Xdidier {The question is where do we put examples of programs that
do not typecheck in \OCaml, are expected to typecheck and that we can
typecheck (without explaining how they typecheck) ?  I gave one example for
record fields in the intro.  Should I also introduce semi-explicit
polymorphism and give an example in the intro?}

\Xalistair{I think it makes sense that this is the intro continued. As for
examples, I think the intro has enough. We should put them in the relevant
subsections  in this section.}\Xdidier {Or the other way round: I think we
need examples early enough. Or keep it as is and say in the intro that we
leave polytype examples, which are more involved for this section.}
\end{version}

% Constraint-based approach

Our insights into directionality come by approaching type inference from a
\textit{constraint-based} \citep{TODO} perspective, in which type inference
is split into two stages, mediated by the language of \textit{constraints}
\citep{TODO}.  In the first stage, a term $\e$ is translated into a constraint
$\C$ which is satisfiable if and only if the term $e$ is well-typed. In the
second stage, the generated constraint $\C$ is solved (or shown to be
unsatisfiable).

% HMX

A key framework for constraint-based type inference is \HMX
\cite{Odersky-Sulzmann-Wehr@tpos}---the Hindley-Milner system parameterized
by a constraint domain $X$, allowing constraints to appear within types. Our
work is inspired by the presentation of \HMX by \citet{Pottier-Remy/emlti}
who fully decoupled constraint generation from constraint solving. The
central innovation in their system for the reduction of type inference to
constraint solving is the inclusion of term variables in the language of
constraints.

\begin{version}{}
% Life is hard
\Xalistair{This paragraph now feels out of place -- nonetheless I think we
need state how difficult this was :/}\Xdidier {It could be move to the
introduction}
Formulating a suitable constraint language for omnidirectional type
inference, a (provably) correct constraint translation for our extensions,
and an efficient, sound and complete solver involves several difficulties.
\Xdidier
{I think what is difficult is (1) the semantics of constraints (2) the
efficient implementation, rather than just a sound and complete
implementation. In particular, efficiency requires to use ranks and an
ordering on the way constraint are solve.}

\Xdidier
{Beside we have no proof that our implementation is efficient. In fact, I
think is not cheap: Assume that a type scheme of size $n$ is solved
incrementally by adding one node at a time and that every time we add a node
we propagate it to all its instantiations. Currently, each propagation with
redo the job, ignoring what had already be done ealier. Hence you get a
factor of $n$ complexity that need not necessarily be there. Plus
constraints are copied before being simplified. Another factor...}

\Xalistair{The spec for the solver and my solver impl does not suffer from this,
it is able to track which new nodes need propagating and leaves the rest
untouched.}

\Xdidier {This is not what you told me when we talk. There may be
misunderstanding. We should discuss it. However, this is not so important.
My point about efficient implementation is that we do not have anything to
validate that the implementation is efficient for the moment.}
\end{version}

\begin{bnffig}[t]%
  {fig:constraint-syntax}%
  {Syntax of types and constraints}
  \emptyentry[Type variables]{\cva, \cvb, \cvc}{}
\\
\entry[Constraint types]{\t}{
     \cv \mid \overline{\t} \Fapp ~ \mid \ldots
}\\
\entry [Type constructors]{\Fapp}{
         \tunit
  \and   \cdot \tarrow \cdot
  \and   \dots
}\\
\entry[Constraints]{\C}{
        \ctrue
  \and  \cfalse
  \and  \Cone \cand \Ctwo
  \and  \cexists \cv \c
  \and 	\cfor \tv \c
  \and  \cunif \tone \ttwo
  \nextline
  \and  \cletin \x {\clam \cv \Cone} {\Ctwo}
  \and  \cinst \x \t
  \nextline
  \and  \cmatch \t  \D  \hf
}\\
\entry[Shapes] \sh {\tvs \F \and \ldots}
\\
\entry[Closure contexts] \D {\eset \and \D, \tv \and \D, \x}
\end{bnffig}
\Xdidier {It should be made clear somewhere whether our framework works with
recursive types. Since it does, shouldn't we state it?}

% Syntax!

Building on this foundation, we adopt a constraint language that includes
both term and type variables. Its syntax is given
in~\Cref{fig:constraint-syntax}.
%
The constraint language contains tautologic constraints ($\ctrue$),
unsatisfiable constraints ($\cfalse$), and logical conjunctions ($\cone
\cand \ctwo$). The constraint form $\cunif \tone \ttwo$ asserts that the
types $\tone$ and $\ttwo$ are equal.  The constraint form $\cexists \tv \c$
binds an existentially quantified type variable $\tv$ in $\c$, while the
constraint $\cfor \tv \c$ binds $\tv$ universally.
%
For convenience, we use the notation $\ts \leq \t$ as syntactic sugar for
the constraint $\cexists \tvs \cunif \tp \t$, where $\ts$ is a universally
quantified type $\tfor \tvs \tp$; its intended meaning is the instantiation
of the polymorphic type $\ts$.

% Constraint abstractions

There are two constructs that deal with the introduction and elimination of
constraint abstractions. As the name suggests, a constraint abstraction
$\cabs \tv \c$ can simply be seen as a function which when applied to some
type $\t$ returns $\c \where {\tv := \t}$. Constraint abstractions are
introduced by a let-construct $\cletin \x {\cabs \tv \cone} \ctwo$ which
binds the constraint abstraction to the term variable $\x$ in
$\ctwo$---additionally ensuring the abstraction is satisfiable. They are
eliminated using the application constraint $\capp \x \t$ which applies the
type $\t$ to the abstraction bound to $x$.

\Xdidier {Since you write $\capp \x \t$, instead of $\x \le \t$, then I
would write the abstraction as $\cletin {\capp \x \tv} \Cone \Ctwo$
instead of $\cletin {\cabs \tv \Cone} \Ctwo$.}

% Suspended match constraints

Finally, we introduce \textit{suspended match
constraints}\footnote {Previously dubbed `frozen constraints' \citep{TODO}}.
These constraints defer constraint generation until the \textit{shape}, \eg
the top-level constructor, of a type becomes known (through unification), at
which point the constraint is matched on the shape to generate a new
constraint. Formally, a suspended constraint is written as $\cmatch \t \D
\hf$, where:
\begin{enumerate}
\item
  The matchee $\t$ is a type. The constraint remains suspended until the
  shape of $\t$ is determined.
\item
  The handler $f$ is a meta function from shape ($\sh$) to constraints.
  Once the constraint is unblocked, this function is applied to generate the
  constraint to be solved.  Conceptually, $f$ represents a meta-level
  functionalization of the match's branches.
\item
  The closure $\D$ is the set of both term and type variables that $f$ is
  permitted to reference.  The role of $\D$ in constraint solving is
  discussed further in \cref{sec:solving}.
\end{enumerate}

\Xdidier{Why make $f$ a meta-function, and not just a concrete
function $\efun p \C$ where $p$ is a shape pattern that matches in
the shape and binds underlying type variables $\tvs$ to types $\tys$?}

\begin{mathparfig}[t]
{fig:shape-fn-def}
{Definition of $\shape(\t)$, a partial function from types to
shape-substitution pairs $\langle \sh, \theta \rangle$}
\shape(\tv) \eqdef
    \bot

\shape(\overline{\t} \Fapp) \eqdef
        \langle \overline{\tv} \Fapp, [\overline{\tv := \t}] \rangle
\end{mathparfig}

% Informal semantics

Informally, the semantics of $\cmatch \t \Delta \hf$ are as follows: the
constraint remains suspended until the shape of $\t$ is defined (through
unification), at which point the handler is applied to this shape $\hf(\sh)$
and the generated constraint is scheduled to be solved. If the shape of $\t$
is never determined, the constraint remains unsatisfiable.

% Applications

The remainder of this section illustrates the role of suspended constraints
in supporting \emph{fragile} language features as defined above.
These include:
\begin{enumerate}
  \item Semi-explicit first-class polymorphism.
  \item Constructor and record label overloading for nominal algebraic
  datatypes;
  \item Overloaded tuple projection in the style of \SML;
  \Xalistair{I think its worth noting that \SML also relies on static
    overloading, but I'm not too sure whether its worth putting
    this feature in our calculus (which is now quite large)}
  \Xdidier{We will have to discuss this in the related works. We thus need
  to understand how they solved it, ie. their spec, which I do not know.}
  \Xdidier{I agree that we should not include it. The place to mention it
  depends on what we will have to say and how they compare with record
  tuples.}
\end{enumerate}
We demonstrate how the typablity of each of these features can be elaborated
into constraints, formalized using a constraint generation function of the
form $\cinfer e \tv$, which, given a term $e$ and expected type $\tv$,
produces a constraint $C$ which is satisfiable if and only if $e$ is
well-typed. A formal account of the semantics of suspended constraints and
the declarative typing rules for these features is deferred to
\cref{sec:semantics} and \cref{sec:language}, respectively.

% When solving $\cmatch \cv \Delta \hf$, the solver proceeds as follows:
% \begin{enumerate}
% \item
%   If $\cv$ is already unified to a non-variable type, apply the handler
%   $f$ immediately to the resolved type.
% \item
%   If $\cv$ is still unresolved, enqueue the suspended constraint to the
%   list of suspended constraints blocked by $\cv$.
% \item
%   During unification, whenever a type variable with a pending constraint
%   becomes resolved, the solver retrieves all blocked constraints on that
%   variable and schedules them.
% \end{enumerate}
% Once all constraints have been processed, any pending suspended constraints
% that remain unsolved indicate failure.

% \paragraph{Constraint-solving: an intuition}


% \paragraph{Applications}
% \begin{itemize}
% \item
%   type-based disambiguation of data constructors (and record fields)
% \item
%   polytypes (are we sure?) \\ ensuring that there is a unique polytype
%   skeleton
% \item
%   anti-unification-based overloading (are we sure?) \\ there is some
%   uniqueness of the head / variational type
% \end{itemize}
% \Xdidier {For me, the three examples are mandatory.
% disambiguation of data constructors and record fields is too simple...
% Also, polytypes may or may not include polyparams. Polyparams are a real
% challenge because most constraints will be delayed, hence the need for a
% strategy to resolve them. Static ad hoc overloading is more general.
% }

% \Xgabriel{I care most about the semantics and implementation of the
% constraint language.}
% \Xdidier{It would be nice to have typing rules for the surface language, and
% show that they match the constraints generated.}
% \Xalistair{I agree.}

% For each feature:
% \begin{itemize}
% \item propose *informal* surface-language typing rules
% \item elaborate them into the constraint language
% \end{itemize}

% We will provide *precise* declarative semantics in the ``Semantics of
% constraints'' section.

\subsection{Semi-explicit first-class polymorphism}
\label {sec/constraints/polytypes}

% Intro (and annotations)
Semi-explicit first-class polymorphism \citep{Garrigue-Remy/poly-ml} uses
\textit{annotated types} to track the origins of polymorphic types.
%
The type constructor $\tapoly \ts \av$ boxes a polymorphic type
$\ts$ turning it into a \textit{polytype} annotated with the annotation
variable $\av$.  Once boxed, the polytype $\tapoly \ts \av$ is considered
a monotype, thereby enabling impredicative polymorphism. Annotation variables
may themselves be generalized, yielding type schemes such as
$\tfor \av {\tapoly \ts \av}$.


% Boxing

The introduction form for polytypes is a boxing operator $\expoly
\e {\exi \tvs \ts}$ with an explicit polytype annotation $\exi \tvs \ts$
where $\tvs$ are type variables that are free in $\ts$.
%
The resulting expression has type $\tapoly {\ts \where {\tvs \is \tys}} \av$
where $\av$ is an arbitrary (typically fresh) annotation variable and $\tys$
are arbritraty types that may be used in place of the free variables $\tvs$.
The annotation variable $\av$ can thus be generalized.  That is $\expoly \e
{\exi \tvs \ts}$ can also be assigned the type scheme $\all \av {\tapoly {\ts
\where {\tvs \is \tys}} \av}$.

% Unboxing (principality restriction)

Conversely, to instantiate a polytype expression, one must use an explicit
unboxing operator $\einst \e$, which requires no accompanying type
annotation.  However, the operator requires $e$ to have a polytype scheme of
the form $\all \av \tapoly \ts \av$ and then assigns $\einst \e$ the type
$\t$ that is an instance of $\ts$. If, by constrast, $\e$ has the type
$\tapoly \ts \av$ for some non-generalizable annotation variable $\av$, then
$\einst \e$ is ill-typed.  It is precisely the polymorphism of $\av$ that
ensures that the polytype is indeed known and not being inferred.

% Example ill-typed term

In particular $\efun \x {\einst \x}$ is not typable, since the
$\lambda$-bound variable $\x$ is assigned a monotype. Consequently, the
only admissible type for $\x$ is $x : \tapoly \ts \av$ for some $\ts$ and $\av$.
However, since $\av$ is bound in the surrounding context at the point of
typing $\einst \x$, it cannot be generalized prior to unboxing, rendering the
term ill-typed.

% Annotations

However, type annotations can be used to freshen annotation variables.
We usually omit annotation variables in annotations, since we can
implicitly introduce fresh ones in their place. For example,
$\efun {\x : \tapoly \ts {}} {\einst \x}$ which is syntactic sugar
for $\efun \x {\elet \x {(\x : \tapoly \ts {})} {\einst \x}}$, is
well-typed because the explicit annotation introduces a fresh
variable annotation $\ava$, which can then be generalized, yielding
$\tfor \ava {\tapoly \ts \ava}$.

\begin{version}{}
%% Old version
Principality is tracked using generalization.  This is done by tracking the
polymorphism of labels. Type schemes quantify not only over type variables
but $\varepsilon$ variables. As a result, a term $e$ with the type $\tapoly
\ts \varepsilon$ for some fresh $\varepsilon$ may be generalized to
$\tfor \varepsilon {\tapoly \ts \varepsilon}$.  The typing rule for
$\einst e$ in fact requires $e$'s type to be $\tfor \varepsilon {\tapoly
\ts \varepsilon}$, not just a plain polytype $\tapoly \ts
\varepsilon$. The rational is that a label is polymorphic if and only if it
is principally known and not inferred.  This is why the introduction form
for polytypes $\expoly e \ts$ is able to introduce a \textit{fresh}
$\varepsilon$ variable.
\end{version}

% Short commings of annotation variables

The very purpose of annotation variables is to distinguish \emph{known},
polymorphic polytypes from yet-unknown, monomorphic ones. However, they are
\Xdidier[somewhat difficult to understand and]{I would cut this as I
somewhat disagree...}  sensitive to the placement of type
annotation, an artifact of the fixed directionality of generalization in
\geninst-directional inference. For instance, the following two terms differ
only in the position of the annotation, yet only the one on the left-hand
side is well-typed.
\begin{mathpar}
 \efun f {\eapp {\einst {(f : \tpoly {\tfor \tv {\tv \to \tv}})}} f}

\efun f {\eapp {\einst f} {(f : \tpoly {\tfor \tv {\tv \to \tv}})}}
\end{mathpar}
The difference lies in how generalization and annotation variables interact.
In the first term, the annotation occurs in an unboxing operator introducing
fresh annotation variables and may therefore be generalized to the type
scheme $\tfor \av {\tapoly {\tfor \tv {\tv \to \tv}} \av}$, enabling
unboxing to proceed. Whereas, the second term applies the annotation to the
argument $f$, which fixes $f$'s type to the monotype $\tapoly {\tfor \tv
{\tv \to \tv}} \ava$ for some fresh annotation variable $\ava$. Because this
type is assigned to $f$ at its binding site, $\ava$ is bound in the context
when typing $\einst f$, and cannot be generalized. As unboxing requires a
generalized polytype, the second term is ill-typed despite the annotation.

% Suspended match constraints fixes this

Suspended match constraints eliminate this sensitivity to directionality,
by allowing any type information to be treated as known---rendering annotation
variables unnecessary.
Typechecking $\expoly e \ts$ is, intuitively:
\begin{mathpar}
\cinfer {\expoly \e {\tfor {\tvbs} \t}} \tva \wide\eqdef
  \parens{\cfor \tvbs \cinfer \e \t}
\cand
   \cva = \tpoly {\tfor \tvbs \t}
\end{mathpar}
If $\e$ is already known to have the type $[\ts]$, then we can simply
instantiate it.  However, if the type of $\e$ is not yet known, \ie  it is a
(possibly constrained) type variable $\tv$: then, we must defer until more
information is available.  We capture this behaviour in the following constraint:
\begin{mathpar}
\cinfer {\einst \e} \tva \wide\eqdef
    \cexists \tvb \cinfer \e \tvb
\cand
    \cmatch  \tvb {\tva} (\lambda \tpoly \ts. ~ \ts \leq \tva)
\end{mathpar}
Here, the match constraint $(\cmatchdots \tvb)$ suspends the instantiation
until $\tvb$ is resolved to be a known polytype. If it is already known, the
constraint discharges immediately and behaves like a standard instantiation
constraint.
%
By waiting for $e$'s type to be \emph{known}, we may ensure principal types
without annotation variables.

\subsection{Static overloading of constuctors and record labels}

% What do we mean by static overloading?

\emph{Static overloading} denotes a form of overloading in which resolution is
performed entirely at compile time, enabling the compiler to select a unique
implementation without relying on runtime information---in contrast to
\emph{dynamic overloading}, which defers resolution to runtime via
mechanisms such as dictionary-passing or dynamic dispatch.

% Other languages

Many languages offer statically resolved overloading to avoid the overhead
of dynamic dispatch. C++ and Java resolve overloaded functions through
compile-time specialization based on argument types. Conversely, languages
like Rust and Haskell primarily employ dynamic overloading via traits and
type classes, respectively, which can incur runtime overhead unless
optimized away by monomorphization and aggressive inlining.

% How it relates to OCaml?
\OCaml supports a limited yet useful form of static overloading for record
labels and data type constructors.
\begin{program}[input]
  type three = {x : int; y : int}
  type four  = {y : int; z : int}
  type five  = {x : int; z : int}
\end{program}
\programjoin
\begin{program}[input]
  type m = L | M
  type n = L
\end{program}

% OCaml's approach (PI-directionality)

When encountering overloaded labels or constructors, \OCaml resolves ambiguity
using local type information. This process is underpinned by \geninst-directional
inference, where type constructors carry annotation variables $\av$, written
$\tys \Fapp^\av$, to track which types are \emph{known}. As discussed in
\cref{TODO}, this mechanism allows one to deduce known types from
generalized type constructors $\tfor \av {\tys \Fapp^\av}$.

% A note on bidirectional expected type propagation

Because static overloading involves more intricate flows of information than
polytype instance, \OCaml complements \geninst-directionality with a
limited form of bidirectional propagation of expected types. This allows the
following to be well-typed:
\begin{program}[input]
  let e_8 = (function (L : m) | _ -> 1) L
\end{program}

% Closed-world reasoning
Beyond propagation, \OCaml also exploits \emph{closed-world reasoning} to resolve
ambiguities in record types. For instance:
\begin{program}[input]
  let e_9 = fun {x; z} -> {x = -z; z = x}
\end{program}
\programjoin
\begin{program}[output]
  val e_9 : five -> five
\end{program}
Here, \code{x} and \code{y} appear together only in the type \code{five},
allowing the type checker to unambiguously infer the type of \code{e_8} as \code{five -> five}.

% Default rules
If local type information and closed-world reasoning are insufficient,
\OCaml falls back to a syntactic default: it selects the most recently
defined compatible type. For example:
\begin{program}[input]
  let proj$_z$ = fun {z; _} -> z
\end{program}
\programjoin
\begin{program}[output]
  val proj$_z$ : five -> int
\end{program}
The expression is compatiable with both \code{four} and \code{five},
since each defines a field \code{z}. But \code{five} is chosen simply
because it appears later in the source.
We do not treat this behaviour as principal; accordingly, we provide
no formalisation of such ``default'' rules, though their implementation is
discussed further in \cref{sec:implementation}.

This fallback mechanism highlights the directionality of \OCaml inference.
Once the compiler selects a type, it commits to it---even if that choice
causes errors downstream:
\begin{program}[input]
  let e$_{10}$ = fun r -> let x (* infers [five] *) = r.x in x + r.y
\end{program}
\programjoin
\begin{program}[error]
  Error: The expression has type five
	 There is no field y within type five
\end{program}

% Record field disambiguation in suspended constraints
We propose an alternative account of static overloading using suspended
match constraints.  For example, in the case of an ambiguous record
projection $\efield \e \elab$, we generate the typing constraint:
\begin{mathpar}
\cinfer {\efield \e \elab} \tva \wide\eqdef
  \cexists \tvb \cinfer \e \tvb
  \cand
  \cmatch \tvb {\cva, \cvb}
    \parens
      {\clam {(\bar \tvc \Fapp)}
	{\cletin \ts {\Omega(\elab / \F)} {\ts \leq \tva \to \tvb}}
      }
\end{mathpar}
This constraint defers resolution until more information is available:
specifically, until the type constructor of $\tvb$ of the record expression
$\e$ is known.

We assume a global typing environment $\Omega$, which provides type scheme
declarations for
constructors and record fields. These are written in the form:
\begin{align*}
  \K : \tfor \tvs \t \to \tvs \Fapp &\in \Omega \\
  \elab : \tfor \tvs \t \to \tvs \Fapp &\in \Omega
\end{align*}
Since constructors and labels may be overloaded across distinct type
constructors, $\Omega$ may contain multiple such schemes for a given
constructor or label.  However, within a given data type, we assume that
each constructor or label is uniquely defined. Thus, we write $\Omega(\elab
/ \F)$ or $\Omega(\K / \F)$ to denote the unique type scheme associated with
a specific label or constructor in the context of the type $\F$.

Once the record type of $\tvb$ is known, the suspended constraint
$(\cmatchdots \tvb)$ triggers the matching clause, which retrieves the
appropriate type scheme for $\elab$ from $\Omega$, scoped to the
corresponding type $\F$. It then instantiates this scheme and imposes the
resulting constraints on the argument and return types of the label.

\Xgabriel{It would be nice to be able to show some code examples and explain
how they work in our setting, informally -- or, sometimes, how the
naive/simple elaboration is not enough.}
\TODO{Do some typing constraints for the examples presented in this section}

% Suspended constraints are better

As seen in the examples above, this approach is faithful to \OCaml's current
behaviour\footnote {modulo default disambiguation rules.}, and in fact
improves on it: certain expressions, such as \code{e}$_{10}$ are well-typed
under our account but rejected by \OCaml's current type checker.

% Next section

Suspended constraints offer a robust mechanism for dealing with extensions of
\ML that depend on a notion of \emph{known} type information. What remains
is to formalize this notion: that is, to give a precise semantics for when
a type variable is considered \emph{known}, and how suspended constraints
are discharged accordingly.


\section{Semantics of suspended constraints}
\label{sec:semantics}

% Judgement shape
The semantics of constraints is given, as is standard, by a satisfiability
judgment of the form $\semenv \vdash \c$: the constraint $\c$ is satisfied
by the solution, or valuation, $\semenv$. The semantic environment $\semenv$
maps type variables to \emph{ground} types, and term variables to constraint abstractions.

% Definition exampled
The judgment is defined, for all constraint-formers except suspended
constraints, in \cref{fig:constraint-semantics}. $\ctrue$ is satisfied in
any environment, and $\cfalse$ in none. An environment $\semenv$ satisfies
$\cone \cand \ctwo$ if it satisfies both $\cone$ and $\ctwo$. The
existential rule $\semenv \vdash \cexists \tv \c$ requires that $\semenv$
provides a ``solution'' for $\tv$: it maps it to a type $\t$ that
satisfies the expected constraints in $\c$. The universal constraint
$\cfor \tv \c$ requires $\c$ to be satisfiable under any type mapped to $\tv$
in $\semenv$. A unification constraint holds if both sides are
mapped to the exact same type under the environment.

The rule for let-bindings reflects that the constraint abstraction $\cabs
\tv \c$ must first be \emph{satisfiable}, checking that $\cone$ holds under
some instantiation of its bound variable; then the resulting constraint
abstraction is bound to $\x$ in $\semenv$ and may subsequently be used in
$\ctwo$.

Application of a constraint abstraction $\cinst x \t$ is interpreted as
follows: if the environment maps $\x$ to an abstraction $\cabs \tv \c$, then
applying $\t$ to $\x$ succeeds just when the body $\c$ of the abstraction is
satisfied after sunstituting $\tv$ for $\t$.

\begin{mathparfig}[t]%
  {fig:constraint-semantics}%
  {Semantics of constraints (without suspended constraints)}
  \begin{bnfgrammar}
    \entry[Semantic environment]{\semenv}{
      \cdot
      \and \semenv, \tv := \t
      \and \semenv, \x := \cabs \tv \c
    }
  \end{bnfgrammar}
\\
  \infer[True]
    {}
    {\semenv \vdash \ctrue}

  \infer[Conj]
    {\semenv \vdash \cone \\
     \semenv \vdash \ctwo}
    {\semenv \vdash \cone \cand \ctwo}

  \infer[Exists]
    {\semenv, \tv := \tau \vdash \c}
    {\semenv \vdash \cexists \cv \c}

  \infer[Forall]
    {\forall \t, ~ \semenv, \tv := \t \vdash \c}
    {\semenv \vdash \tfor \tv \c}

  \infer[Unif]
    {\semenv(\tone) = \semenv(\ttwo)}
    {\semenv \vdash \cunif \tone \ttwo}

  \infer[Let]
    {\semenv \vdash \exists \tv. \cone \\
     \semenv, \x :=  \cabs \tv \cone \vdash \ctwo}
    {\semenv \vdash \cletin \x {\cabs \tv \cone} \ctwo}

  \infer[App]
    {\semenv(x) = \cabs \tv \c \\
     \semenv, \alpha := \semenv(\t) \vdash \c}
    {\semenv \vdash \cinst x \t}
\end{mathparfig}

% Why this semantics is useful

While this semantics feels a bit tautological, the rules above, which omit
suspended constraints, are straightforward. It is a useful exercise to ensure
that we can give an explanation of each constraint-former in our
mathematical meta-language that is reasonably simple and declarative.
This specification is essential for proving properties of type inference,
such as the soundness and completeness of the constraint generator---as well
as for reasoning about the correctness of the solver.

% Example

Closed constraints are either satisfiable in the empty semantic environment
$(\cdot)$ or unsatisfiable. For example, consider the constraint $\cexists
\tv {\cunif \tv \tint}$, its satisfiability is established by the following
derivation:
\begin{mathpar}
  \infer*[Right=Exists]
    {\infer*[Right=Unif]
      {\infer*{}{\tint = \tint}}
      {\cdot, \tv := \tint \vdash \cunif \tv \tint}}
  {\cdot \vdash \cexists \tv \cunif \tv \tint}
\end{mathpar}

\subsection{Suspended constraints}

% Natural semantics

To extend these semantics to our suspended constraints, a first natural idea
is to propose the following rule---henceforth referred to as the
\emph{natural semantics} of suspended constraints:
\begin{mathpar}
  \infer[Susp-Nat]
  {\shape(\semenv(\tau)) = \angles {\sh,\theta} \\
   \semenv_{\mid \Delta}, \theta \vdash f(\sh)}
  {\semenv \vdash \cmatch \tau \Delta f}
\end{mathpar}
% Explain the rule
This rule states that a suspended constraint is satisfied by $\semenv$
whenever $\semenv$ resolves $\t$ to a non-variable type with
some shape $\sh$, with a corresponding substitution $\theta$ for its
subcomponents.
The match case $f(\sh)$ is then checked for satisfiability under an environment
restricted to the closure context~$\D$, denoted $\semenv_{\mid \D}$, extended
with the shape binding substitution $\theta$. In effect, $\sh$ supplies the
structural information needed to discharge the match constraint, while $\theta$
ensures the correct scoping of its constituents.

% Good example
For example, the constraint $\cexists \tv \cunif \tv \tint \cand \cmatch \tv
{} {(\lambda \wild.~ \ctrue)}$ is satisfied as follows:
\begin{mathpar}
  \infer*[Right=Conj]
   {\infer*[Left=Unif]
    {\tint = \tint}{\cdot, \tv := \tint \vdash \cunif \tv \tint} \\
    \infer*[Right=Susp-Nat]{\shape(\tint) = \angles {\tint, []} \\
     \infer*{}{\cdot, \tv := \tint \vdash \ctrue}
    }
    {\cdot, \tv := \tint \vdash \cmatch \tv {} {(\lambda \wild.~ \ctrue)}}
    }
   {\infer*[Right=Exists]
     {\cdot, \tv := \tint \vdash
      \cunif \tv \tint
      \cand
      \cmatch \tv {} {(\lambda \wild.~ \ctrue)}}
  {\cdot \vdash
     \cexists \tv \cunif \tv \tint
     \cand
     \cmatch \tv {} {(\lambda \wild.~ \ctrue)}}}
\end{mathpar}
% The problem
Unfortunately, this semantics is not satisfying because it does not match
our intution for suspended constraints, which is that they should resolve to
a concrete constraint when its matchee $\t$ becomes \emph{known} from the
surrounding context.  Consider for example the constraint $\cexists \cv
\cmatch \cv \cv (\lambda \wild.~ \cv = \tint)$.  This constraint does not
unify $\cv$ outside the suspended constraint, but once it is scheduled,
the variable $\cv$ must be unified with the type $\tint$. So our
informal solver suggests that this constraint is unsatisfiable since
$\cv$ is never unified outside the match constraint. Yet we can use the
above rule to show that this constraint is satisfiable:
\begin{mathpar}
  \infer*[Right=Exists]
    {
      \infer*[Right=Susp-Nat]
        {
          \tint = \tint \\
          \infer*[Right=Unif]
            {\tint = \tint} {\cv := \tint \vdash \cv
            = \tint}
        }
        {\cv := \tint \vdash \cmatch \cv \cv (\lambda \wild. ~ \cv
        = \tint)}
    }
    {\cdot \vdash \cexists \cv \cmatch \cv \cv (\lambda \wild.~ \cv = \tint)}
\end{mathpar}
This semantics let us \emph{guess} types instead of requiring that they are
\emph{known}. A reasonable solver (that does not attempt to guess or
backtrack) cannot match it. It also breaks the existence of principal
solutions: consider the function $\efun \x \efield \x z$, which projects the
field $z$, the semantics of the generated constraint would let us guess
any record type with the field $z$ for the type of $x$, and may be no most
general choice.\Xalistair{Maybe we could show a bit of the constraint,
simplified.}\Xdidier{This is unclear, as you are discussing overloading of
record fields without telling us which record fields have been declared.
The example would work better with tupples that are structural---or with
structural record types.}

% Contextual semantics
Rather than relying on the guesswork inherent in the natural semantics,
we adopt a \emph{contextual} semantics. This formulation ensures
the shape is \emph{known} from the surrounding context.
% Open/use rule for suspended constraints
First, we introduce a simplified variant of the natural rule (\Rule{Susp-Nat}):
\begin{mathpar}
  \infer[Susp-Use]
    {\shape(\t) = \angles {\sh, \theta} \\
     \semenv_{\mid \Delta}, \semenv(\theta) \vdash f(\sh)
    }
    {\semenv \vdash \cmatch \t \Delta f}
\end{mathpar}
Here, the rule \Rule{Susp-Use} only applies when $\tau$ already has a shape.
Unlike the natural rule, which takes the shape of $\semenv(\tau)$, which may
contain parts that have been ``guessed'' in $\semenv$, this version demands
that the shape $\sh$ of $\t$ must already be known in the constraint,
statically.

\Xdidier {The positioning is unclear: you say that this is a simplified case,
of a rule that does not work, hence we read this as a temporary useless
rule. But you use it later.  This should me made clearer.}
\Xdidier {Right now this rule is not a subcase of \Rule {Susp-Ctx} but it
would if you allowed $\C {\ty !\ty}$ and I think you should allow it since
constraints should be substituable. It would be simple to treated is as a
subcase, hence a shortcut for telling the story and writing derivations.}


% Contextual rule

The key innovation of the contextual semantics lies in the following rule, which---unlike
the others---is not syntax-directed:
\begin{mathpar}
  \infer[Susp-Ctx]
    {\Cshape \C \tv \sh \\
      \semenv \vdash \C[\cexists {\fvs \sh} \cunif \tv \sh \cand
      \cmatch \sh \Delta f]
    }
    {\semenv \vdash \C[\cmatch \tv \Delta f]}
\end{mathpar}
This rule lets us substitute the matchee variable $\tv$ by a more defined
shape $\sh$, provided the constraint context $\C$ determines $\tv$ to have
precisely this shape. The side-condition $\Cshape \C \tv \sh$ ensures this
precisely: it expresses that the context $\C$ constrains all
the solutions of $\tv$ to have the \emph{same} shape $\sh$, modulo
$\alpha$-equivalence of shapes. The
formal definition of this condition, which is somewhat technical, will be
introduced later.

In other words, we can \emph{discover}\Xdidier{Was \emph{guess} but this is
not the correct word, you do not guess in our sense, since there is no other
choice even better we are force to make this choice. (\emph{use} would also
be possible but I prefer \emph{doscover}} a shape, as in the natural semantics,
but only if ambient constraint context $\C[\square]$ guarantees that
there is a unique possible shape.\Xdidier {was: a unique shape to guess.}
% Examples
We illustrate the contextual semantics for suspended constraints
with a series of examples.
\begin{example}
Consider the two examples from before:\Xdidier{from above?}
\begin{mathpar}
\exists \tv. \tv = \tint
  \cand
  \cmatch \tv {} {(\lambda \wild. \ctrue)}

\cexists \cv \cmatch \cv \cv (\lambda \wild.~ \cv = \tint)
\end{mathpar}
In the first example, we apply the contextual rule with the context $\C :=
(\exists \tv. \tv = \tint \cand \square)$. Any solution $\phi$ of this part
of the constraint necessarily has $\phi(\tv) = \tint$, therefore the shape
of $\tv$ is uniquely determined. The side condition of \Rule{Susp-Ctx}
rule is therefore satisfied , and the suspended constraint can
be resolved.

In constrast, the second example has no contextual information around
the suspended constraint ($\C[\square] = \square$), so any solution
$\semenv$ satisfies it, and $\tv$ can take an arbitrary shape in this context.
Consequently, the uniqueness condition fails, the \Rule{Susp-Ctx} rule
cannot be applied, and the constraint remains unsatisfiable---as intended.
\end{example}
\begin{example}
Consider the more intricate example:
\begin{mathpar}
  \exists \tva \tvb.
  \Parens{\begin{array}{l}
    \quad \cmatch \tva {\tvb} {(\cabs \wild {\tvb = \tbool})} \\
    {} \cand \cmatch \tvb {} {(\cabs \wild \ctrue)} \\
    {} \cand \tva = \tint
  \end{array}}
\end{mathpar}
To prove that this constraint is satisfiable we can use the \Rule{Susp-Ctx}
twice, once with a hole around the suspension on $\tva$, and once with a
hole around the suspension on $\tvb$. In fact, this can be done in any order:
\begin{itemize}
\item
  If you focus on $\tva$ first, it is easy to prove that the context around
  it, $(\square \cand \cmatchdots \tvb \cand \tva = \tint)$, imposes a
  unique solution thanks to $\tva = \tint$. Then, we use the
  $\Rule{Susp-Ctx}$ rule in the context $(\cmatch \tint {\beta} {(\cabs
  \wild {\beta = \tbool})}) \cand \square \cand \tva = \tint)$, and we have
  to prove that $\tvb$ is uniquely determined in this context. This is in
  turn proved by using the conjunction rule followed by \Rule{Susp-Use},
  which gives us access to the unification $\tvb = \tbool$.

\item
  If you focus on $\tvb$ first, proving that $\tvb$ is uniquely determined
  in the context $(\cmatch \tva \tvb {(\cabs \wild {\tvb = \tbool})} \cand
  \square \cand \tva = \tint)$ requires using the \Rule{Susp-Ctx} again as
  in the case above. Then, we end up with the constraint $(\cmatchdots \tva
  \cand \cmatchdots \tbool \cand \tva = \tint)$, and we can use
  \Rule{Susp-Ctx} yet another time on $\alpha$.

\end{itemize}
This example illustrates that the declarative semantics supports arbitrary
orders of resolution for suspended constraints---as long as the contextual
conditions are met. This flexibility accommodates both top-down and
bottom-up reasoning styles.
\end{example}

\begin{example}
Finally, let us consider a constraint with a cyclic dependency between match
constraints:
\begin{mathpar}
  \exists \tva \tvb.
  \left(\begin{array}{l}
    \quad \cmatch \tva {\tvb} {(\cabs \wild {\tvb = \tbool})} \\
    {} \cand \cmatch \tvb {\tva} {(\cabs \wild {\tva = \tint})}
  \end{array}\right)
\end{mathpar}
This constraint can be proved satisfiable with the ``natural semantics''
that we proposed earlier, by guessing the instantiation $\tva := \tint, \tvb
:= \tbool$. Our solver fails on it, and so does our contextual
semantics. Without loss of generality, suppose that we try to use
\Rule{Susp-Ctx} on $\tva$ first: we get to prove $\Cshape \C \tva \tint$ for the
context $\C := (\square \cand \cmatch \tvb \tva {(\cabs \wild {\tva =
\tint})})$: we have to prove that $\tint$ is a unique head structure for
$\tva$ such that $\C[\ldots]$ is satisfiable. But
this does not hold, as the constraint $(\cmatch \tvb \tva {(\cabs \wild
{\tva = \tint})})$ cannot be solved: we cannot use the \Rule{Susp-Use} rule
yet as $\beta$ has no head, and we cannot use the \Rule{Susp-Ctx} rule as
the empty context $\square$ does not uniquely constrain $\tvb$.

This example illustrates a key distinction between the natural and contextual semantics:
while the former allows unconstrained guessing, the latter requires well-foundedness in constraint
dependencies, ensuring that resolutions are guided by contextual information rather than arbitrary choice.
\end{example}

\subsection{Technical details}

Having motivated and illustrated the semantics of suspended constraints,
we now turn to some of the more technical details before summarizing the
rules. We begin defining the so-called unicity predicate, the condition
under which a variable's shape can be considered uniquely determined by
a context.

\paragraph{Unicity predicate}
A key ingredient of the contextual rule \Rule{Susp-Ctx} is determining
when a type variable's shape is uniquely fixed by its surrounding
constraint context. We formalize this using the unicity predicate
$\Cshape \C \tv \sh$, which expresses that within the context
$\C[\square]$, the variable $\tv$ must have shape $\sh$ in any
satisfying assignment.

\begin{definition}
  A type variable $\tv$ has a \emph{uniquely known} shape within the context
  $\C[\square]$, written $\Cshape \C \tv \sh$, iff for all assignments
  $\semenv$ and shapes
  $\shp$, then $\semenv \vdash \C[\cexists {\fvs \shp} \cunif \tv \shp]$ implies
  that $\sh$ and $\shp$ are equal modulo alpha equivalence.
%
  In other words:
  \begin{mathpar}
    \Cshape \C \tv \sh \Wide\eqdef \forall \semenv, \shp, \uad
      \semenv \vdash \C[\cexists {\fvs \shp} \cunif \tv \shp] \implies
        \sh =_{\setminus \alpha} \shp
  \end{mathpar}
  This condition ensures that were not guessing a shape arbitrarily;
  the shape must already be determined by the ambient constraint.
  \Xdidier  [Thats what justifies applying \protect\Rule{Susp-Ctx}.]
  {I would cut this, which is out of context or misplaced}
\end{definition}

\paragraph{Well-foundedness}

The astute reader may have noticed a compilication resulting
from our definition of the unicity predicate:
$\Cshape \C \tv \sh$ contains a negative occurrence of the
satisfiability judgement, in order to express that $\sh$
is unique among all solutions of the ambient context $\C$.
%
This negative occurrence means that \emph{a priori}, the rules
from \cref{fig:constraint-semantic} including \Rule{Susp-Use}, \Rule{Susp-Ctx}
do not form an inductive definition in the usual sense.

This is not merely a technical quirk: it threatens the foundational
structure of the semantics. If left unaddressed, it would prevent
us from interpreting the rules as a well-behaved inference system.

However, it can easily be resolved by obersving that the rules, while not
syntactically well-founded, can be defined in a decreasing way with some
defined measure $|-|$. Informally, the reason for well-foundedness is that the
conclusion of \Rule{Susp-Ctx}, which is $\semenv \vdash \C[\cmatch \tv
\Delta f]$, is structurally larger than the negatively-occurring judgment
$\semenvp \vdash \C[\cexists {\fvs \shp} \cunif \tv \shp]$.

Instead of consdering the rules defined above as a \emph{single}
inductive definition, we consider them to determine a function
$\bbrackets -$ from constraints $\c$ to sets of valuations $\semenv$.
The satisfiability relation is defined as
$\semenv \vdash \c \iff \semenv \in \bbrackets \c$.
We can prove that $\bbrackets \c$ is well-defined by
metered recursion of~$\c$. Furthermore, we show that the
inference rules we have so far defined hold and are
invertible.  We give the full details for this in \Cref{appendix:wf}.
This ensures that the semantics is well-defined, even if not syntactically
inductive.

That said, well-foundedness doesn't preclude infinite derivations in
practice. For example, the constraint
$$
  \c_\text{loop} \eqdef \cexists \tv \cunif \tv \tint
    \cand \cmatch \tv {} {(\lambda \wild.~ \c_\text{loop})}
$$
unfolds into an infinite sequence of match constraints, and thus has no
finite proof as satisfiability. This form of non-termination in the
semantics is exploited using the meta-language as opposed to a flaw in the
object language \ie constraints.

\Xdidier{I don't understand $\C_{\text{loop}}$: this cannot be written since
there is not recursion in the language of constraints}.
\Xdidier{Actually, I don't understand what $f$ is in the meta language and
why we do not express is as a constraint}

\paragraph{Interaction with polymorphism}

\Xalistair{I'm not too sure this belongs here, but I think
it is a tricky part of our semantics}

Suspended constraints interact subtly with polymorphism
and let-generalisation. Though our semantics appears modular,
it is carefully crafted to give us the desired properties.
%
% Partially generalised type schemes (example)
To illustrate, let us consider again \code{e}$_{10}$:
\begin{program}[input]
  type three = { x : int; y : int }
  ...
  let e$_{10}$ r = let x = r.x in x + r.y
\end{program}
A simplified typing constraint for this program is:
\begin{mathpar}
  \cexists \tv
    \cletin x {\cabs \tvb {\cmatch \tva {\tvb} {(\lambda (\tvcs \Fapp).~ \ldots )}}}
      \cinst x \tint \cand \cunif \tv {\mathsf{three}}
\end{mathpar}
Here, $\tv$ is \code{r}'s type. The constraint suspends
until the shape of \code{r}'s type becomes available, which occurs when typing
\code{r.y} using closed-world reasoning of records.

% Generalisation
At the point of generalising \code{x}, the type $\tvb$
remains unknown: it is captured by a suspended match constraint
and has not yet been refined. We must nevertheless assign a type
scheme to $x$, yielding $\tfor \tvb \tvb$.
This appears unsound---$\tvb$ is generalised despite depending on
unresolved information. However, $\tvb$ is not `fully' generalised in
the traditional sense: it may be (and is) further constrained once the match
constraint is discharged.

To support this, we introduce our second novel mechanism for omnidirectional
type inference: \emph{partial type schemes}.  Such type schemes (\ie $x$'s
scheme) delay commitment to certain quantifications, allowing generalised
variables like $\tvb$ to be later refined (\eg, to \code{int}) once
suspended constraints are discharged. We defer the solver-level details of
this mechanism to \cref{sec:solver, sec:implementation}.  At the semantic
level, we simply note that partially generalised schemes arise naturally
from the interaction of suspended constraints and let-abstractions.

% Backpropagation

\Xdidier {Can you also have backpropagation for tuples and polytypes?}
A further subtle---but crucial---feature of our semantics is its support for
\emph{backpropagation}.
%
\Geninst-directional inference, as exemplified by \OCaml, performs inference
in a strictly let-directional manner:
each let-binding is inferred using only local information,
\Xdidier{not quite true---monorphic parts are non locally inferred, but
they are not part of the known information}\Xdidier{This repeats, less
precsisely, I think from what has been told in the introduction. Perhaps,
the introduction should be more detailed, but repeateing it here, which a
different story could be confusing} and generalisation forms a boundary that
prevents information from later use sites from influencing the generalised
type--this is why polymorphic type information is considered `known'.

\Xdidier{Then, it should be added to the intro to criticize the \OCaml approach.}
For example, the following program is rejected by \OCaml:
\begin{program}[input]
  let e$_{11}$ = let f r = r.x in f ({ x = 1; y = 1 })
\end{program}
Yet this example ought to be well-typed: the type of \code{f}'s
argument is unambiguously the record type \code{three}. The reason
this is ill-typed is that \geninst-directional inference cannot propagate
the known type of the record (\ie \code{three}) \textbf{back} to
\code{f}'s type.

In constract, our approach permits such refinement.
The (simplified) constraint generated when typing \code{e}$_{11}$ is:
\begin{mathpar}
  \cexists \tv
  \cletin f {\cabs \delta {\cexists {\tvb, \tvc}
    \cunif \delta {\tvb \to \tvc}
    \cand \cmatch \tvb {\tvc} {(\ldots)}}}
    {\cinst f {(\mathsf{three} \to \tv)}}
\end{mathpar}
Here, type information flows from an instance constraint ($\cinst f
{(\mathsf{three} \to \tv)}$) into a potentially generalisable variable
$\tvb$ within the constraint abstraction.  At generalisation time, $\tvb$
remains abstract. However, at the use site, the instance type \code{three}
is known, and thus uniquely determines the shape of $\tvb$ to be
$\mathsf{three}$. The unicity predicate can be used to show that no other
shape can be inferred, without the constraint being unsatisfiable.

This controlled backpropagation allows us to resolve the suspended constraint
\emph{without} requiring $\tvb$ to be marked as a non-generalisable variable.

\TODO{derivation}

\paragraph{Summary}

To recap, the semantics of suspended constraints is governed by three
principles:
\begin{enumerate*}
  \item If the shape of the matchee is statically known, we use it
    directly via \Rule{Susp-Use};
  \item If not, we may guess the shape--but only when the context ensures
    the shape is unique, via the $\Cshape \C \tv \sh$ condition and
    \Rule{Susp-Ctx};
  \item If the shape cannot be uniquely inferred from the context, the constraint
    is unsatisfiable.
\end{enumerate*}

These rules, together with the base rules from
\cref{fig:constraint-semantics}, define the full semantics. For convenience,
we collect all the rules pertaining to the satisfiability judgement in
\cref{fig:contextual-semantics}.

\begin{mathparfig}[t]
  {fig:contextual-semantics}
  {Semantics of constraints including suspended constraints}
  \begin{bnfgrammar}
    \entry[Semantic environment]{\semenv}{
      \cdot
      \and \semenv, \tv := \t
      \and \semenv, \x := \cabs \tv \c
    }
  \end{bnfgrammar}
\\
\infer[True]
    { }
    {\semenv \vdash \ctrue}

  \infer[Conj]
    {\semenv \vdash \cone \\
     \semenv \vdash \ctwo}
    {\semenv \vdash \cone \cand \ctwo}

  \infer[Exists]
    {\semenv, \tv := \tau \vdash \c}
    {\semenv \vdash \cexists \cv \c}

  \infer[Forall]
    {\forall \t, ~ \semenv, \tv := \t \vdash \c}
    {\semenv \vdash \tfor \tv \c}

  \infer[Unif]
    {\semenv(\tone) = \semenv(\ttwo)}
    {\semenv \vdash \cunif \tone \ttwo}

  \infer[Let]
    {\semenv \vdash \exists \tv. \cone \\
     \semenv, \x :=  \cabs \tv \cone \vdash \ctwo}
    {\semenv \vdash \cletin \x {\cabs \tv \cone} \ctwo}

  \infer[App]
    {\semenv(x) = \cabs \tv \c \\
     \semenv, \alpha := \semenv(\t) \vdash \c}
    {\semenv \vdash \cinst x \t}

  \infer[Susp-Use]
    {\shape(\t) = \angles {\sh, \theta} \\
     \semenv_{\mid \Delta}, \semenv(\theta) \vdash f(\sh)
    }
    {\semenv \vdash \cmatch \t \Delta f}

  \infer[Susp-Ctx]
    {\Cshape \C \tv \sh \\\\
      \semenv \vdash \C[\cexists {\fvs \sh} \cunif \tv \sh \cand
      \cmatch \sh \Delta f]
    }
    {\semenv \vdash \C[\cmatch \tv \Delta f]}
  \\

  \Cshape \C \tv \sh \Wide\eqdef \forall \semenv, \shp, \uad
      \semenv \vdash \C[\cexists {\fvs \shp} \cunif \tv \shp] \implies
            \sh =_{\setminus \alpha} \shp
\end{mathparfig}

% Contextual semantics is cool, and our approach extends to type systems
Together, these rules define a flexible but principled system for constraint resolution,
capable of both deferring and resolving shape information as determined by the constraint context.
Importantly, the same style of contextual reasoning extends naturally to the static semantics of terms.

\begin{version}{}
\Xgabriel{Maybe reuse what's below.}
Our conclusion is that the na\"ive semantics should be rejected in favour
for the semantics of our informal solver. Our intent with suspended
constraints is that the blocking variable should not be guessed out of thin
air, but deduced from the surrounding context without knowledge of the
constraint generated by the handler.

The idea, inspired by type systems that require principality (such as
FreezeML), is to stipulate that the only satisfying assignments for the
context of the suspended constraint requires $\cv$ to be assigned to some
type with a unique type constructor.
\Xalistair[]{This doesn't read entirely correctly, we want a unique
constructor for all assignments}.


\TODO{Show the natural-semantics rule for suspended constraints,
  explain that (1) it is not our intention for this typing construct (it
  admits ``guesses out of thin air'') (2) a non-guessing solver cannot be
  complete with respect to it.}

\TODO{Show the contextual semantics,
  first an informal explanation of $\c[\tv !]$,
  then show the full definition, then discuss the fine-grained
  points on examples.
}

\TODO{Discuss well-foundedness.}

\Xdidier{We are missing an introduction to the semantics of constraints}


A na\"ive denotation semantics for suspended constraints would be to simply
apply the assignment of $\cva$ to $f$. More formally,
\begin{mathpar}
  \inferrule*
    {\semenv(\cva) = \overline{\t} \Fapp \\ \semenv_{\setminus
    \Delta}[\overline{\cvb := \t}] \vdash f(\overline{\cvb} \Fapp)}
    {\semenv \vdash \cmatch \cva \Delta f}
\end{mathpar}
However, our informal solver would be incomplete with respect to this
semantics.  We show this by considering the constraint $\cexists \cv
\cmatch \cv \cv (\lambda \_.~ \cv = 1)$.  This constraint does not
unify $\cv$ outside the suspended constraint, but once it is scheduled,
the variable $\cv$ must be unified with the unit type $1$. So our
informal solver suggests that this constraint is unsatisfiable since
$\cv$ is never unified outside the match constraint. Yet we can use the
above rule to show that this constraint is satisfiable:
\begin{mathpar}
  \inferrule*
    {
      \inferrule*
        {
          (\cv \mapsto 1)(\cv) = 1 \\
          \inferrule*
            {(\cv \mapsto 1)(\cv) = 1} {\cv \mapsto 1 \vdash \cv
            = 1}
        }
        {\cv \mapsto 1 \vdash \cmatch \cv \cv (\lambda \_. ~ \cv
        = 1)}
    }
    {\cdot \vdash \cexists \cv \cmatch \cv \cv (\lambda \_.~ \cv = 1)}
\end{mathpar}
The dependency of suspended constraints can also be complex, as in the
following example: $\cexists {\cva, \cvb} \cmatch \cva \cvb {(\lambda
\_. ~ \cvb = 1)} \cand \cmatch \cvb \cva {(\lambda \_. ~ \cva = 1)}$.
Worse still, is that the binding location of $\cva$ may not even contain
the sufficient information to show that $\cva$ is realised.  For instance,
\begin{align*}
  \cexists \cvb \cvb = 1 \cand \cexists \cva \cva = \cvb \cand \cmatch \cva \Delta f
\end{align*}

Our conclusion is that the na\"ive semantics should be rejected in favour
for the semantics of our informal solver. Our intent with suspended
constraints is that the blocking variable should not be guessed out of thin
air, but deduced from the surrounding context without knowledge of the
constraint generated by the handler.

The idea, inspired by type systems that require principality (such as
FreezeML), is to stipulate that the only satisfying assignments for the
context of the suspended constraint requires $\cv$ to be assigned to some
type with a unique type constructor.
\Xalistair[]{This doesn't read entirely correctly, we want a unique
constructor for all assignments}.

This motivates our next definition: the principal realisation of a type
variable $\cv$ by some constraint context $\cctx[-]$.

\begin{definition}
  A type variable $\cv$ is principally realised with the constructor $\F$
  by the context $\cctx[-]$, written $\cctx[\cv !] \rhd \F$,
  iff for all assignments $\semenv$, there exists an assignment $\semenvp$
  for the
  hole such that $\semenv \vdash \cctx[\semenvp \vdash \ctrue]$ implies
  $\semenvp(\cv) = \overline{\t} \Fapp$
\end{definition}

\TODO
{Here, we not only rely on the definition of a constraint with a hole
$\mathcal{C}[-]$ but a judgement with a hole and some way to translate
between the two. We'll need formal definitions for this.}

\TODO
{Some examples of principal realisation}

As we've previously hinted, the semantics for a suspended constraint must
require that the surrounding context principally realises the matchee. This
is formally specified as:
\begin{mathpar}
\inferrule*
   {\cctx[\cva !] \rhd \F \\
    \semenv \vdash \cctx[\semenvp_{\setminus \Delta}[\overline{\cvb :=
      \t}] \vdash f(\overline{\cvb} \Fapp)] \\
    \semenvp(\cva) = \overline{\t} \Fapp}
   {\semenv \vdash \cctx[\semenvp \vdash \cmatch \cva \Delta f]}
\end{mathpar}

\TODO: precise declarative semantics of constraints. They should coincide
with the semantics of the typing rules.

Alistair has yet another proposal derived from his simplification of his
timestamp semantics.

\TODO
{derive precise declarative semantics for the language features of
Section 2.}


Meta-substitution lemma: if $\semmetav$ occurs exactly once in $\c$, then
$\semmetaenv, (\semenvp \vdash \semmetav); \semenv \vdash C$ and $\semenvp
\vdash \cp$ if and only if $\semmetaenv; \semenv \vdash \c[\semmetav :=
\cp]$.

Convention: we write $\c[\cp]$ for $\c[\square := \cp]$. In particular
$\c[\square] = \c$ when $\square$ occurs once in $\c$.

Next steps:
\begin{itemize}
\item introduce the condition
  $\c[\cv !]$:
  \begin{mathpar}
    \begin{array}{l}
    \exists! \F, \quad \forall \semenv, \semenvp,\\ \qquad (\semenvp \vdash
    \square); \semenv \vdash \c[\square] \implies
    \mathsf{head}(\semenvp(\cv)) = \F
    \end{array}
  \end{mathpar}
\item and finally the global rule
  \begin{mathpar}
    \infer
    {\c[\cva !]
      \and \semenvp(\cva) = \overline{\t} \Fapp
      \and \semmetaenv, (\semenvp, \overline{\cvb := \t} \vdash
      \square)\vdash \c[f(\overline{\cvb} \Fapp)] }
    {\semmetaenv, (\semenvp \vdash \square); \semenv \vdash \c[\cmatch \cva \Delta f]}
  \end{mathpar}
\end{itemize}

The intuition for the judgment $\semmetaenv, (\semenvp \vdash \square);
\semenv \vdash \c[\cp]$ is that it is a proof of satisfiability of
$\c[\square := \cp]$ with valuation $\semenv$, where we furthermore know
that the $\cp$ sub-derivation has the valuation $\semenvp$. Note that, after
the the global rule is added, the derivations of this judgment are not
syntactically directed by $\c$ anymore.

\end{version}
\section{The language}
\label{sec:language}

\Xdidier{We need to add structure to this section, with subsections}

% Running example: tuple projection disambiguation

% We need a spec, but this itself is hard

In order to show our approach is sound and complete with respect to our
constraint generation translation, we must first define a formal
specification in the form of a calculus and accompanying type
system. Surprisngly, identifying an appropriate declarative type system to
use as a specification is itself an interesting problem!

% Why is it hard? Examples of prev work:

\Xdidier {This paragraph is repeating (or should be added to) the
introduction}
Much work has been done on various extensions of \ML with features such as
static overloading \citep{TODO} and first-class polymorphism
\citep{TODO}. However, many of these approaches either fail to ensure
principality or require intricate menachisms -- such as labelled types -- to
do so.

% Why do naive approaches not guarantee principal types.

Na\"ive specifications, though accessible, often lack principal types. Take
overloaded tuple projects \textit{\`a la \SML}: an expression $\efield e j$
governed by the rule:
\begin{mathpar}
  \infer
    {\G \th \e : \Pi\iton \ti \and 1 \leq j \leq n}
    {\G \th \efield \e j : \tj}
\end{mathpar}
admits many typings, as any tuple of at least size $j$ satisfies the
premise. This multiplicity undermines principality.

% Partial annotations (shapes) to the rescue!

Our first insight is that partial type annotations often suffices to
recover principality. We refer to these annotations as \textit{shapes}. For
example, explicitly annotating the tuple projection with its arity, as in
$\exfield \e n j$, disambiguates the expression:
\begin{mathpar}
   \inferrule*
      {\G \th \e : \Pi\iton \ti \\ 1 \leq j \leq n }
      {\G \th \exfield \e n j : \tj}
\end{mathpar}
Yet, requiring users to write such annotations manually is impractical.

% Inference of shapes

Our solution is to permit \textit{inference} of shapes, provided that the
inferred annotation is \textit{uniquely determined by context}.  This
approach ensures principality by construction and yields a clear
specification with predictable annotability requirements for the everyday
programmer.

% A note on stratified type inference

\Xdidier {This paragraph should also fit into the introduction, I think}
The idea of leveraging a principled core language with inference of shapes
is not novel. \textit{Stratified type inference}
by~\citet{Pottier-Regis-Gianas/stratified@popl06} pursued a similar goal---in
the context of generalized algebraic data types. It operates in two phases:
the first phase transforms the program into an intermediate language and
generates shapes using \textit{bidirectional typing}; the second performs
\HM type inference for the intermediate language. While the second phase is
sound, complete and enjoys principal types, the overall system remains
incomplete due to the heuristic nature of the first phase.

% How do we specify our intuition?

Specifying the notion of unique inferrability from context is
tricky. Luckily, we can leverage many of the formal methods developed in the
previous section---in particular, the manipulation of contexts within the
derivation=--to prove uniqueness of shapes. Allowing us to
systematically replace the ambiguous constructs with their disambiguated
counterparts:
\begin{mathpar}
  \inferrule*
    {\eshape \E e {\Pi\iton \tvs} \\ \G \th E[\exfield \e n j] : \t}
    {\G \th \E[\efield \e j] : \t}
\end{mathpar}
The intuition here is that the context $E$ is sufficiently large to
constrain the shape of $e$'s type as a tuple of size $n$.

% Limitations

The cheif limitation of our approach is that disambiguation can still
require explicit annotations in inherently ambiguous scenarios.  This is
fine. Another option would be the use of sensible ``defaults'', such as
resolving $\efield e j$ by assuming the tuple has arity $j$. While
appealing, such defaults compromise principality. We return to this
pragmatic tradeoff in \cref{sec:implementation}.

\subsection{The language and its core-\ML typing rules}

\begin{bnffig}{fig/syntax}{Syntax of types and terms}
\entry[Terms]{\e}{
  x \and
  () \and
  \efun x e \and
  \eapp \eone \etwo \and
  \elet x \eone \etwo \and
  (e : \exi \tvs \t) \andcr
  \erecord {\overline{\elab = e} } \and
  \efield e \elab \and
  \exfield e \F \elab \andcr
   (\eone, \ldots, \en) \and
   \efield e j \and
   \exfield e n j \andcr
   \epoly e \and
   \epoly[\exi \tvs. \ts] e  \and
   \einst e \and
   \exinst e \tvs \ts
}\\[1ex]
\entry[Types]{\t}{
   \tv \and
   1 \and
   \tya \to \tyb \and
   \tys \F \and
   \Pi_{i = 1}^n \ti \and
   \tpoly {\exi \tvs \ts}
}\\
\entry[Type schemes]{\ts}{
     \t \and
     \tfor \tv \ts
}\\
\entry[Contexts]{\G}{
   \eset \and
   \G, x : \ts \and
   \G, \tv
}\\
\end{bnffig}

\paragraph{Terms}

In \cref {fig/syntax}, we give the grammar for our calculus. Terms include
all of the \ML calculus: variables $x$, the unit literal $\eunit$,
lambda-abstractions $\efun x e$, applications $\eapp \eone \etwo$,
annotations $(e
: \t)$ and let-bindings $\elet x \eone \etwo$. Our extensions include:
\begin{enumerate}
\item
  Constructor and record label disambiguation, modelled using record
  literals $\erecord { \labone = \eone; \ldots; \labn = \en }$ and field
  projections $\efield e l$.

\item
  Tuples $(\eone, \ldots, \en)$ with overloaded tuple projections
  $\efield e i$.

\item
  For semi-explicit first-class polymorphism, we have the boxing construct
  $\epoly \e$  and the unboxing construct $\einst e$.

\end{enumerate}
Each construct that endangers principality (written $\e^i$) has an
explicitly annotated counterpart (written $\e^x$) that is $\erecord {\overline{\ell / \F = \e}}$, $\eproj [\F] \e
\elab$, $\eproj [n] \e j$, $\epoly [\exi \tvs \ts] \e$, or $\einst [\exi
\tvs \ts] \e$.

\paragraph{Types}

As usual, types are split into monotypes (or just types) and type
schemes. Types $\t$ include the variables $\tv$, the unit type $1$, function
types $\tone \to \ttwo$, $n$-ary product types $\Pi_{i = 1}^n \ti$ and
nominal record types $\tys \F$. Type schemes $\sigma$ extend monotypes
allowing the universal quantification of zero or more type variables $\all
\tvs \t$. We write $\tv, \tvb, \tvc$, \etc for type variables.  We also have
polytypes \textit{without the annotations} $[\ts]$ as monotypes.

\paragraph{Typing contexts}

Tying contexts $\Gamma$ are an ordered sequence of expression variable
typings $x : \sigma$ and universally quantified type variables. Our
presentation of \ML explicitly tracks the type variables in scope in the type
context $\G$. (This may differ from traditional presentations that permit type
variables to ``float around''.)


\paragraph{Typing}

\begin{mathparfig}{fig/typing}{Typing rules}
  \inferrule[Var]
    {x : \sigma \in \G}
    {\G \th x : \sigma}

  \inferrule[Fun]
    {\G \th \tone \\
     \G, x : \tone \th e : \ttwo }
    {\G \th \efun x e : \tone \to \ttwo}

  \inferrule[App]
    {\G \th \eone : \tone \to \ttwo \\
     \G \th \etwo : \tone}
    {\G \th \eapp \eone \etwo : \ttwo}

  \inferrule[Unit]
    {}
    {\G \th () : 1}

  \inferrule[Annot]
    {\G \th e : \t\where {\tvs \is \tys}}
    {\G \th (e : \exi \tvs \t) : \t\where {\tvs \is \tys}}

  \inferrule[Gen]
    {\G, \tv \th e : \sigma \\ \tv \disjoint \G}
    {\G \th e : \tfor \tv \sigma}

  \inferrule[Inst]
    {\G \th e : \sigmaone \\
     \G \th \sigmaone \leq \sigmatwo}
    {\G \th e : \sigmatwo}

  \inferrule[Let]
    {\G \th \eone : \sigma \\
     \G, x : \sigma \th \etwo : \t}
    {\G \th \elet x \eone \etwo : \t}

  \inferrule[Tuple]
    {\parens{\G \th \ei : \ti}\iton}
    {\G \th (\eone, \ldots, \en) : \Pi\iton \ti}

  \inferrule[Proj-X]
    {\G \th \e : \Pi\iton \ti \\
     1 \leq j \leq n}
    {\G \th \exfield \e n j : \tj}

  \inferrule[Proj-I]
    {\eshape E e {\Pi\iton \tvs} \\
     \G \th E[\exfield e n j] : \t}
    {\G \th E[\efield e j] : \t}

  %% \inferrule [Use-X]
  %%   {\G \th \e : \tpoly {\ts \where {\tvs \is \tys}}}
  %%   {\G \th \einst[\exi \tvs \ts] \e : \ts \where {\tvs \is \tys}}

  %% \inferrule [Use-I]
  %%   {\eshape \E \e {\tpoly {\ts \where {\tvs \is \tys}}} \\
  %%    \G \th \E \where {\einst[\exi \tvs \ts] \e : \t}}
  %%   {\G \th \E \where {\einst \e} : \t}

  \inferrule [Poly-X]
    {\G \th \e : \ts\where {\tvs \is \tys}}
    {\G \th \epoly[\exi \tvs \ts] \e : \tpoly {\ts \where {\tvs \is \tys}}}

  \inferrule [Poly-I]
%%     {\Eshape \E \e {\tpoly {\exi \tvs \ts}} \\
%%      \G \th \E \where {\epoly[\exi \tvs \ts] \e} : \t}
%%     {\G \th \E \where {\epoly \e} : \t}
    {\Eshape \E \e {{\exi \tvs \tpoly \ts}} \\
     \G \th \E [\epoly[\exi \tvs \ts] \e] : \t}
    {\G \th \E [\epoly \e] : \t}

  \inferrule [Use-X]
    {\G \th \e : \tpoly \ts \where {\tvs \is \tys}}
    {\G \th \exinst e \tvs \sigma : \ts \where {\tvs \is \tys}}

  \inferrule [Use-I]
    {\eshape E  \e {\exi \tvs \tpoly \ts} \\
     \G \th E[\exinst e \tvs \ts] : \t}
    {\G \th E[\einst e] : \t}

  \inferrule[Rcd-Assn]
    {\G \th \e : \t \\
     \G \th \elab \leq \t \to \tys \Fapp}
    {\G \th \elab = \e : \tys \Fapp}

  \inferrule[Rcd]
    {\parens{\G \th \labi = \ei : \t}\iton}
    {\G \th \erecord {\elaba = \eone; \ldots; \elab_n = \en} : \t}

  \inferrule[Rcd-Proj]
    {\G \th \e : \tp \\
     \G \th \elab \leq \t \to \tp}
    {\G \th \efield \e \elab : \t}

  \inferrule[Lab-X]
    {\elab / \F : \ts \in \D \\
     \G \th \ts \leq \tsp}
    {\G \th \elab / \F \leq \tsp}

  \inferrule[Lab-I]
    {\Lshape \Lab \elab \F \\
      \G \th \Lab[\elab / \F] : \t}
    {\G \th \Lab[\elab] : \t}

\end{mathparfig}

\begin{mathparfig}
  {fig:inst-rel}
  {Instantiation relation $\mlsub \ts \tsp$}

  \infer[Inst-Refl]
    {}
    {\mlsub \t \t}

  \infer[Inst-ForallL]
    {\mltyp \t \\
     \mlsub {\ts[\tv := \t]} \tsp}
    {\mlsub {\tfor \tv \ts} \tsp}

  \infer[Inst-ForallR]
    {\tv \disjoint \G \\
      \mlsub [\G, \tv] \ts \tsp}
    {\mlsub \ts {\tfor \tv \tsp}}
\end{mathparfig}

As usual, the typing judgment $\G \th \e : \ts$ states that in context
$\G$, expression $\e$ has type scheme $\ts$.  Typing rules are given on
\cref {fig/typing}.

\Xdidier{The are  also judgments $\G \th \ell : \ts$
and $\G \th \ell = e : \ts$ where $\ell$ and $\ell = \e$ are not expressions
and a judgment $\mlsub \ts \tsp$.}

% Simple typing rules explained

\Xdidier {I am not sure that we need and will have space to read in details
the typing rules of Core \ML}

Rule \Rule{Var} retrieves the type scheme $\x : \ts$ from the context $\G$.
Function types are introduced via lambda abstractions: in Rule \Rule{Fun}, the
system guesses a well-formed type $\tone$ for the type of $x$, typechecks
the body $e$ is under the extended context $\G, \x : \tya$ producing the
return type $\tyb$, and assigns the abstraction the function type
$\tya \to \tyb$. Conversely, function types are eliminated by applications;
in Rule \Rule{App}, the type of the argument must match the function's parameter
type $\tya$ and application returns the type $\tyb$. Rule \Rule{Unit} asserts
that $()$ has the unit type $\tunit$.

% Gen/Inst explained

Rules \Rule{Gen} and \Rule{Inst} correspond to implicit
\textit{generalization} and \textit{instantiation} respectively.
Generalization universally quantifies a type variable $\tv$, introducing it
as a fresh polymorphic variable in the typing context. In \Rule{Inst}, we
specialize a type scheme $\tsa$ to $\tsb$ using the \emph{instantiation
relation} given in \cref{fig:inst-rel}.  This expresses that one type scheme
is a polymorphic generalization of another.

% Instantiation relation
\Xdidier {Is there a reason to use an intermediate instantiation relation,
rather than just a Rule \Rule {Inst}---since there is already a rule \Rule
{Gen}, which we must keep?}

Rules \Rule{Inst-Refl} trivially asserts that a monotype is an instance of
itself. In \Rule{Inst-ForallL}, a universally quantified type on the left may be
instantiated by substituting $\tv$ with any monotype to match the right hand
side. Conversely, \Rule{Inst-ForallR} has a type scheme $\tfor \tv \ts$ on the
right-hand side. In this case, we introduce a fresh polymorphic type variable
$\tv$ into the context to test whether the $\tsa$ is an instance of $\tsb$
under any instantiation of $\tv$---this is essentially a form of skolemization.

% Let rule

Let-polymorphism is handled by the \Rule{Let} rule, where a \textit{polymorphic}
term can be bound. This allows a single definition to be instantiated differently
at each use site---an essential feature of \ML. In this rule, the term $\eone$
has a polymorphic type scheme $\ts$, adds $\x : \ts$ into the context $\G$ to
typecheck $\etwo$.

% Annotations

Annotations $(e : \exi \tvs \t)$ ensures that the type of $e$ is (an instance
of) the type $\t$. The type variables $\tvs$ are
\emph{flexibly} (or existentially) bound in $\t$, meaning that $\tvs$ may be
unified with some types $\tys$ to produce a well-typed term. For instance,
the term $(\efun x x + 1 : \exi \tv
\tv \to
\tv)$ is well-typed with $\tv := \tint$ in \Rule{Annot}.

% Contextual rules (Poly-*, Use-*, Proj-*)

\subsection {Advanved typing}

The typing rules for fully annotated terms ($\e^x$) are unsurprising.
However, typing rules for terms with omitted type annotations are
non-compositional as they depend on a surrounding one-hole context
$\E$. Hence, they assert that the typability of the expression $\G \th \E
\where {\e^i}: \t$ where $\e^i$ is an expression with an implicit type
annotation.
%
We first request a typing for the expression with an explicit annotation $\G
\th \E \where {\e^x}: \t$ where $\e^x$ is a fully annotated variant of $\e^i$.
We then request that (the shape of) the annotation is fully determined from
context, either from the type of the expression, which we write $\eshape \E
\e \sh$, or from the type of the hole, which we write $\Eshape \E \e \sh$.

% Shapes
Shapes are one of the following form:
\begin{mathpar}
\begin{bnfgrammar}[\def \and {\wide\mid}]
\entry{\sh}{
       \tunit
  \and \tva \to \tvb
  \and \Pi\iton \tvs
  \and \tvs \F
  \and \exi \tvs \tpoly \ts
}
\end{bnfgrammar}
\end{mathpar}
\Xalistair{This syntax is a bit odd for polytypes as no other shape quantifies
over its monomorphic part}\Xdidier{Initially, shapes where just
constructors for me---not shallow terms}\Xdidier{The syntax is not enough:
we should ensure that all type variables are disjoint, \ie. shape are
equivalent to constructors..., even for polytypes thanks to the existence of
a canonical representation}

The shape of a variable is undefined (as in \cref{fig:shape-fn-def-typing}).
The shape of the unit type $\tunit$ is $\tunit$. Monotypes that contain
nested types, such as the function type $\t \to \tp$, have the shape of
their top-level type constructor with their children replaced by variables,
\ie $\tva \to \tvb$.
%
The shape of a polytype $\tpoly \ts$ is the \emph{polytype skeleton} of
$\tpoly \ts$ abstracted over the monomorphic parts. It is the minimal
closed type expression $\exi \tvas {\tpoly {\all \tvbs \t}}$ (up to the
ordering of variables) such that there exists types $\tys$ such that
$\tfor \tvbs \t \where {\tvas \is \tys}$ is equal to $\ts$.
%
Closed, means that $\tvas$ is equal to the free type variables
$\ftv \ts$ of $\ts$. The minimality condition is equivalent to the fact that
any suffix of a non-variable node in $\t$ leads to a polymorphic type
variable, \ie a variable in $\tvbs$.
%
When $\exi \tvs \tpoly \ts$ is a polytype skeleton, we write $\cpoly \tys \ts$ for
the polytype $\tpoly \ts \where {\tvs \is \tys}$.

\begin{mathparfig}[t]
{fig:shape-fn-def-typing}
{Definition of $\shape(\t)$, a partial function from types to
shape-substitution pairs $\langle \sh, \theta \rangle$, extended for all
types defined in the calculus}
  \shape(\tv) \eqdef \bot

  \shape(1) \eqdef 1

  \shape(\t \to \tp) \eqdef \langle \tv \to \tv', [\tv := \t, \tv' := \tp] \rangle

  \shape(\Pi\iton \tys) \eqdef \langle \Pi\iton \tvs, \where {\tvs := \tys} \rangle

  \shape(\tys \Fapp) \eqdef
	\langle \tvs \Fapp, \where {\tvs := \tys} \rangle

  \shape(\tpoly \ts) \eqdef \langle \exi \tvs \tpoly \ts, \where {\tvs := \tys} \rangle
\end{mathparfig}

\Xalistair{I feel the definition for $\tpoly \ts$ is not formal enough}

In order to describe the judgments $\eshape \E \e \sh$ and
$\Eshape \E \e \sh$, we introduce a cast operation $\ecast \e \t \tp$
which allows an expression $\e$ of type $\t$ to be seen with type $\tp$.
That is the typing rule for casts is
\begin{mathpar}
  \inferrule[Cast]
    {\G \th e : \t}
    {\G \th \ecast e \t \tp : \tp}
\end{mathpar}
Casts are not allowed on source terms and are just a device for the
definition of unavoidable shapes. Finally, we define it means for a shape
to be determined from the context or the expression:
\begin{mathpar}
\def \Eqdef {&\eqdef&}
{\begin{tabular}{RCL}
\eshape E \e \sh \Eqdef
  \forall \G, \t, \tp, \uad
  \G \th E \where {\ecast \e \typ \typp} : \t \wedge \shape(\typ) \downarrow \implies \shape(\typ).1 = \sh
\\[1ex]
\Eshape E \e \sh \Eqdef
  \forall \G, \t, \tp, \uad  \G \th E\where{\ecast \e \typ \typp} : \t \wedge \shape(\typp) \downarrow \implies
  \shape(\typp).1 = \sh
\end{tabular}}
\end{mathpar}
These states that the shape $\sh$ of expression $\e$ in context $\E$ is
determined by the expression $\e$, in the former case, or by the context
$\E$ in the latter case.

Shapes are equal modulo alpha equivalence and the removal of useless
polymorphic type variables. They do not have useless existential variables.


% Expression-based implicit rules
The implicit rule \Rule{Proj-I} types the projection $\eproj \e j$ provided the
context $\E$ \emph{infers} that the shape of $\e$ must be a tuple with arity $n$.
Similarly, \Rule{Use-I} permits instantiating a polytype in $\einst e$ if
the context $\E$ infers that the type of $\e$ must be a polytype with skeleton
$\exi \tvs \tpoly \ts$. The rule \Rule{Poly-I} types the implicit boxing
construct $\epoly \e$ by \emph{checking} the expected type of $\epoly \e$ in the
context $\E$ is a polytype with the skeleton $\exi \tvs \tpoly \ts$. This rule
differs from the previous two as the shape is determined by the expected type
within the context as opposed to the inferred type of $\e$.


% Labels

We adopt a similar non-compositional approach for elaborating overloaded labels,
whether in record projection ($\efield e \elab$) or record construction
($\erecord {\overline{\elab = \e}}$). Here, a one-hole label context $\Lab$
provides the surrounding context in which a label $\elab$ appears, and we require
that the record type $\F$ associated with $\elab$ is uniquely determined.
This is expressed by the judgment $\Lshape \Lab \elab \F$, where
the context $\Lab$ is as follows:
\begin{mathpar}
\begin{bnfgrammar}
\entry [Label contexts]{\Lab}{
    \E \where {\e.\square} \and
    \E \where
         {\erecord
            {\elaba = \eone; \ldots; \square = \ei ; \ldots; \elab_n = \en}
         }
}
\end{bnfgrammar}
\end{mathpar}
The judgement $\Lshape \Lab \elab \F$holds if either:
\begin{enumerate*}
\item
  the label $\elab$ can be uniquely resolved to a record type $\F$ in the
  typing context, written $\eshape \Lab {\elab ~} \F$;
\item
  the context $\Lab$ infers that $\elab$ must be instantiated to the record
  type $\tys \Fapp$, written $\Eshape \Lab \elab \F$.
\end{enumerate*}
We thus define:
\begin{mathpar}
  \Lshape \Lab \elab \F \Wide \eqdef
     \eshape \Lab \elab \F \vee \Eshape \Lab \elab \F
\end{mathpar}
The first case applies when the record type is uniquely determined by the
syntax of the context, without needing shape inference. There are two main
subcases:
\begin{enumerate*}
\item
  in a record projection context $\E[\e.\elab]$, if the label $\elab$ is not
  overloaded, then the (record) typing context $\D$ assigns a unqiue record
  type $\F$ to $\elab$;  \Xdidier {What is a record typing contexte
  $]D$?---I don't understand the rules below: why not use $\G$? How is
  $\D$ related to $\E$ and $\e$?}

\item
  in a record expression $\E [\erecord {\elab_1 = \eone; \ldots; \elab_n =
  \en}]$, if the set of labels ${ \elab_1, \ldots, \elab_n }$ uniquely
  identifies a record type $\F$ in the typing context $\D$.
\end{enumerate*}
\begin{mathpar}
  \infer[Lab-Uniq-Proj]
    {\elab \mathop{!} \F \in  \Delta}
    {\eshape {\E[\e.\square]} \elab \F}

  \infer[Lab-Uniq-Rcd]
    {\bar \elab \mathop{!} \F \in \Delta}
    {\eshape
      {\E[\erecord
            {\elaba = \eone; \ldots; \square = \ei; \ldots;  \elab_n = \en }]}
      \elab \F }
\end{mathpar}

To support the second case, we introduce a casting label judgment
(\Rule{Lab-Cast}) that behaves analogously to the cast operator in
expression contexts. It enables us to assume a hole $\square : \t$ in label
position may be instantiated to a function type $\tp \to \t$:
\begin{mathpar}
  \infer[Lab-Cast]
    {}
    {\G \th \square : \t \leq \tp \to \t}

\Eshape \Lab \elab \F \Wide\eqdef
   \forall \G, \t, \tp, \G \th \Lab[\square : \tp] : \t \implies
        \shape(\tp).1 = \tvs \F
\end{mathpar}

% Contexts can be the entire term

\subsection {Examples of typings}

The following lemma shows that we can always take a larger context
$\E$ or $\Lab$ for implicit rules \Rule {Proj-I}, \Rule {Use-I}, \Rule {Poly-I}
and \Rule{Lab-I}.
That is, there is always a derivation using only toplevel contexts.
\begin{lemma}
\newcommand {\Eab}{\parens{\Ea\where \Eb}}
Assume $\eshape \Eb \e \sh$ and $\G \th \Ea \Where {\Ga \th \Eb \where {\e^x} :
  \tya} \t$, then $\eshape \Eab \e \sh$ and $\G \th \Eab\where {\e^i} : \t$.
  Similarly, for label contexts, assuming $\Lshape \Lab \elab \F$ and $\G \th \E [\Ga \th \Lab[\elab] : \tya] : \t$, then $\Lshape {(\E[\Lab])} \elab \F$ and $\G \th (\E[\Lab])[\elab] : \tau$.
\end{lemma}
\begin{proof}[Proof Sketch]
Formally, can be proved by induction on $\Ea$ and $\E$, respectively.
\end{proof}

% Examples
We now illustrate the typing of implicit constructs with a few examples.
\begin{example}
To illustrate a simple case of non-typability, we show that the expression $e$
equal to $\efun \x {\eproj \x k}$ is ambiguous, \ie that it does not
typecheck.
%
Let $e_n$ be the explicitly annotated version $\efun r
{\eproj[n] \x i}$. for $n \le k$.  If there is a derivation of $\efun r
{\eproj \x i}$ then there must be one of the form:
\begin{mathpar}
\infer*[Right=Proj-I]{
                        \eshape E \x {\Pi\iton} \\
                \eset \th E \where {\eproj[n] \x k} : \t_k
}{%             -------------------------------------------
                  \eset \th E \where {\eproj \x k} : \t_k
}
\end{mathpar}
where $E$ is the term $\efun \x \ehole$, which is the largest possible
context. (If there is one with a smaller $E$, there is also one with a
larger $E$.)
%
We have  the following derivation:
\begin{mathpar}
\infer* [Right=Fun]{
          \infer*[Right=Proj-X]
                {\x : \Pi\iton \ti \th \x : \Pi\iton \ti}
                {r : \Pi\iton \ti \th \eproj[n] \x k : \t_k}
}{%             --------------------------------------------
                 \eset \th E \where{\eproj[n] \x k} : \t_k
}
\end{mathpar}
However, $\eshape E  \x {\Pi\iton}$ does not hold.
Indeed, we have $\eset \th E \where {\emagic {\x :\t_n}} : \wild$
for any $\t_n$ of the form $\Pi\iton \tys$ and $n \ge k$.
Hence, $\Pi\iton[k]$ and $\Pi\iton[k+1]$ are two possible shapes
for the type of $\x$.
\end{example}

\begin{example}
\locallabelreset
We now illustrate a non-ambiguous example, showing that the
expression $e$ equal to $\th \eapp {(\efun \x {\eproj
\x  k})} {(1, 2)} : \tint$.

Let $k$ and $n$ be $1$ and $2$.
Let $E$ be the context $\th \eapp {(\efun \x \ehole)} {(1, 2)} : \tint$.  We
have the derivation:
\begin{mathpar}
\infer* [Right=Proj-I]{
                        \eshape E \x {\Pi\iton} \\
                \eset \th E \where {\eproj[n] \x k} : \t_k
}{%             -------------------------------------------
                  \eset \th E \where {\eproj \x k} : \t_k
}
\end{mathpar}
Indeed, $\eset \th E \where {\eproj[n] \x k} : \t_k$ obviously holds.
Therefore, it just remains to show $\eshape E \x {\Pi\iton}$~\llabel C.
Assume $\eset \th \E \where{\ecast \x \typ \typp} : \t$. Since
$\x: \Pi\iton (\tint,\tint)$ is bound in the context at the hole $E$,
there is no other choice but $\tp$ equal to $\Pi\iton (\tint,\tint)$,
hence $\tshape \t = \Pi\iton$, which proves~\lref C.
\end{example}

The following example of non-typability illustrates is how the typing rules
still forces to reject typing of expressions whose annotations would
be unambiguous. This is intended, to prevent us from having to focus
at several annotation simultaneously. Our typing rules enforces the
resolution of annotation, locally, one construct at a time.

\begin{example}
\newcommand{\tyid}{\ty_{\kwd{id}}}
\newcommand {\epid}[1][]{\epoly[#1]{\efun z z}}
Let $\tyid$ be $\tpoly{\all \tv \tv \to \tv}$.
%
We show that the expression $e$ equal to $\elet \x {\epoly {\efun z z}}
{(\eapp {\einst \x} 1, \eapp {\einst \x} \eunit)}$ is rejected as ambiguous.
Let $\tyid$ be $\tpoly {\all \tv \tv \to \tv}$.  Clearly, we have $\elet \x
{\epoly [\tyid] {\efun z z}} {(\eapp {\einst[\tyid] \x} 1, \eapp
{\einst[\tyid] \x} \eunit)}$.  This is actually the only possible fully
annotated derivation.
%
To show that $e$ is typable, we must be able to make all annotations
optional, sequentially.  Therefore, the final step, which will eliminate the
last annotation has a single point of focus \Xgabriel[$\E{\e^x}$]{?}, which could be any
of the three positions with an annotation.  We consider each case
independently, and show that it is actually not typable.
\proofcase
{$\E$ is $\elet \x \ehole (\eapp {\einst \x} 1, \eapp {\einst \x}\eunit)$}
%
If this holds, we should have a derivation that ends with
\begin{mathpar}
\infer*[Right=Poly-I]{
                         \Eshape \E \epid \tyid \\
                  \eset \th \E \where {\epid [\tyid]}: \t
}{%               ---------------------------------------
                       \eset \th \E \where \epid : \t
}
\end{mathpar}
However, $\Eshape \E {\epoly {\efun z z}} {\tpoly \tyid}$ does not hold.
Indeed, the following judgment $\eset \th \E \where {\epid [s]}: \t$ where
$s$ is either $\all \tv \tv \to \tv$ or $\all \tv (\tv\to\tv) \to
\tv\to\tv$. Hence, this case cannot occur.

\proofcase
{$\E$ is
 $\elet \x \epid (\eapp {\einst \ehole} 1, \eapp {\einst \x} \eunit)$}
%
The derivation must end with:
\begin{mathpar}
\infer*[Right=Proj-X]{
                          \eshape \E \x \tyid \\
                \eset \th \E \where {\einst[\tyid] \x} : \t
}{%             -------------------------------------------
                    \eset \th \E \where {\einst \x} : \t
}
\end{mathpar}
However, $\eshape \E \x \tyid$ does not hold (the proof is similar to the
previous case).

\proofcase {$\E$ is  $\elet \x \epoly {\efun z z} (\eapp {\einst \ehole} 1, \eapp
{\einst \x} \eunit)$} This is symmetric to the previous case, which cannot
hold either.
\end{example}

\begin{example}
Let $\e$ be $\elet f {\efun \x {\eproj \x 1}} {\eapp f (1, 2)}$.
We should that $\e$ is well-typed.
$\e$ is of the form $\E \where {\x}$ where  $\E$ is the context $\elet f
{\efun \x \ehole} {\eapp f (1, 2)}$.
We have $\eset \th \E \where {\eproj[2] \x 1} : \tint$.
Let us show that $\eshape \E \x {\Pi\iton[2](\tint, \tint)}$.
%
Assume $\eset \th \E \where {\ecast \x \typ \typp} : \t$.
The type $\typ$ of $\x$ cannot be that of an arbitrary record, since
there is not such type as a record of arbitrary size. Hence,
$\typ$ must be a record of some shape $\Pi\iton$ for some size
$n$. Since the codomain of $f$ must be a tuple of size~$2$, then $n$ must
also be $2$. This shows that $\eshape \E \x {\Pi\iton[2]}$.

\Xalistair{This proof is invalid, see below.}

Here is a sketch that $\neg \eshape \E \x {\Pi\iton[2]}$.
We have the following typing derivation
for $\emptyset \th \E[\ecast \x \tva \tvb] : \tunit$:
\begin{mathpar}
  \infer
    {
    \infer
      {
	\infer
	  {
	    \infer
	      {
		\infer
		  {}
		  {\tva, \tvb, x : \tva \th x : \tva}}
	      {\tva, \tvb, x : \tva \th \ecast x \tva \tvb : \tvb}}
	  {\tva, \tvb \th \efun x {\ecast x \tva \tvb : \tva \to \tvb}}}
      {\emptyset \th \efun \x \ecast \x \tva \tvb : \tfor {\tva, \tvb} \tva \to \tvb} \\
    \infer
      {\ldots}
      {f : \tfor {\tva, \tvb} \tva \to \tvb \th \eapp f (1, 2) : \tunit}}
    {\emptyset \th \elet f {\efun \x {\ecast \x \tva \tvb}} {\eapp f (1, 2)} : \tunit}
\end{mathpar}
Yet $\shape(\alpha).1 = \sbot$.
\end{example}
\Xdidier{Yes, I found this problem, but lost my text had been lost in the
big merge (change of hierarchy): I was proposing the following as a fix}

I was suggesting to replace the premises of rule \Rule {Proj-I} by
\begin{mathpar}
\G \th E[\exfield e n j] : \t

(\G \th E[\exfield e k j] : \t \implies k = n)
\end{mathpar}
This will force $\x$ to be a product, and therefore a pair.
(It might force us to infer more, however.)

\Xalistair{I don't think this would fix it, since the issue isn't with the
$\G \th \E[\exfield \e n j] : \t$ premise but the $\eshape \E \e \sh$
premise}
\Xdidier{The right-hand side above is an attempt to replace the $\eshape \E
\e \sh$, but I agree that this is likely not the right direction}

Hoverver, I think the correct solution is rather to formalize the fact that
shape should be shared by all instances and that the constraint solver
should know than, \ie you idea of using a mornomirphic higher-order function
applied to polymorphic type variables, since this is really the intuition.

\Xdidier{Yet an other attempt}
\begin{version}{\blue\True}
If we used row variables to typecheck tuples, then an we could force
the row variabe
There is an easy solution for tuples label, which is to use row variables
and request that the row variable be allocated at the toplevel.
\end{version}

\subsection{Constraint generation}
\label{sec:constraint-gen}

% Intro
We now present the formal translation from terms $\e$ to constraints $\C$,
such that the resulting constraint is satisfiable if and only if the term is
well typed. The translation is defined as a function $\cinfer \e \t$, where $\e$
is the term to be translated and $\t$ is the expected type of $\e$.

% Explanation of expected type
The expected type $\t$ is permitted to contain type variables, which can be
existentially bound in order to perform type inference. The models of constraint
$\cinfer \e \t$ interpret the free variables of $\t$ such that
$\t$ becomes a valid type of $\e$. For example, to infer the entire type of $\e$
we may pick a fresh type variable $\tv$ for $\t$.

\begin{mathparfig}
  {fig:constraint-gen}
  {The constraint generation translation for ??}

  \cinfer x \t \eqdef \cinst x \t

  \cinfer {()} \t \eqdef \cunif \t \tunit

  \cinfer {\efun \x \e} \t \eqdef
    \cexists {\tva, \tvb} \cunif \t {\tva \to \tvb}
    \cand \cletin \x {\cabs \tvc {\cunif \tvc \tva}} {\cinfer \e \tvb}

  \cinfer {\eapp \eone \etwo} \t \eqdef
    \cexists {\tva, \tvb} \cinfer \eone \tva \cand \cinfer \etwo \tvb
    \cand \cunif \tva {\tvb \to \t}

  \cinfer {\elet \x \eone \etwo} \t \eqdef
    \cletin \x {\cabs \tva {\cinfer \eone \tva}} {\cinfer \etwo \t}

  \cinfer {\eannot \e \tvs \tp} \t \eqdef \cexists \tvs \cunif \t \tp \cand \cinfer \e \tp

  \cinfer {\etuple {\eone, \ldots, \en}} \t \eqdef \cexists \tvs \cunif \t {\Pi\iton \tvs}
    \cand \cAnd \iton \cinfer \ei {\tv_i}

  \cinfer {\exfield \e j n} \t \eqdef \cexists {\tv, \tvbs} \cunif \tva {\Pi\iton \tvbs}
    \cand \cinfer \e \tva
    \cand \cunif \t {\tvb_j}

  \cinfer {\efield \e j} \t \eqdef \cexists \tv \cinfer \e \tv
    \cand \cmatch \tv {\fvs \t} {(\lambda (\Pi \iton \tvbs).~ \cunif \t {\tvb_j})}

  \cinfer {\expoly \e {\exi \tvbs \tfor \tvs \tp}} \t \eqdef \cexists {\tvbs}
    (\cfor \tvs \cinfer \e \tp)
    \cand \cunif \t {\tpoly {\tfor \tvs \tp}}

  \cinfer {\einst \e} \t \eqdef \cexists \tva
    \cinfer \e \tva
    \cand \cmatch \tva {\fvs \t} {(\lambda (\exi \tvbs \tpoly \ts).~ \ts \leq \t)}

  \cinfer {\efield \e \elab} \t \eqdef \cexists \tv \cinfer \e \tv
    \cand \bbrackets {\elab \mathop{!} \tv}
    \cand \bbrackets {\elab \leq \t \to \tv}

  \cinfer {\erecord {\overline{\elab = \e}}} \t \eqdef
    \bbrackets {\bar \elab \mathop{!} \t}
    \cand \cAnd \iton \bbrackets {\elab = \e : \t}

  \bbrackets {\elab = \e : \t} \eqdef \cexists \tv \cinfer \e \tv
    \cand \bbrackets {\elab \leq \tv \to \t}

  \bbrackets {\elab \leq \t \to \tp} \eqdef
    \cmatch \tp {\fvs \t} {(\lambda (\tvs \Fapp).~ \cletin \ts {\D(\elab/\F)} \ts \leq \t \to \tvs \Fapp)}
\end{mathparfig}
\Xalistair{We need a cute name for our calculus -- we could go for
$\textsf{ML}_\textsf{sus}$ (or symbol for the subscript)}

% Explanation of constraint gen cases
% Simple constraint gen
The function $\cinfer - {\mathop{=}}$ is defined in \cref{fig:constraint-gen}.
We assume that all variables $\tv_i$ existentially bound by the translation are fresh.
Unsurprisingly, variables generate an instantiation constraint. Unit $()$ requires
the type $\t$ to be $\tunit$. Function generate a constraint that binds two fresh
flexible type variables for the argument and return types. We use a $\Let$ constraint
to bind the argument in the constraint generated for the body of the function. The
$\Let$ constraint is monomorphic since $\tv_3$ is fully constrained by type variables
defined outside the abstraction's scope and therefore cannot be generalized. Function
application binds two fresh flexible type variables for the function and argument types
and ensures $\tv$ is the return type of the function. Let terms generate a polymorphic
let constraint; $\cabs \tv {\cinfer \e \tv}$ is a principal constraint abstraction for $\e$:
its intended interpretation is the set of all types that $\e$ admits.

% New constraint gen cases
\TODO{Explain new constraint gen cases}


% Soundness/completeness of constraint gen
The translation is sound and complete with respect to ?? typing rules. That is to say, the term $e$ is well-typed if and only if $\llbracket e : \alpha \rrbracket$ is satisfiable.
%
\begin{theorem}{(Constraint generation is sound and complete)}
  Given a well-formed assignment $\semenv$. Then $\semenv \th \cinfer \e \tv$
  if and only if $\floor \semenv \th \e : \semenv(\tv)$.
\end{theorem}
\Xalistair{Our type system has well-formedness conditions (that typically
make proving this result (using a ground semantics for constraints), difficult)}




\section{Solving constraints}
\label{sec:solving}

\TODO
{a declarative solver.}

% Intro

% Shape of solver

% Constraint contexts (and their use in solving)

% Solved forms

% Unification

% Solver rules

% Explainations for the main rules

% Term variable constraints

% Suspended match constraints

% Metatheory


\section{HM Prototype and discussion}
\label{sec:implementation}

% Generalization trees

% Lazy generalization

% Schedulers

% Partial generics and instances

\paragraph{Interaction with let-polymorphism}
\Xgabriel{This was moved from the introduction, it's related to ``choice points'' below.}

%% Our intuition is not entirely correct here.

While the above constraint generation already improves on
OCaml's current inference approach, it is not powerful enough to capture the
simple idea that to resolve an ambiguous constructor we must use contextual
information to show there is a unique type constructor $\F$ for the
application.

We demonstrate this is with the following example:
\begin{program}
let foo =
  let f x = match x with L -> 1 in
  f (L : m)
\end{program}
With the above contraint generation, we would keep the type of \code{x}
as a generalizable variable, thus the application of \code{f} would not
resolve this ambiguity, requiring an annotation at the definition.

We require some way of splitting the head of the type from the type itself,
treating the head of the variable monomorphically and the rest
polymorphically.
\Xalistair{An explaination of kinded(1) types here }

\subsection{Choice points}

The current typing rules don't support back-propagation:

\begin{program}
fun (r1 : one) ->
  let g r = r.x in
  g r1
\end{program}


\subsection{Default clauses}


\section{Semi-unification Prototype}

Rank-2 polymorphic type inference with semi-unification!

\Xdidier{When you do rank-2 unification, you don't need a 'let' anymore
because 'lambda' is enough. You have rank-2 types but you cannot bind
them. It's not as exciting as I thought it would be.}

\section{Related work}
\label{sec:related-work}

\subsection{Qualified types}

\Xdidier {The following paragaph is just take from the old introduction}

Qualified types~\citep*{TODO} represent additional knowledge on a type. They
are used in particular in Haskell type-classes, where a constraint
$\mathsf{Show}~\alpha$ represents the fact that a part of the inferred term
needs to print values of type $\alpha$. A constraint on a ground type such
as $\mathsf{Show}~\mathsf{Int}$ can be resolved to a known printer. But if
the undetermined variable $\alpha$ becomes generalizable in a type $\tau$,
we get a type-scheme $\tfor{(\alpha \mid \mathsf{Show}~\alpha)}{\tau}$ that
also includes the type-class constraint.

DRAFT: Type-classes let each use-site choose a different typeclass instance
-- but this implies dictionary-passing or specialization. Sometimes we want
the choice to be shared by all use-sites:


\subsection{Suspended constraints in \textsc{OutsideIn}}

\Xalistair{They first solve simple constraints (existentials,
unification). Then they solve ``... constraints'', with implication
constraints, which corresponds to the case of GADT matches. Crucially, they
abandon local let generalization.}

\subsection{Suspended constraints in dependent-type systems}


\subsection{Bi-directional type inference}

\Xgabriel{Our approach should have better properties for disambiguation, but
which ones?}

In the simply-typed case our system works better. But with generalization
you can have more issues.

\Xdidier{Should we try to extend what we did to predicative polymorphism?}

\Xalistair{Not ready yet and would probably need too much space.}

\Xgabriel{CoreML + bidirectional disambiguation of constructors?}

\TODO
{do we understand what to say precisely about bidirectional type inference?}

\subsection{Principality tracking in \OCaml}

\Xdidier{The point of that is to enforce a directional type inference that
is based on let-bindings. When we check that the level is generic, we check
that we already ``closed'' this thing, it is an earlier 'let'
binding. Because we are omni-directional, we don't have principality issues
anymore -- except with default rules. We are principal by construction, we
never make any choice.}

\Xdidier{Principality tracking is making the choice that we are going to
make a directional let-binding-based type inference. We get rid of that, we
are omni-directional, and don't have any principality issues.}

\Xgabriel{We should not claim too much if we don't understand default
clauses well enough. It could people the impression that we hide the issue
under the carpet.}

\Xdidier{First a declarative/principal system, and then non-principal
heuristics to refine it, a two-phase process. It's fine.}

\section{Future work}
\label{sec:future-work}

% \begin{acks}
% \end{acks}


%% \bibliographystyle{ACM-Reference-Format}
\bibliography{suspended}

\appendix

\section{Well-foundedness}
\label {appendix:wf}
\TODO{Show weel-foundedness of the constraint definition}


%%% Below this line will is a draft
\Draft{}{\end{document}}\color{blue}

\section{DRAFT: a later TODO  list}

\paragraph {Recursive types}

We should be clear about whether we allow or disallow recursive types.  It
would be a plus to allow them. This should be easy since we are working with
constraints. The only difficulty that I see is with polytypes, but there
should be canonical forms for polytype skeletons, so their equality should
be easily checkable (after put into normal form).

\section{DRAFT: shape variables}

This solution might be very close or equal to your high-rank approach (as I
got the intuition, but never saw the details).

I wonder if introducing shapes and shape variables in the language of
constraints would not simultaneously simplify the presentation and enable
backpropagation.

We now claim that backpropagation works for record fields
(because of a closed word approach I think), but they still do not work
for structural tuples and polytypes, do they?

I think we should make backpropagation to work---this is the whole idea of
omnidirectional.

By introducing shape variables, we may specify that the type of a structural
tuple is a tuple of unknown size, that is of the form $\Pi\varphi \tys$
where $\varpi$ is any tuple shape/type constructor, that should be
introduced at the toplevel. This can be let-bound to a type scheme
$\all \tvs \Pi\varphi \tys$ where $\varphi$ is being shared.

What we need is to speak about a type that has a toplevel symbol of unknown
arity. This could be presented by a variable constraint by a shape
\newcommand{\hasshape}{::}
$\all \tv\hasshape\varphi. \tv$.

We should have a global final check that shape variables are uniquely
solved. And the same kind of mechanism to move on by solving shapes when
uniquely determined.

The unknown shaped variables would be introduced for each occurrence of
fragile construct. These shapes would get unified when two types are
unified.

This should be closed to the idea you had. What don't we try to implement
it. Since we already introduce shapes at the meta-level to explain
constraint resolutions, we could as well introduce them at the concrete
level in the syntax of constraints.


\end{document}

% LocalWords:  omnidirectional typecheck polymorphism Hindley Milner kinded
% LocalWords:  GADTs typechecked codomain typechecking subexpressions Bodin
% LocalWords:  monomorphic subexpression Dunfield Riboulet jfla subtyping
% LocalWords:  greek Chargueraud typable monotype polytype Garrigue Remy
% LocalWords:  impredicative polytypes minimality RCL ary Proj toplevel
% LocalWords:  typability backpropagation arity
